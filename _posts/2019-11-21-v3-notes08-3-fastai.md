---
layout: post
title: "Broadcasting, Einstein sum, Pytorch operator"
author: dionne
categories: [Fast.AI-v3]
image: assets/images/5-fc.png
tags: [ featured ]
---


" Lecture 08 - Deep Learning From Foundations-part2 "

### Homework

---

#### CONTENTS
- [The forward and backward passes](#the-forward-and-backward-passes)
- [Foundation version](#foundation-version)
	- [Basic architecture](#frobenius-norm)
	- [Loss function: MSE](#Loss-function:-MSE)
	- [Gradients backward pass](#Gradients-backward-pass)
- [Refactor model](#Refactor-model)
	- [Layers as classes](#Layers-as-classes)
	- [Module.forward()](#module.forawd())
	- [nn.Linear and nn.Module](#nn.Linear-and-nn.Module)


### The forward and backward passes

##### Nomalization

~~~python
train_mean,train_std = x_train.mean(),x_train.std()
>>> train_mean,train_std
(tensor(0.1304), tensor(0.3073))
~~~

- Dataset, which is x_train, mean and standard deviation is not 0&1. **But we need them to be** which means we should substract means and divide data by std.
- You should not standarlize *validation set* because training set and validation set should be aparted.

- after normalize, mean is close to zero, and standard deviation is close to 1.

##### Variable definition

- **n,m**: size of the training set
- **c**: the number of activations we need in our model


### Foundation Version
#### Basic architecture

- Our model has one hidden layer, output to have 10 activations, used in cross entropy.
- But in process of building architecture, we will use mean square error, output to have 1 activations and lator change it to cross entropy

- number of hidden unit; 50

see below pic

![](/assets/images/model.jpg)

#### kaiming initializating

- We also want to make w1&w2 mean and std be 0&1.

It can made by, in case w1, 
~~~python
torch.randn(m,nh)
~~~
~~~python
torch.randn(m,nh)/math.sqrt(m)
~~~

In case w2,
~~~python
torch.randn(nh,1)
~~~
~~~python
torch.randn(nh,1)/math.sqrt(nh)
~~~

**Why we should divide weight by root of length row?**




#### Loss function: MSE



#### Gradients backward pass

### Refactor model
#### Layers as classes
#### Modue.forward()
#### nn.Linear and nn.Module



~~~python
~~~


~~~python
~~~