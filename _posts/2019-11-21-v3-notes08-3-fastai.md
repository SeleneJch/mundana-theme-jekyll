---
layout: post
title: "Broadcasting, Einstein sum, Pytorch operator"
author: dionne
categories: [Fast.AI-v3]
image: assets/images/5-fc.png
tags: [ featured ]
---


" Lecture 08 - Deep Learning From Foundations-part2 "

### Homework

---

#### CONTENTS
- [The forward and backward passes](#the-forward-and-backward-passes)
- [Foundation version](#foundation-version)
	- [Basic architecture](#frobenius-norm)
	- [Loss function: MSE](#Loss-function:-MSE)
	- [Gradients backward pass](#Gradients-backward-pass)
- [Refactor model](#Refactor-model)
	- [Layers as classes](#Layers-as-classes)
	- [Module.forward()](#module.forawd())
	- [nn.Linear and nn.Module](#nn.Linear-and-nn.Module)


### The forward and backward passes

##### Nomalization

~~~python
train_mean,train_std = x_train.mean(),x_train.std()
>>> train_mean,train_std
(tensor(0.1304), tensor(0.3073))
~~~

Remember!
- Dataset, which is x_train, mean and standard deviation is not 0&1. **But we need them to be** which means we should substract means and divide data by std.
- You should not standarlize *validation set* because training set and validation set should be aparted.
- after normalize, mean is close to zero, and standard deviation is close to 1.

##### Variable definition

- **n,m**: size of the training set
- **c**: the number of activations we need in our model


### Foundation Version
#### Basic architecture

- Our model has one hidden layer, output to have 10 activations, used in cross entropy.
- But in process of building architecture, we will use mean square error, output to have 1 activations and lator change it to cross entropy

- number of hidden unit; 50

see below pic

![](/assets/images/model.jpg){:height="80%" width="80%"}

- We want to make w1&w2 mean and std be 0&1.
	- why initializating and make mean zero and std one is important?[^1]

##### simplified karming init

- what about hidden(first) layer?

~~~python
w1 = torch.randn(m,nh)
b1 = torch.zeros(nh)
t = lin(x_valid, w1, b1) # hidden

>>> t.mean(), t.std()

((tensor(2.3191), tensor(27.0303))
~~~

In output(second) layer,

~~~python
w2 = torch.randn(nh,1)
b2 = torch.zeros(1)
t2 = lin(t, w2, b2) # output

>>> t2.mean(), t2.std()

(tensor(-58.2665), tensor(170.9717))
~~~

- which is terribly far from normalzed value.

- But if we apply simplified kaiming init

~~~python
w1 = torch.randn(m,nh)/math.sqrt(m); b1 = torch.zeros(nh)
w2 = torch.randn(nh,1)/math.sqrt(nh); b2 = torch.zeros(1)
t = lin(x_valid, w1, b1)
t.mean(),t.std()
>>> (tensor(-0.0516), tensor(0.9354))
~~~

- But, actually, we use activations not only linear function
- After applying activations relu at linear layer, mean and deviation became 0.5.

![](/assets/images/relu.jpg){:height="70%" width="70%"}

##### Glorrot initialization

Paper2: [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a.html)
	- Gaussian(, bell shaped, normal distributions) is not trained very well.
	- How to initialize neural nets?

![](/assets/images/xavier.png){:height="80%" width="80%"}


with 

\$$n_i$$

 the size of layer n, the number of filters i.

- But there is **No acount** for import of ReLU
- If we got 1000 layers, vanishing gradients problem emerges 

##### Kaiming initializating

Paper3: [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)
	- Kaiming He, explained [here](https://pouannes.github.io/blog/initialization/)
	- rectifier: rectified linear unit
	- rectifier network: neural network with rectifier linear units


- 

##### Initialization is important




<!-- 1:27:19 -->


#### Loss function: MSE



#### Gradients backward pass

### Refactor model
#### Layers as classes
#### Modue.forward()
#### nn.Linear and nn.Module



~~~python
~~~


~~~python
~~~

#### Reference

[^1]: [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/abs/1901.09321)
