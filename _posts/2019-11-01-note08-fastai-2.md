---
layout: post
title: "Lecture 08 - Deep Learning From Foundations-part2"
author: dionne
categories: [fast.ai-v3]
image: assets/images/5.png
tags: [ featured ]
---

#### Frobenius norm

$$\| A \|_F = \left( \sum_{i,j=1}^n | a_{ij} |^2 \right)^{1/2}$$ &nbsp;

- above converted into &nbsp; 

<code>(m\*m).sum().sqrt()</code>&nbsp;
- Plus, don't suffer from mathmatical symbols. He also copy and paste that equations from wikipedia.
- and if you need latex form, download it from archive.

###### What is the meaning of elementwise?
- We do not calculate each component. But all of the component at once. Because, length of column of A and row of B are fixed.

#### How much time we saved?

![](assets/images/4.png)



- So now that takes 1.37ms. We have removed one line of code and it is a 178 times faster...

\#TODO
I don't know where the `5` from. but keep it.
Maybe this is related with frobenius norm...?
&nbsp;&nbsp;
as a result, **the code before**&nbsp;
&nbsp;

`for k in range(ac):
    c[i,j] += a[i,k] + b[k,j]`

the code after&nbsp;
&nbsp;&nbsp;

`c[i,j] = (a[i,:] * b[:,j]).sum()`
&nbsp;&nbsp;&nbsp;

To compare it (result betweet *original* and *adjusted* version) we use not test_eq but other function. The reason for this is that due to rounding errors from math operations, matrices may not be exactly the same. As a result, we want a function that will “is a equal to b **within some tolerance**”

 &nbsp;
`\#export
def near(a,b): 
    return torch.allclose(a, b, rtol=1e-3, atol=1e-5)

def test_near(a,b): 
    test(a,b,near)

test_near(t1, matmul(m1, m2))`
&nbsp;

### Broadcasting

Now, we will use the broadcasting and remove

`c[i,j] = (a[i,:] * b[:,j]).sum()`
&nbsp;

- How it works?

`a=tensor([[10,10,10],
          [20,20,20],
          [30,30,30]])
b=tensor([1,2,3,])
a,b`

>(tensor([[10, 10, 10],
         [20, 20, 20],
         [30, 30, 30]]), tensor([1, 2, 3]))
         
`a+b`

>tensor([[11, 12, 13],
        [21, 22, 23],
        [31, 32, 33]])     
         
![](assets/images/3.png)
<p style="text-align: center;">Figure 2. demonstrated how array **b** is broadcast to compatible with **a** *Adopted from* [*numpy_tutorial*](https://www.tutorialspoint.com/numpy/numpy_broadcasting.htm)</p>&nbsp;

there is no loop, but it seems there is exactly the loop.

**This is [not from jeremy](#what-if-columns)(actually after a moment he cover it) but i wondered How to broadcast an array by columns?**

`c=tensor([[1],[2],[3]])
a+c`

> tensor([[11, 11, 11],
        [22, 22, 22],
        [33, 33, 33]])
---
        
#### What is tensor.stride()?

`help(t.stride)`

Help on built-in function stride:

stride(...) method of torch.Tensor instance
stride(dim) -> tuple or int
Returns the stride of :attr:`self` tensor.
Stride is the jump necessary to go from one element to the next one in the
specified dimension :attr:`dim`. A tuple of all strides is returned when no
argument is passed in. Otherwise, an integer value is returned as the stride in
the particular dimension :attr:`dim`.
Args:
    dim (int, optional): the desired dimension in which stride is required
Example::
`x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])`

`x.stride()`
> (5, 1)
`x.stride(0)`
> 5
`x.stride(-1)`
> 1

---

#### unsqueeze & None index

- We can manipulate rank of tensor
- Special value 'None', which means please squeeze a new axis here

`c = torch.tensor([10,20,30])
c[None,:]`

in c, _squeeze a new axis in here please._


### Matmul with broadcasting

`for i in range(ar):
#   c[i,j] = (a[i,:]).          *[:,j].sum() #previous
    c[i]   = (a[i]).unsqueeze(-1) * b).sum(dim=0)`

⭐️Tips🌟    
1) _Anytime there's a trailinng(final) colon in numpy or pytorch you can delete it_
2) any number of colon commas at the start, you can switch it with the single elipsis.

> ex) c[i, :] = c [i]

### Reference

- [TensorRank](http://mathworld.wolfram.com/TensorRank.html)
- [ti note](https://forums.fast.ai/t/forum-markdown-notes-lesson-8/41896)