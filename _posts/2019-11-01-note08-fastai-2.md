---
layout: post
title: "Lecture 08 - Deep Learning From Foundations-part2"
author: dionne
categories: [fast.ai-v3]
image: assets/images/5.png
tags: [ featured ]
---

#### Frobenius norm

![](/assets/images/7.png) <br />

- above converted into <br /> 

~~~ python
(m*m).sum().sqrt()
~~~

- Plus, don't suffer from mathmatical symbols. He also copy and paste that equations from wikipedia.
- and if you need latex form, download it from archive.

##### What is the meaning of elementwise?
- We do not calculate each component. But all of the component at once. Because, length of column of A and row of B are fixed.

#### How much time we saved?

![](/assets/images/4.png)

- So now that takes 1.37ms. We have removed one line of code and it is a 178 times faster...

\#TODO
I don't know where the `5` from. but keep it.
Maybe this is related with frobenius norm...?
<br /><br />
as a result, **the code before**<br />
<br />

~~~ python
for k in range(ac):
    c[i,j] += a[i,k] + b[k,j]
~~~

the code after<br />
<br /><br />

~~~ python
c[i,j] = (a[i,:] * b[:,j]).sum()
~~~
<br />

To compare it (result betweet *original* and *adjusted* version) we use not test_eq but other function. The reason for this is that due to rounding errors from math operations, matrices may not be exactly the same. As a result, we want a function that will ‚Äúis a equal to b **within some tolerance**‚Äù

~~~ python
\#export
def near(a,b): 
    return torch.allclose(a, b, rtol=1e-3, atol=1e-5)

def test_near(a,b): 
    test(a,b,near)

test_near(t1, matmul(m1, m2))
~~~

### Broadcasting

- Now, we will use the broadcasting and remove

~~~ python
c[i,j] = (a[i,:] * b[:,j]).sum()
~~~

- How it works?

~~~ python
a=tensor([[10,10,10],
          [20,20,20],
          [30,30,30]])
b=tensor([1,2,3,])
a,b
~~~
    
*(tensor([[10, 10, 10],
         [20, 20, 20],
         [30, 30, 30]]), tensor([1, 2, 3]))*{::}
         
~~~ python
a+b
~~~

*tensor([[11, 12, 13],
        [21, 22, 23],
        [31, 32, 33]])*{::}
         
![](/assets/images/3.png)
###### <Figure 2> demonstrated how array **b** is broadcasting(or copied but not occupy memory) to compatible with **a**. *Refered from [numpy_tutorial][1]*

[1]: https://www.tutorialspoint.com/numpy/numpy_broadcasting.htm

- there is no loop, but it seems there is exactly the loop.

- This is not from jeremy (actually after a moment he cover it) but i wondered How to broadcast an array by columns?

~~~ python
c=tensor([[1],[2],[3]])
a+c
~~~

*tensor([[11, 11, 11],
        [22, 22, 22],
        [33, 33, 33]])*{::}s

---
        
#### What is tensor.stride()?

~~~ python
help(t.stride)
~~~

*Help on built-in function stride:<br />
    stride(...) method of torch.<br />
Tensor instance<br />
stride(dim) -> tuple or int<br />
Returns the stride of :attr:'self' tensor.<br />
Stride is the jump necessary to go from one element to the next one in the specified dimension :attr:'dim'.<br />
A tuple of all strides is returned when no argument is passed in.<br />
Otherwise, an integer value is returned as the stride in the particular dimension :attr:'dim'.<br />*

Args:
    dim (int, optional): the desired dimension in which stride is required
Example::*{::}

~~~ python
x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])`
`x.stride()`
 (5, 1)
`x.stride(0)`
 5
`x.stride(-1)`
 1
~~~


---

#### unsqueeze & None index

- We can manipulate rank of tensor
- Special value 'None', which means please squeeze a new axis here

~~~ python
c = torch.tensor([10,20,30])
c[None,:]
~~~

in c, _squeeze a new axis in here please._


### Matmul with broadcasting

~~~ python
for i in range(ar):
#   c[i,j] = (a[i,:]).          *[:,j].sum() #previous
    c[i]   = (a[i]).unsqueeze(-1) * b).sum(dim=0)
~~~

‚≠êÔ∏èTipsüåü    
1) _Anytime there's a trailinng(final) colon in numpy or pytorch you can delete it_
2) any number of colon commas at the start, you can switch it with the single elipsis.

*ex) c[i, :] = c [i]*

### Reference

- [TensorRank](http://mathworld.wolfram.com/TensorRank.html)
- [ti note](https://forums.fast.ai/t/forum-markdown-notes-lesson-8/41896)