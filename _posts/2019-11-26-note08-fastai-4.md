---
layout: post
title: "fully-connected forward, gradient backward"
author: dionne
categories: [Fast.AI-v3]
image: assets/images/4-backward.png
tags: [ featured ]
---


" Lecture 08 - Deep Learning From Foundations-part2 "

### Homework
{: .no_toc}


---

### CONTENTS
{: .no_toc}

- [Foundation version](#foundation-version)
{:toc}

### 2. Foundation version

#### 2.2 Loss function: MSE

- Mean squared error need unit vector, so we remove unit axis.
~~~python
def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()
~~~
- In python, in case you remove axis, you use 'squeeze', or add axis use 'unsqueeze'
- [torch.squeeze](https://pytorch.org/docs/stable/torch.html#torch.squeeze) where code commonly broken. so, when you use squeeze, **clarify** dimension axis you want to remove

~~~python
tmp = torch.tensor([1,1])
tmp.squeeze()
>>> tensor([1, 1])
~~~

- make sure to make as float when you calculate


#### 2.3 Gradients backward pass

- Gradients is output with respect to parameter
- we've done this work in this path(below)

![](/assets/images/4-calculus.jpeg)

- to simplify this calculus, we can just change it into

$$ y=f(u) $$,
$$ u=g(x) $$

- So, you should know of the derivative of each bit on its own, and then you multiply them all together. As a result, it would be \$$dy$$ over \$$dx$$ cross over the data.



### 3. Refactor model
#### 3.1 Layers as classes
#### 3.2 Modue.forward()
#### 3.3 nn.Linear and nn.Module


~~~python
~~~	