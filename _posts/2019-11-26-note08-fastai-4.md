---
layout: post
title: "Gradient backward"
author: dionne
categories: [Fast.AI-v3]
image: assets/images/4-backward.png
tags: [ featured ]
---


" Lecture 08 - Deep Learning From Foundations-part2 "

### Homework
{: .no_toc}

- I often confuse that y is scalar, not a vector.

---

### CONTENTS
{: .no_toc}

- [Foundation version](#foundation-version)
{:toc}

- Forward process

![](/assets/images/4-forward.png)

### 2. Foundation version

#### 2.3 Gradients backward pass

- Gradients is output with respect to parameter
- we've done this work in this path(below)

![](/assets/images/4-calculus.jpeg)

- to simplify this calculus, we can just change it into

$$ y=f(u) $$,
$$ u=g(x) $$

- So, you should know of the derivative of each bit on its own, and then you multiply them all together. As a result, it would be \$$dy$$ over \$$dx$$ cross over the data.

![](/assets/images/4-derivative.png)

- So you can get gradient, output with respect to parameter

![](/assets/images/4-chain_rule.png)

- What order should we calculate?

![](/assets/images/4-calculus.jpeg)

BTW, why Jeremy wrote <script type="math/tex">\hat y</script>, not *Loss function*?[^1]
{: style="color:gray; font-size: 110%; text-align: center;"}

##### decompose function

- We want to get derivative of <script type="math/tex"> w_1 </script> which forms <script type="math/tex">t=x_1 @ w_1 + b_1</script>
- But, we have a estimation of answer (*we call it y hat*) now
- So, I will decompose funciton to trace target variable.

$$ \overset{\rightharpoonup}{u}\ \ linear_1\ \ \overset{\rightharpoonup}{w}\ \ ReLU\ \ \overset{\rightharpoonup}{v}\ \ linear_2\ \ \overset{\rightharpoonup}{a}\ \ MSE = \hat y $$

- Using the above forward pass, we can suppose some function from the end.
- start from <script type="math/tex">  \hat y </script>, We know MSE funciton got two parameters, output, <script type="math/tex">u</script> and target <script type="math/tex">y</script>.
- from MSE's input we know <script type="math/tex">  linear_2</script> function's output and supposing v is input of that function, <script type="math/tex">linear_2(v) = u</script>
- similarly, *v* became output of <script type="math/tex">ReLU, ReLU(t) = v</script>


![](/assets/images/4-backward3.jpeg){:height="70%" width="70%"}


##### chain rule with code

- examplify backward process by random sampling

~~~python
print(f"{x_train.shape}, {w1.shape}, {b1.shape}\n")
print(f"{lin2.shape}, {w2.shape}, {b2.shape}\n")
f"{mse_loss.shape}, {y_train.shape}, {lin3.shape}"

>>> torch.Size([50000, 784]), torch.Size([784, 50]), torch.Size([50])

torch.Size([50000, 50]), torch.Size([50, 1]), torch.Size([1])

'torch.Size([]), torch.Size([50000]), torch.Size([50000, 1])'
~~~

1) start with the very last function, which is loss funciton. MSE

![](/assets/images/4-mse.jpeg){:height="70%" width="70%"}

$$\color{red}{\frac{\partial}{\partial u}MSE(u,y)} \times\frac{\partial}{\partial v}l_2(v)\times\frac{\partial}{\partial t}ReLU(t)\times\frac{\partial}{\partial x}l_1(x)$$

- in *mse_grad* function, we store the gradient in inp.g{: style="color:gray; font-size: 110%; text-align: center;"}

~~~python
def mse_grad(inp, targ):
    # grad of loss with respect to output of previous layer
    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]
~~~

- And cz we defined mse_grad function's parameters before, we can use lin3 as input of mse

~~~python
(lin3.squeeze(-1)-y_train).unsqueeze(-1)

>>> tensor([[ -1.3000],
        [ -0.2116],
        [ -5.8100],
        ...,
        [ -2.2477],
        [ -0.3279],
        [-11.7130]])
~~~

- We can just calculate using broadcasting, not using squeeze. then why should do and unsqueeze again?<br />
ðŸŽ¯ It's related with random access memory(RAM).. If I don't *squeeze*, (I'm using colab) **it out of RAM**.

3) Derivative of linear3 function

- the gradient of a matrix product is the matrix product with the transposed , \$$ inp.g = out.g \  @  \ w.t() $$

~~~python
def lin_grad(inp, out, w, b):
    # grad of matmul with respect to input
    inp.g = out.g @ w.t()
    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)
    b.g = out.g.sum(0)
~~~


2) derivative of ReLU (linear2)

![](/assets/images/4-relu.png)

![](/assets/images/4-relugrad.png)

$$\frac{\partial}{\partial u}MSE(u,y) \times \color{red}{\frac{\partial}{\partial v}l_2(v)} \times \frac{\partial}{\partial t}ReLU(t)\times\frac{\partial}{\partial x}l_1(x)$$

- Because this is chain rule, after we get <script type="math/tex">\color{red}{\frac{\partial}{\partial v}l_2(v)}</script> we should multiply it with **inp.g**

- However, 

~~~python
def relu_grad(inp, out):
    # grad of relu with respect to input activations
    inp.g = (inp>0).float() * out.g
~~~

3) Derivative of linear1

- the gradient of a matrix product is the matrix product with the transposed , \$$ inp.g = out.g \  @  \ w.t() $$

~~~python
def lin_grad(inp, out, w, b):
    # grad of matmul with respect to input
    inp.g = out.g @ w.t()
    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)
    b.g = out.g.sum(0)
~~~

4) Then it goes backward pass

~~~python
def forward_and_backward(inp, targ):
    # forward pass:
    l1 = inp @ w1 + b1
    l2 = relu(l1)
    out = l2 @ w2 + b2
    # we don't actually need the loss in backward!
    loss = mse(out, targ)
    
    # backward pass:
    mse_grad(out, targ)
    lin_grad(l2, out, w2, b2)
    relu_grad(l1, l2)
    lin_grad(inp, l1, w1, b1)
~~~
 



### 3. Refactor model
#### 3.1 Layers as classes
#### 3.2 Modue.forward()
#### 3.3 nn.Linear and nn.Module


~~~python
~~~ 

### Footnote
{: .no_toc}
---
[^1]: [fast.ai forums Lesson-8](https://forums.fast.ai/t/lesson-8-2019-discussion-wiki/41323/433)
