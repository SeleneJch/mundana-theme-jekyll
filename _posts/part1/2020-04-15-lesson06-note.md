---
layout: post
title: "BatchNorm, Convolution arithmetic"
author: dionne
categories: [ fastai-v3 ]
image: assets/images/week8/att_00069.png
---


Fastai2 package dev version

1. fastai said it is `editable install` - but what is it?
2. what is egg-link, egg-info, fastai-1.0.34-py3.6.egg-info
3. find -E . -regex ".*\fast.+"  >>  	`.*\` is in need because `find` matches the whole match.
4. I can't rename env, but should clone and remove beforehand.

-------

# Lesson 06

- Tabular data: categorical variable vs Continuous variable.
- if datatype is int, fastai think it is classification, not a regression.
- Root mean square percentage error. as loss function.
-  when you assign the y_range, it's better to assign little bit more than actual maximum.
-  intermediate layers, which is weight matrix is 1) 1000, and 2) 500 -> which means our parameter would be 500*1000.


~~~python
learn.model
~~~

### What is drop out and embedding drop out?

[Nitish Srivastava, Dropout: A Simple way to prevent Neural Networks from Overfitting]()

- you can dropout `p` value, make it specified to specific layer, or make it applied to all the layers.
- Pytorch code 1) bernoulli, which decides whether you will hold it? 2) and divide the noise value depends on noise value. so noise became 2 or remain 0.
	 - According to pytorch code, We do change at training time, but we do nothing at test time. and this means you don't have to do anything special with inference time.'
	 - <b>TODO</b>: find at forums `what is inference time`?

- Embedding dropout is just a dropout.
	- It's different between continuous variable and embedding layer.  <b>TODO</b> Still can't understand. why embedding dropout is effective. or,... in need.
	- Let's delete at random, some of the results of the embedding. 
	- and It worked well especially at Kaggle

### Batch Normalization

[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift]() -> came out false! According to [How Does Batch Normalization Help Optimization?]()

- The key was  `multiplicative` bias {\gamma} and `additive` bias {\beta}`
- Explain
	- Let \$$ \hat{y}  = f(w_1, w_2, w_3, ... , x)} $$ ,  loss = MSE , Then `y_range` should be between 1 and 5`
	- And Activation function ends with `-1 -> +1`
	- To mitigate this problem, we can add the other parameter, like \$$w_n$$
	- But there're so much interactions in the process so just re-scale the output.

### Momentum parameter at BatchNorm1d
- Different from momentum at optimization.
-  Exponentially weighted moving average. 
	-  Small number: `mean standard deviation` would be less from mini_batch to mini_batch >> less regularization effect. Large Number

	
### What is convolution?
	
