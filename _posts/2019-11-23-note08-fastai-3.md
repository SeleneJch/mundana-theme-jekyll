---
layout: post
title: "data init, loss func, gradient backward"
author: dionne
categories: [Fast.AI-v3]
image: assets/images/2-relu.png
tags: [ sticky, featured ]
---


" Lecture 08 - Deep Learning From Foundations-part2 "

### Homework
{: .no_toc}

read section 2.2 of kaiming paper

---

### CONTENTS
{: .no_toc}

- [The forward and backward passes](#the-forward-and-backward-passes)
{:toc}
	* Normailzation
	{:toc}
- [Foundation version](#foundation-version)
{:toc}
	* [Basic architecture](#basic-architectures)
	{:toc}
	* [Loss function: MSE](#Loss-function:-mse)
	{:toc}
	* [Gradients backward pass](#Gradients-backward-pass)
	{:toc}
- [Refactor model](#Refactor-model)
{:toc}
	* [Layers as classes](#Layers-as-classes)
	{:toc}
	* [Module.forward()](#module.forawd())
	{:toc}
	* [nn.Linear and nn.Module](#nn.Linear-and-nn.Module)
	{:toc}

### 1. The forward and backward passes

#### 1.1 Nomalization

~~~python
train_mean,train_std = x_train.mean(),x_train.std()
>>> train_mean,train_std
(tensor(0.1304), tensor(0.3073))
~~~

Remember!
- Dataset, which is x_train, mean and standard deviation is not 0&1. **But we need them to be** which means we should substract means and divide data by std.
- You should not standarlize *validation set* because training set and validation set should be aparted.
- after normalize, mean is close to zero, and standard deviation is close to 1.

#### 1.2 Variable definition

- **n,m**: size of the training set
- **c**: the number of activations we need in our model

### 2. Foundation Version
#### 2.1 Basic architecture

- Our model has one hidden layer, output to have 10 activations, used in cross entropy.
- But in process of building architecture, we will use mean square error, output to have 1 activations and lator change it to cross entropy

- number of hidden unit; 50

see below pic

![](/assets/images/model.jpg){:height="80%" width="80%"}

- We want to make w1&w2 mean and std be 0&1.
	- why initializating and make mean zero and std one is important?[^1]

##### 2.1.1 *simplified* karming init

- what about hidden(first) layer?

~~~python
w1 = torch.randn(m,nh)
b1 = torch.zeros(nh)
t = lin(x_valid, w1, b1) # hidden

>>> t.mean(), t.std()

((tensor(2.3191), tensor(27.0303))
~~~

In output(second) layer,

~~~python
w2 = torch.randn(nh,1)
b2 = torch.zeros(1)
t2 = lin(t, w2, b2) # output

>>> t2.mean(), t2.std()

(tensor(-58.2665), tensor(170.9717))
~~~

- which is terribly far from normalzed value.

- But if we apply simplified kaiming init

~~~python
w1 = torch.randn(m,nh)/math.sqrt(m); b1 = torch.zeros(nh)
w2 = torch.randn(nh,1)/math.sqrt(nh); b2 = torch.zeros(1)
t = lin(x_valid, w1, b1)
t.mean(),t.std()
>>> (tensor(-0.0516), tensor(0.9354))
~~~

- But, actually, we use activations not only linear function
- After applying activations relu at linear layer, mean and deviation became 0.5.

![](/assets/images/relu.jpg){:height="70%" width="70%"}

##### 2.1.2 Glorrot initialization

Paper2: [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a.html)
- Gaussian(, bell shaped, normal distributions) is not trained very well.
- How to initialize neural nets?

![](/assets/images/xavier.png){:height="80%" width="80%"}


with $$n_i$$ the size of layer $$ n $$, the number of filters $$ i $$.

- But there is **No acount** for import of ReLU
- If we got 1000 layers, vanishing gradients problem emerges 

##### 2.1.3 Kaiming initializating

Paper3: [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)
- Kaiming He, explained [here](https://pouannes.github.io/blog/initialization/)
- rectifier: rectified linear unit
- rectifier network: neural network with rectifier linear units

![](/assets/images/kaiming.png){:height="80%" width="80%"}

- This is kaiming init, and why suddenly replace one to two on a top?
	- to avoid vanishing gradient(weights)
	- But it doesn't give very nice mean tough.{: .blue}

##### 2.1.4 Pytorch package

![](2-forward-linear.jpg)
- Why fan_out?
	- according to pytorch documentation,

`choosing 'fan_in' preserves the magnitude of the variance of the wights in the forward pass.`

`choosing 'fan_out' preserves the magnitues in the backward pass(, which means matmul; with transposed matrix)`

➡️ in the other words, torch use fan_out cz pytorch transpose in linear transformaton.

- What about CNN in Pytorch?

I tried and 
![](/assets/images/2-forward-conv2d.jpg){:height="90%" width="90%"}

jeremy said

![](/assets/images/2-torch-nn-conv-jeremy.jpg){:height="90%" width="90%"}

![](/assets/images/2-torch-nn-conv-jeremy-2.jpg){:height="90%" width="90%"}<br />
[^2]

- in Pytorch, it doesn't seem to be implemented kaiming init in right formula. so we should use our own operation.
- But actually, this has been discussed in Pytorch community before.[^3]  [^4]
- Jeremy said it enhanced variance also, so I sampled 100 times and counted better results.

![](/assets/images/2-100times-sampling.png)


- To make sure the shape seems sensible. check with assert. (remember we will replace 1 to 10 in *cross entropy*)

~~~python
assert model(x_valid).shape==torch.Size([x_valid.shape[0],1])
>>> model(x_valid).shape
(10000, 1)
~~~

- We have made Relu, init, linear, it seems we can forward pass
- code we need for basic architecture

~~~python
nh = 50
def lin(x, w, b): return x@w + b;
w1 = torch.randn(m,nh)*math.sqrt(2./m ); b1 = torch.zeros(nh)
w2 = torch.randn(nh,1); b2 = torch.zeros(1)

def relu(x): return x.clamp_min(0.) - 0.5
t1 = relu(lin(x_valid, w1, b1))

def model(xb):
    l1 = lin(xb, w1, b1)
    l2 = relu(l1)
    l3 = lin(l2, w2, b2)
    return l3
~~~

#### 2.2 Loss function: MSE

#### 2.3 Gradients backward pass

### 3. Refactor model
#### 3.1 Layers as classes
#### 3.2 Modue.forward()
#### 3.3 nn.Linear and nn.Module

~~~python
~~~

~~~python
~~~

### Reference

[^1]: [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/abs/1901.09321)
[^2]: [Pytorch implementaion on Kaiming init of conv and linear layers](https://github.com/pytorch/pytorch/blob/3df79f403e8b9621d5adb0447266becd10d633b0/torch/nn/modules/linear.py#L58-L63)
[^3]: [Pytorch kaiming init issue](https://github.com/pytorch/pytorch/issues/15314)
[^4]: [Pytorch kaiming init explained](https://discuss.pytorch.org/t/why-the-default-negative-slope-for-kaiming-uniform-initialization-of-convolution-and-linear-layers-is-5/29290)

