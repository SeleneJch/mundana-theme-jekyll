---
layout: post
title: "doing simple comparison with HMM & Viterbi and Naive Bayes & Logistic regresson"
author: dionne
image: /assets/images/nb-crf.png
---
 
  Above
(Lafferty et al, 2001)

<span style="color:blue">This document will be translated into English soon.</span>


## HMM vs Viterbi에 관하여

- HMM은 Model이다.
	- sequence length가 정해졌다고 가정했을 때(사실 이것도 큰 제약이다), 각각 node에 들어갈 수 있는 단어는 K다.
	- 하지만 이 모델에서는 그 수가 S라 가정했다.
	- 하지만 우리는 이 S를 가정했을 뿐 알 수 없다.(그래서 이 모델은 Hidden이다.)
	- 동시에 Bayes rule을 적용해 state에 의해 output이 결정된다고 가정했다. (그래서 이 모델이 Bayesian Network ,directed graphical model라고도 불리는 것)
	- 여기서 state는 latent variable이다. (추적해야 할 path의 수가 $K^K$에서 $S^K$로 줄었다.
	- 또한 Markov assupmtion은 Time invariant 또한 가정했다. (그래서 transition table은 그냥 double들로 채울 수 있다.)

- viterbi는 algorithm이다.
	- viterbi는 그래서 무엇이 가장 그럴듯한, 최적의, 확률이 높은 observation sequence냐, 그것을 어떻게 찾느냐에 대한 해답중 하나다.
	- 보통 이것과 병행해서 배우는 것이 forward-backward algorithm. 하지만 이 알고리즘은 individually most likey를 구해서 가장 큰 값을 내놓기 때문에 global값을 구해주지 못한다.
	- 이에 대한 대한이 viterbi algorithm.
	- dynamic programming을 이용해서 path(즉 sequence에 대한 prob라 할 수 있다.)를 저장하는 것.

\#todo

- then, what relation has HMM to graphical models?

---

## Naive Bayes vs Logistic Regression에 대하여

<!--최근 유행하는 CRF(conditional Markov random field) 모델, CM(markov)RF 등등 여러 개념이 섞여있음.-->

- Naive Bayes
	- 기본적으로 베이즈 정리는 다음과 같다. $:p(C_k \mid \mathbf{x}) = \frac{p(C_k) \ p(\mathbf{x} \mid C_k)}{p(\mathbf{x})}$
	- 보이는 바와 같이 x는 Represented vector, $C_k$는 classify 하고자 하는 대상. 따라서 generative가 되며 이것이 sequential하고 (+다른 제약도 가지면 HMM이 된다.)
	- 하지만 이 친구는 Logistic regression과 달리 [conditional independence](https://en.wikipedia.org/wiki/Conditional_independence) 조건을 갖고 있다.
	- 왜? 확률사건이니까 더해서 1이 나와야함. 부모가 재산을 물려줄 때 다 합쳐봤자 자기 재산이다(즉 총합이 1, 확률이므로.)
	- 자식들끼리 나중에 추후에 지지고 볶고 부모가 물려준걸로 자기들끼리 공증서고(?) 영향 받지 않는다고 이 모델에서 가정 했음. 
	- 따라서, GENERAL GRAPHS에 Generative directed models를 보면 보면, 부모 노드에서 나온 자식 노드끼리는 서로 연결되지 않는다. 반면에 conditional 한 General CRFs는 연결되어있다.
	- 이게 Logistic regression에서 나온 애들(아래로 나온 conditioanl한 화살표 있는 것) 과 가장 큰 차이

	
	
### 일반적으로 쓰는 Notions used in the HMM 
- (ref: Manning C.D., Schütze H - Foundations of Statistics)
   
Set of states&nbsp;&nbsp;$S=\{ s_{1}, ... , s_{N} \}$

Output alphabet&nbsp;&nbsp;$K=\{k_{1}, ... , k_{M}\} = \{1, ... ,M\}$

Intial state probabilities&nbsp;&nbsp;$\Pi = \{\pi_{i}\}, i \in S $

State transition probabilities&nbsp;&nbsp;$ A =\{a_{ij}, i,j \in S \}$

Symbol emission probabilities&nbsp;&nbsp;$ B = \{b_{ijk} \}, i,j \in S, k \in K$

State sequence&nbsp;&nbsp;$ x = (X_1, ... , X_{T+1})$  &nbsp;$X_{t}\ :\ S \mapsto \{1, ...,N\}$

Output sequence&nbsp;&nbsp;$O =(o_1, ... , o_T)\ o_t \in K$

---	