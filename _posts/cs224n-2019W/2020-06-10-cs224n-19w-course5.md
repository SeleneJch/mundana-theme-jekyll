---
layout: post
title: "CS224N 2019: Lecture5 note & Assignment 3 Solution"
author: dionne
categories: [ cs224n ]
image: assets/images/cs224n/gate-l5.png
---

# Lecture 5: Dependency Parsing


Table of contents
---

- [Consistency and Dependency](#1\)-consistency-and-dependency)
{:toc}

## Contents
{: .no_toc}


"We will focus more on how human language is made and used"

**HW3:** Dependency parsing and neural network foundations<br/> you can find material: [code](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a3.zip) [handout](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a3.pdf)

## 1) Consistency and Dependency

1) Phrase Structure [Grammar]<br/>
organizes words into **nested constitutes**

- Phrase structure examples
	- NP -> DET + (ADJ) + N + PP
	- PP -> Prep + NP

- Different languages have different phrase structure.

2) Dependency Structure
<br/>: which words *depend on*(modify / arguments) which other words

Note: Having study with TWIML community, folks wondered why this is 'Context free' [^1]

**Why this syntactic structure is important?**

1) Prepositional phrase attachment **ambiguity**

`San Jose cops kill man with knife`

1. `with knife` modifies `man`
2.  `with knife` modifies `San Jose cops kill man`

`The board approved [its acquisition] [by Royal Trustco Ltd] [of Toronto] [for $27 a share] [at its monthly meeting].`
<br/>
<br/>

![](/assets/images/cs224n/l5-pp.PNG)

[Catalan numbers](https://en.wikipedia.org/wiki/Catalan_number) $$C_n = \frac{2n!}{(n+1)!n!}$$

- An Exponentially growing series, which arises in many tree-like contexts:
- e.g., the number of possible triangulations of a polygon with n+2 sides
	- turns up in triangulation of probabilistic graphical models(cs228)...

2) Coordination scope ambiguity

`Doctor: No heart, cognitive issues`

1. No [heart and/or cognitive] issues
2. [No heart] and [cognitive issues]

3) Adjectival Modifier Ambiguity

`Students get first hand job experience`

1. Students get [[first hand] [job] experience]
2. Students get [[first [hand job] experience]

4) Verb Phrase (VP) attachment ambiguity

`Mutilated body washes up on Rio beach to be used for Olympics beach volleyball`

1. Mutilated body washes up on [Rio [beach to be used for Olympics beach volleyball]]
2. Mutilated body [washes up on Rio beach] to be used for Olympics beach volleyball

## 2) Dependency Grammar

Two ways of representing the dependency structure of sentence

1. in a line and drawing arrows
2. tree structure

Generally, people type arrows with the name of grammatical relationships, but in this case we will not go that far ahead.

Dependency tree is acyclic, single-head, connected.

- Dependency Grammar/Parsing history

## 2) Treebanks
{:no_toc}

[Universal Dependencies](http://universaldependencies.org/), cf (#todo fill out)

People annotated structure dependencies heuristically (And this project handles with many languages other than English)

### Dependency Conditioning Preferences (some dependency rules)

1. Only one word is a dependent of ROOT
2. Don't make it cyclic.
3. in little case, bootstrapping happens(when parse becomes non-projective)
	- will comment further at next class

## 3) Arc-standard transition-based parser

Shift `Buffer`'s element to left `Stack`, until stack finds `Head` component.
<br/>-> When you find head at stack, start to reduce
<br/>-> but notice not to reduce `head` to `root` until buffer has no element.

- Left Arc means reduction of left argument, and keep head
- Right Arc means reduction of right argument, and keep head.
- Finish condition is when buffer is empty, and stack has only one element, which is `[root]`


But the problem of this method is how we choose the next step is uncertain.<br/>
Usually these problem was handled using Dynamic programming to avoid too much selection cases.

## 4) Neural dependency parsing

1) MaltParser

[Nivre and Hall 2005]

built ML classifier,

$$Accuracy = \frac{\#\ correct\ deps}{\#\ of\ deps}$$

UAS = 4 / 5 = 80% (exclude label)
<br/>LAS =  2 / 5 = 40% (include label)

2) Conventional Feature Representation was very complex.

- incomplete
- **expensive computation** (critical problem)
- sparse

3) Neural dependency parsers

[Chen and Manning 2014]

* `sent. / s` means number of sentences that algorithm can perform.
* Not only fastest methods, but also accuracy gets almost highest.
* the dense representation is the key 
* Used **treebanks**

4) Further researches

done by Google

- Using bigger, deeper networks, tuned hyperparameters
- Beam search
- Global inference, CRF over the decision sequence

## 5) Solution of Assignment3

### 1. Machine Learning & Neural Networks (8 points)

(a) Adam Optimizer

i. Momentum is working as acceleration reflecting the previous gradients tendency.<br/>
Hight variance is well known to raising overfitting problem.

ii. Parameters that previous gradients are small and not volatile get larger updates.<br/>
This helps model to handle with sparse gradients(merits of AdaGrad) and also non-stationary objectives(merits of RMSProp)

(b) Dropout

i. $$\gamma = \frac{1}{1-P_{drop}}$$

![](/assets/images/cs224n/a3-1-b.jpg)

ii. Drop out is one of the regularizations, which restrict an overfitting. Randomly setting units zero is for make units stronger on abrupt absence of other units.

And evaluation is made from train, to model being improved. But if we use drop out also at evaluation, it does re-trained with same circumstances, so that can't do well in test.


### 2. Neural Transition-Based Dependency Parsing (42 points)

(a)


|Stack|Buffer|New dependency|Transition|
|-|-|-|-|
|[ROOT]|[I, parsed, this, sentence, correctly]||`Initial`|
|[ROOT]|[I, parsed, this, sentence, correctly]||`SHIFT`|
|[ROOT, I]|[parsed, this, sentence, correctly]||`SHIFT`|
|[ROOT, I, parsed]|[this, sentence, correctly]|parsed$\rightarrow$|`LEFT-ARC`|
|[ROOT, parsed]|[this, sentence, correctly]||`SHIFT`|
|[ROOT, parsed, this]|[sentence, correctly]||`SHIFT`|
|[ROOT, parsed, this, sentence]|[correctly]|sentence$\rightarrow$this|`LEFT-ARC`|
|[ROOT, parsed, sentence]|[correctly]|parsed$\rightarrow$sentence|`RIGHT-ARC`|
|[ROOT, parsed]|[correctly]||`SHIFT`|
|[ROOT, parsed, correctly]|[]|parsed$\rightarrow$correctly|`RIGHT-ARC`|
|[ROOT, parsed]|[]|ROOT$\rightarrow$parsed|`RIGHT-ARC`|
|[ROOT]|[]||


(b)

n steps ($\because$ one-to-one mapping)

(c), (d), (e)

<script src="https://gist.github.com/SpellOnYou/52755d046fc8be9aa2ac2a127ab5fe3c.js"></script>

Tips: problem 2-d needs shallow copy, not deep copy and TA advised not to use `del` function, here is good explaination [remove vs del](https://stackoverflow.com/a/45572488/7934832)
![](/assets/images/cs224n/a3.png)

(f)

i.

- **Error type**: Verb Phrase Attachment Error
- **Incorrect dependency**: wedding → fearing
- **Correct dependency**: heading → fearing

ii.

- **Error type**: Coordination Attachment Error
- **Incorrect dependency**:  rescue → and
- **Correct dependency**: rescue → rush

iii.

- **Error type**: Prepositional Phrase Attachment Error
- **Incorrect dependency**:  named → Midland
- **Correct dependency**: guy → Midland

iv. 

- **Error type**: Modifier Attachment Error
- **Incorrect dependency**: element → most
- **Correct dependency**: crucial → most


[^1]: D. Jurafsky and J. H. Martin, *Speech and Language Processing*, 2/e, Prentice Hall, 2008.

