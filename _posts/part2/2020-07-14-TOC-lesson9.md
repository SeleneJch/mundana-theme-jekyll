---
layout: post
title: "Part2 lesson 9, 03 minibatch training | fastai 2019 course -v3"
author: dionne
categories: [ fastai.v3]
image: assets/images/fastai/03.png
tags: [ feature ]
---

Since fastai instructor including many people who studied fastai highly emphasized to copy the Jeremy's code from scratch, I've been using that approach.<br/>
It was fine with Part1, but from Part2, applying same strategy which was `trying to copy nbs from blank cell without peeking up original` was not enough.<br/>First, there were lots of sub-topics worth to dig in, since we've moved to bottom-up phase. And I couldn't inspect deep inside of each topic(sometimes even don't know what I am doing) with just replicating same code.<br/>Second, literally it was too difficult to rely on my own memory. I should open and close up the Jeremy's too many times.<br/>I thought if I could draw overall picture of each notebook, replicating process would be easier. And it was. (At least with `01_matmul`, `02_fully_connected`, `02b_initializing`)<br/>So I share my own supplementary hoping that it could help you to replicate code easily and grasp the lesson Jeremy intended.

---

I learned how to re-define our NN model and update training replicating code 

Original notebook [link](https://github.com/fastai/course-v3/blob/master/nbs/dl2/03_minibatch_training.ipynb)

* [Initial Setup](#initial-setup)
{:toc}


## Initial Setup

### Data

1. Fastai for Colab env
2. import data
	* Hyper parames
		* Number of input size = data size
		* Number of output unit = value
		* Number of hidden unit
		* Number of hidden size = 1
	* Initialize the model ***(1st)*** with which gets initialized and do the forward.

### Loss function: Cross entropy loss

1. Softmax
	* Write down the formula of softmax
	* Why do we need this softmax function at the last layer?
	* Write down softmax as code.
	* Draw the softmax probability function using matplotlib
	* Why do I need a log of softmax?

2. Cross Entropy
	* Write down the formula of cross entropy
	* Draw the function using matplotlib
	* Why is this function adequate to the loss of categorical variables?

3. Negative log likelihood function
	* Why `-sum( x * log( p(x) ) )` has the name of negative log likelihood?
	* Why do we find a `negative` value instead of likelihood function? (Hint: Related to finding arg min value) [^1]
4. LogSumExp
	* What is LogSumExp
	* Why this is called trick somehow?

## Basic Training Loop

1. Write down the procedure the training loop repeats (4 steps)
2. Define the accuracy function.
3. Do the above step for an epoch with model defined at setting ***(1st training loop)***
	* Get the cross entropy function from PyTorch and use it at 1 epoch process
	* You don’t have to do this for reversed sequence since you’ve already got gradients from PyTorch.
	* Don’t forget to check the existence of attribute before you update and zero it before you get out of loop 
4. Print the loss and accuracy of one batch [^2]

## Using parameters and optimal
### Parameters

> we will use *nn.Module.__setattr__* and move relu to functional. [^3]

1. Re-define Model class using nn.Module. ***(2nd Model)***
2. What’s difference from that you made first?
3. See the layers inside model using `named_children` method
4. see one layer
5. Re-define `fit` function with using parameters, not layers. ***(2nd Training Loop)***
	* What’s difference between nn.Module’s layer / parameters?
	* Why this diff makes shorter code?
6. Make `DummyModule` class to simulate pytorch’s `__setattr__` function ***(3rd Model)***
	* 3 dunder method
		* Init
		* setattr
		* repr
	* parameter function which generates each parameters in the layers
	* What does the setattr do here?
		* How can i check if I defined parameters properly?
	* What does the repr do here?
7. Call the instance of dummymodule and see repr and shape of parameters in instance

### Registering modules

1. Re-define Model class ***(4th Model)***
	* Use original layers approach
	* Register the modules
2. What is `registering modules`?
3. See the model instance (i.e. repr)

### nn.ModuleList
1. Define class SequentialModel ***(5th Model)***
	* Use nn.ModuleList
2. What does nn.ModuleList do for us? (hint: fit, loss and accuracy function)

### nn.Sequential

1. Make model instance using nn.Sequential  ***(6th Model)***
2. Why does this class make jobs easier?

### Optim

1. Define class Optimier
	* Init - params, lr
		* Why do we add list of parameters at learning rate?
	* step
	* zero_grad
	* Why do we have to no_grad() at step and zero_grad function?
2. Do the one epoch learning with optimizer instance ***(3rd Training Loop)***
3. See the loss and accuracy [^4]
4. Do the one epoch learning using PyTorch’s optim.SGD functionality ***(4th Training Loop)***
	* Define get_model function
	* return: (1) model instance from nn.Sequential, (2) Optimizer function from optim.SGD
	* See the loss and accuracy [^5]

## Dataset and DataLoader


### Dataset

### DataLoader

### Random sampling

### PyTorch DataLoader

## Validation

## Unsolved questions

[^1]: I’ve heard that finding convex downward / upward is not the same and there are preferences. Find out which is easier and explain why.
[^2]: What’s relationship with above one epoch trainiing and this one batch loss / accuracy?
[^3]: Dunder init including __setattr__
[^4]: I don’t know if I’m getting better loss since I’m using better approach or doing same thing at same data repeatedly -> shallow: check I would get same result when I run this cell before others deep: inspect details of mechanisms each approach uses
[^5]: What does it mean ‘except we’ll be doing it in a more flexible way!’?
