---
layout: post
title: "Part2 lesson 9, 04 callbacks | fastai 2019 course -v3"
author: dionne
categories: [ fastai.v3]
image: assets/images/fastai/03.png
tags: [ feature ]
---

Since fastai instructor including many people who studied fastai highly emphasized to copy the code from scratch, I've been using that approach.<br/>
It was fine with Part1, but from Part2, applying same strategy which was `trying to copy nbs from blank cell without peeking up original` was not enough.<br/>First, there were lots of sub-topics worth to dig in, since we've moved to bottom-up phase. And I couldn't inspect deep inside of each topic(sometimes even don't know what I was doing) only to replicating the code. It would be okay with part1 session, but seemed insufficient with part2<br/>Second, literally it was too difficult to rely on my own memory. I should have open and close up the Jeremy's too many times.<br/>So, I thought it would be great if I could draw overall picture of each notebook, making replicating process easier. And it was. (At least with `01_matmul`, `02_fully_connected`, `02b_initializing`, `03_minibatch_training`)<br/>So I'd like to share my own supplementary, hoping it could help you to replicate code much easily and grasp the lesson that Jeremy intended.

---

Original notebook [link](https://github.com/fastai/course-v3/blob/master/nbs/dl2/04_callbacks.ipynb)

My replicated Notebook [Link]()

* [Initial Setup](#initial-setup)
{:toc}


## Initial Setup

1. Fastai for Colab env

2. Hyper parames
	1. Number of output unit = value max
	1. Number of hidden unit = 50
	1. Number of hidden size = 1
	2. Number of batch size = 64

3. loss function - cross entropy [^1]

4. train_ds, valid_ds - instance from `Dataset()` class

## DataBunch / Learner

To make our model polymorphic, we will use factor out the arguments of `fit` function.

`(epochs, model, loss_fun, opt, train_dl, valid_dl)` -> `(const:epoch, learner)` [^3]

1. Make `Databunch` class
	1. instance initializes with dataloader(s) and label
	2. make train_ds and valid_ds which are `getter method` [^2]
	2. make instance of databunch class.
2. re-define `get_model` function
	1.  params - databunch, learning rate, hidden layer nods
	2.  retur model and optimizer using arguments

3. make class `Learner`
	1. instance initializes with model, optimizer, loss function, data bunch instance

4. re-define fit function
	1. parameters - epochs, learner object
	2. similar except now this fit training uses learner's attribute to access data/parameters
		1. refer [03_minibatch_training]() code
	3. retirm avg loss, acc (* suppose batch size is not variant)

## CallbackHandler

(Issue: I'm still trying to grasp overall content, so the contents would be in mess)

\* Not satisfying `if not` is normal. (i.e. satisfying if not condition means something bad happened) [^4]

1. function
	1. one_batch
	2. all_batches
	3. fit 
2. `class Callback()`
3. `class CallbackHandler()`
4. `class TestCallback(Callback)`

## Runner

## Notes

[^1]: See the document of [torch](https://pytorch.org/docs/stable/nn.html#torch.nn.Functional)

[^2]: why this code need `@property` decorator?

[^3]: why are `factored out` arguments problem?

[^4]: See [the questions]() from lesson09