---
layout: post
title: "fastai 2020 course-v4 Part2, lesson09"
author: dionne
categories: [ FastAI-v3]
image: assets/images/cnn-jiwon.png
tags: [ feature ]
---

## CONTENTS
{: .no_toc}

- [02a why sqrt5](#02a_why_sqrt5.ipynb)
{:toc}

# 02a_why_sqrt5.ipynb

[course notebook link](https://github.com/fastai/course-v3/blob/master/nbs/dl2/02a_why_sqrt5.ipynb) <br/>

convolution layer needs square image.<br/>
as a software developer, be sure to keep refactoring what you've implemented. for example, jeremy made stats function<br/>
**todo** [^1] when you don't understand why the weight shape is `32, 1, 5, 5`

~~~python
torch.nn.modules.conv._ConvNd.reset_parameters??
~~~

check pytorch, they don't handle with default relu ( this could be little different since video was taken at July, 2019)<br/>
PyTorch doesn't handle well with initialization.

~~~python
li = nn.Conv2d(1, nh, 5)
rec_fs = li.weight[0,0].numel()
~~~

this way we can get the size of receptive filter, number of element.

How we calculate the receptive fan_in and fan_out at convolutional layer.<br/>

function gain(0), gain(0.1), ... , gain(math.sqrt(5.)) <- pytorch's doing with<br/>
and also they are using `kaiming_unit` not `normal`<br/>

And jeremy compared his function and pytorch function.<br/>

# 02b_initializing.ipynb

[course notebook link](https://github.com/fastai/course-v3/blob/master/nbs/dl2/02b_initializing.ipynb)


You are out of keep tracking the numbers(=parameters) that matter <br/> 

Some tricky initialization researches and methods would make your own experiment hard. [video](https://youtu.be/AcA8HAYh7IE?t=1590) <br/>

shape of the weight was up-side down, and it was because of some code which was 7 years code. Lua library things. <br/>

# 03_minibatch_training.ipynb 

## Changing mse -> cross entropy

first, we will change mse to cross_entropy since it's categorical variable.

[integer array indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html#integer-array-indexing)

note how novel jeremy's nll function is </br>

logsumexp trick(numerical stability trick) : when you are handling the number which is very big, <br/>

log softmax in negative log likelihood is called cross entropy. <br/> 

<br/>
 
## refactoring nn.module

1. manually going through weight and bias
2. grab all of the parameters of model at once -> DummyModule() will do that

dunder `__setattr__` : will call this method, when you assigned anything inside self, 

[^2] the attribute name doesn't start with underscore,/  because if it does, it might be `\_modules` and then it's going to be like a recursive loop. and also it's not some internal private stuff.
 
And these refactoring things are same with `nn.Module` 

Q1: what code Jeremy exchanged to `super().__init__()` in `SequentialModel` class? <br/>
Q2: what code Jeremy exchanged to `nn.ModuleList(layers)` <br/>
Q2: what code Jeremy exchanged to `nn.SequentialModel(layers)` <br/>

And if you look on source code, you will find that we didn't dumbed down the code <br/>

## Refactoring nn.optim

`PyTorch's optim.SGD ` is doing some significant things including ***weight_decay, momentum, dampening, nestrov***

Two thing that you can learned from his interactive data science experiments.

1. when you architect your model, put things that should be checked
	- even SGD is theoretically imperfect
	- Jeremy used accuracy here, to check those things.
2. 	didn't use seed (very intentionally)
	-  we can observe variations when we run the model different times
	-  reproducible science
	-  BUT, not on your ***model*** when you want to develop intuitive understanding of your model!

## Dataset and DataLoaders

To avoid iterate separately through mini batches of x and y values!


[current pause](https://youtu.be/AcA8HAYh7IE?t=3608)


<br/>
---

[^1]: Study `conv-example.xlsx' excel

[^2]: https://youtu.be/AcA8HAYh7IE?t=2996