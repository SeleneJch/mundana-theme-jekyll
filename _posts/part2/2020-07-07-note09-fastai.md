---
layout: post
title: "fastai2019 v3 | Part2 lesson 9"
author: dionne
categories: [ fastai.v3]
image: assets/images/cnn-jiwon.png
tags: [ feature ]
---

## CONTENTS
{: .no_toc}

- [02a why sqrt5](#02a_why_sqrt5.ipynb)
{:toc}

# 02a_why_sqrt5.ipynb

[course notebook link](https://github.com/fastai/course-v3/blob/master/nbs/dl2/02a_why_sqrt5.ipynb) <br/>

convolution layer needs square image.<br/>
as a software developer, be sure to keep refactoring what you've implemented. for example, jeremy made stats function<br/>
**todo** [^1] when you don't understand why the weight shape is `32, 1, 5, 5`

~~~python
torch.nn.modules.conv._ConvNd.reset_parameters??
~~~

check pytorch, they don't handle with default relu ( this could be little different since video was taken at July, 2019)<br/>
PyTorch doesn't handle well with initialization.

~~~python
li = nn.Conv2d(1, nh, 5)
rec_fs = li.weight[0,0].numel()
~~~

this way we can get the size of receptive filter, number of element.

How we calculate the receptive fan_in and fan_out at convolutional layer.<br/>

function gain(0), gain(0.1), ... , gain(math.sqrt(5.)) <- pytorch's doing with<br/>
and also they are using `kaiming_unit` not `normal`<br/>

And jeremy compared his function and pytorch function.<br/>

# 02b_initializing.ipynb

[course notebook link](https://github.com/fastai/course-v3/blob/master/nbs/dl2/02b_initializing.ipynb)


You are out of keep tracking the numbers(=parameters) that matter <br/> 

Some tricky initialization researches and methods would make your own experiment hard. [video](https://youtu.be/AcA8HAYh7IE?t=1590) <br/>

shape of the weight was up-side down, and it was because of some code which was 7 years code. Lua library things. <br/>

# 03_minibatch_training.ipynb 

## Changing mse -> cross entropy

first, we will change mse to cross_entropy since it's categorical variable.

[integer array indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html#integer-array-indexing)

note how novel jeremy's nll function is </br>

logsumexp trick(numerical stability trick) : when you are handling the number which is very big, <br/>

log softmax in negative log likelihood is called cross entropy. <br/> 

<br/>
 
## refactoring nn.module

1. manually going through weight and bias
2. grab all of the parameters of model at once -> DummyModule() will do that

dunder `__setattr__` : will call this method, when you assigned anything inside self, 

[^2] the attribute name doesn't start with underscore,/  because if it does, it might be `\_modules` and then it's going to be like a recursive loop. and also it's not some internal private stuff.
 
And these refactoring things are same with `nn.Module` 

Q1: what code Jeremy exchanged to `super().__init__()` in `SequentialModel` class? <br/>
Q2: what code Jeremy exchanged to `nn.ModuleList(layers)` <br/>
Q2: what code Jeremy exchanged to `nn.SequentialModel(layers)` <br/>

And if you look on source code, you will find that we didn't dumbed down the code <br/>

## Refactoring nn.optim

`PyTorch's optim.SGD ` is doing some significant things including ***weight_decay, momentum, dampening, nestrov***

Two thing that you can learned from his interactive data science experiments.

1. when you architect your model, put things that should be checked
	- even SGD is theoretically imperfect
	- Jeremy used accuracy here, to check those things.
2. 	didn't use seed (very intentionally)
	-  we can observe variations when we run the model different times
	-  reproducible science
	-  BUT, not on your ***model*** when you want to develop intuitive understanding of your model!

## Dataset and DataLoaders

To avoid iterate separately through mini batches of x and y values! <br/>

Dunders <br/>

### DataLoader

initialize class with datasize and batchsize <br/>
<br/>
Use dunder `iter`
<br/>
coroutine function (when jeremy tried to explain iteration
<br/>
> To the people who feels these processes are so hard
<br/>
Maybe you want to read the code very intuitively, and refactor it like expert, <br/> but it's very hard to maintain and understand the code. And also this ability is key to be a researcher, because if you want try something, you should know how to do it.
<br/>

### Random sampling

one of the remained problem is we see the training data are in order, meaning at every epoch, we are giving our model the exactly same bunch of our data with previously given.
<br/>
Sampler class, which has init and iter.

- this structure can be used often when you're doing streaming computations (so that you out of memory), way of looping through something which is itself for co-routine, and then yielding something which does other things to that.

<br/>

If you want to use different stacked version of tensor (e.g. padded tensor) you can use different kind of collate function. 
<br/>

## Pytorch DataLoader

which does exactly same thing that we've done
<br/>
But unfortunately, they don't have function that grasp just one sample of data. but basically our api and pytorch are doing same thing.
<br/>
One thing we didn't implemented which is in pytorch, `num_workers` [^3]
<br/>

## Validation

Don't forget to call `model.train()` / `model.eval()` before you start train/validation, since technique like batchnorm, dropout should be applied to training only, not validation/evaluation data. [^4]
<br/>
Also, we don't backward with validation data, but just keep tracking of accuracy and loss in batch.
<br/>
Question: Are these validation results correct if batch size varies? [Lecture video](https://youtu.be/AcA8HAYh7IE?t=4392)
<br/>
✔︎ Why this could be problem?
<br/>
we have 2 batches and one minibatch size of 1000, and the other size of 1, then you must add up with weighted average, not just dividing by 2!
<br/>
![figure1](/assets/images/l9-f1.jpeg)
<br/>
Be careful, lots of (almost most of) library does things like this without considering size of batch (of course fast.ai is doing well :wink)
<br/>
when you see the `get_dls` function, you will notice that batch size in validation data is twice of the train's -> Since we don't have to save our backward gradients at validation data set, we have much more space!

# 04_callbacks.ipynb

---

[^1]: Study `conv-example.xlsx' excel

[^2]: https://youtu.be/AcA8HAYh7IE?t=2996

[^3]: find related PyTorch document 

[^4]: Jeremy said if we don't separate train/valid, we will get awful results when test. DO IT BY YOURSELF!