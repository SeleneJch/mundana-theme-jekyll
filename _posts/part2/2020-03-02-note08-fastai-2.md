---
layout: post
title: "What's inside Pytorch Operator?"
author: dionne
categories: [ Fast.AI-v3 ]
image: assets/images/30.png
tags: [ feature ]
---


What's inside Pytorch Operator?
---

Section02

### Time comparison with pure Python
{: .no_toc}

- Matmul with broadcasting<br />
\> 3194.95 times faster

- Einstein summation<br />
\> 16090.91 times faster

- Pytorch's operator<br />
\> 49166.67 times faster

### 1. Elementwise op

#### 1.1 Frobenius norm

![](/assets/images/7.png) <br />

- above converted into <br /> 

~~~ python
(m*m).sum().sqrt()
~~~

- Plus, don't suffer from mathmatical symbols. He also copy and paste that equations from wikipedia.
- and if you need latex form, download it from archive.

### 2. Elementwise Matmul

- What is the meaning of elementwise?
- We do not calculate each component. But all of the component at once. Because, length of column of A and row of B are fixed.

- How much time we saved?

![](/assets/images/4.png)

- So now that takes 1.37ms. We have removed one line of code and it is a 178 times faster...

\#TODO
I don't know where the `5` from. but keep it.
Maybe this is related with frobenius norm...?
as a result, **the code before**

~~~ python
for k in range(ac):
    c[i,j] += a[i,k] + b[k,j]
~~~

the code after<br />

~~~ python
c[i,j] = (a[i,:] * b[:,j]).sum()
~~~

To compare it (result betweet *original* and *adjusted* version) we use not test_eq but other function. The reason for this is that due to rounding errors from math operations, matrices may not be exactly the same. As a result, we want a function that will ‚Äúis a equal to b **within some tolerance**‚Äù

~~~ python
#export
def near(a,b): 
    return torch.allclose(a, b, rtol=1e-3, atol=1e-5)

def test_near(a,b): 
    test(a,b,near)

test_near(t1, matmul(m1, m2))
~~~

### 3. Broadcasting

- Now, we will use the broadcasting and remove

~~~ python
c[i,j] = (a[i,:] * b[:,j]).sum()
~~~

- How it works?

~~~ python
>>> a=tensor([[10,10,10],
          [20,20,20],
          [30,30,30]])
>>> b=tensor([1,2,3,])
~~~
~~~python
>>> a,b
    
(tensor([[10, 10, 10],
         [20, 20, 20],
         [30, 30, 30]]),
tensor([1, 2, 3]))
         
~~~
~~~python
>>> a+b

tensor([[11, 12, 13],
        [21, 22, 23],
        [31, 32, 33]])

~~~         
![](/assets/images/3.png)
- <Figure 2> demonstrated how array **b** is broadcasting(or copied but not occupy memory) to compatible with **a**. *Refered from [numpy_tutorial][1]*

[1]: https://www.tutorialspoint.com/numpy/numpy_broadcasting.htm

- there is no loop, but it seems there is exactly the loop.

- This is not from jeremy (actually after a moment he cover it) but i wondered How to broadcast an array by columns?

~~~ python
c=tensor([[1],[2],[3]])
a+c
~~~

*tensor([[11, 11, 11],
        [22, 22, 22],
        [33, 33, 33]])*{::}s

---
        
- What is tensor.stride()?

~~~ python
help(t.stride)
~~~

*Help on built-in function stride:<br />
    stride(...) method of torch.<br />
Tensor instance<br />
stride(dim) -> tuple or int<br />
Returns the stride of :attr:'self' tensor.<br />
Stride is the jump necessary to go from one element to the next one in the specified dimension :attr:'dim'.<br />
A tuple of all strides is returned when no argument is passed in.<br />
Otherwise, an integer value is returned as the stride in the particular dimension :attr:'dim'.<br />*

Args:
    dim (int, optional): the desired dimension in which stride is required
Example::*

~~~ python
x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])`
x.stride()
>>> (5, 1)
x.stride(0)
>>> 5
x.stride(-1)
>>> 1
~~~

- unsqueeze & None index

- We can manipulate rank of tensor
- Special value 'None', which means please squeeze a new axis here<br />
**== please broadcast here**


~~~ python
c = torch.tensor([10,20,30])
c[None,:]
~~~

- in c, _squeeze a new axis in here please._


#### 2.2 Matmul with broadcasting

~~~ python
for i in range(ar):
#   c[i,j] = (a[i,:]).          *[:,j].sum() #previous
    c[i]   = (a[i].unsqueeze(-1) * b).sum(dim=0)
~~~

- And Using `None` also (As howard teached)

~~~ python
c[i]   = (a[i  ].unsqueeze(-1) * b).sum(dim=0) #howard
c[i]   = (a[i][:,None] * b).sum(dim=0) # using None
c[i]   = (a[i,:,None]*b).sum(dim=0)
~~~



‚≠êÔ∏èTipsüåü    
1) _Anytime there's a trailinng(final) colon in numpy or pytorch you can delete it_
  *ex) c[i, :] = c [i]*
2) any number of colon commas at the start, you can switch it with the single elipsis.
  *ex) c[:,:,:,:,i] = c [...,i]*

#### 2.3 Broadcasting Rules

- What if we `tensor.size([1,3]) * tensor.size([3,1])`?
~~~python
torch.Size([3, 3])
~~~
- What is scale????
- What if they are one array is `times` of the other array?
 <br /> ex) 
`Image : 256 x 256 x 3`<br />
`Scale : 128 x 256 x 3`<br />
`Result: ?`

- Why I did not inserted axis via None, but happened broadcasting? <br />

~~~python
>>> c * c[:,None]
tensor([[100., 200., 300.],
        [200., 400., 600.],
        [300., 600., 900.]])
~~~


maybe it broadcast cz following array has 3 rows<br />

as same principle, no matter what nature shape was, *if we do the operation* tensor broadcasts to the other.<br />

~~~python
>>> c==c[None]
tensor([[True, True, True]])

>>> c[None]==c[None,:]
tensor([[True, True, True]])

>>>c[None,:]==c
tensor([[True, True, True]])
~~~
 
### 3. Einstein summation

![](/assets/images/this-is-einsum.jpg)

- Creates batch-wise, remove inner most loop, and replaced it with an elementwise product
a.k.a

~~~python
c[i,j] += a[i,k] * b[k,j]
~~~
inner most loop

~~~python
c[i,j] = (a[i,:] * b[:,j]).sum()
~~~

elementwise product

- Because K is repeated so we do a dot product. And it is torch.


Usage of [einsum()](https://pytorch.org/docs/stable/torch.html#torch.einsum)
1) transpose
2) diagnalisation tracing
3) batch-wise (matmul)

...

- einstein summation notation

~~~python
def matmul(a,b): return torch.einsum('ik,kj->ij', a, b)
~~~

so after all, we are now 16000 times faster than Python.

### 4. Pytorch op

49166.67 times faster than pure python


And we will use this matrix multiplication in Fully Connect forward, with some initialized parameters and ReLU.

But before that, we need initialized parameters and ReLU, 

---

#### Footnote

- [TensorRank](http://mathworld.wolfram.com/TensorRank.html)
- [ti note](https://forums.fast.ai/t/forum-markdown-notes-lesson-8/41896)

## Resources

- Frobenius Norm Review
- Broadcasting Review (especially *[Rule](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#general-broadcasting-rules)*)
  - Refer [colab!](https://render.githubusercontent.com/view/ipynb?commit=2bfe4a95aac864b23bd0c0729d2720c92e169f8c&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f5370656c6c4f6e596f752f646c66662d6e6f74652f326266653461393561616338363462323362643063303732396432373230633932653136396638632f6e62732f646c322f30315f6d61746d756c5f70726163746963652e6970796e62&nwo=SpellOnYou%2Fdlff-note&path=nbs%2Fdl2%2F01_matmul_practice.ipynb&repository_id=221963162&repository_type=Repository#Broadcasting) (I totally confused with extension of arrays)
- torch.allclose Review
- [np.einsum Review](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html)

h