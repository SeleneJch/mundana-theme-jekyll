<h3 class="no_toc" id="contents">CONTENTS</h3>

<ul id="markdown-toc">
  <li><a href="#creating-dataset-from-google-images" id="markdown-toc-creating-dataset-from-google-images">Creating dataset from google images</a>    <ul>
      <li><a href="#using-google_images_download" id="markdown-toc-using-google_images_download">Using <code class="highlighter-rouge">google_images_download</code></a></li>
      <li><a href="#create-imagedatabunch" id="markdown-toc-create-imagedatabunch">Create ImageDataBunch</a></li>
    </ul>
  </li>
  <li><a href="#train-model" id="markdown-toc-train-model">Train model</a>    <ul>
      <li><a href="#fit_one_cycle" id="markdown-toc-fit_one_cycle">fit_one_cycle()</a></li>
      <li><a href="#lets-find-tune" id="markdown-toc-lets-find-tune">Let’s find-tune</a></li>
      <li><a href="#lets-train-the-whole-model" id="markdown-toc-lets-train-the-whole-model">Let’s train the whole model!</a></li>
      <li><a href="#lets-make-batch-size-bigger" id="markdown-toc-lets-make-batch-size-bigger">Let’s make batch size bigger!</a></li>
    </ul>
  </li>
  <li><a href="#interpretation" id="markdown-toc-interpretation">Interpretation</a></li>
  <li><a href="#model-in-production" id="markdown-toc-model-in-production">Model in production</a></li>
</ul>

<p>Code can be found <a href="https://github.com/SpellOnYou/dlff-note/blob/master/toy-project/What_is_your_city_lesson02.ipynb">here</a><br />
Deployed model <a href="https://my-city-classifier.onrender.com/">here</a></p>

<p>Making a classifier which can distinguish <span style="color: red">Seoul</span> from <span style="color: red">Munich</span> and <span style="color: red">Sanfrancisco</span>!
(hoping my well in Munich!)</p>

<h3 id="creating-dataset-from-google-images">Creating dataset from google images</h3>

<p>In machine learning, you always need data before you build your model.</p>

<p>You can use either URLs or <code class="highlighter-rouge">google_images_download</code> package. Since <a href="">Jeremy explained specifically</a>, I will try the other.</p>

<h4 id="using-google_images_download">Using <code class="highlighter-rouge">google_images_download</code></h4>

<p>note: <em>This is not google official package</em> <br /></p>

<p>Refer to <a href="https://google-images-download.readthedocs.io/en/latest/index.html">Official Doncument</a>, put that arguments.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">google_images_download</span> <span class="kn">import</span> <span class="n">google_images_download</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">google_images_download</span><span class="o">.</span><span class="n">googleimagesdownload</span><span class="p">()</span>   <span class="c1">#class instantiation
</span><span class="n">out_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s">'../../materials/dataset/pkg/'</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">out_dir</span><span class="p">)</span>
<span class="n">arguments</span> <span class="o">=</span> <span class="p">{</span><span class="s">"keywords"</span><span class="p">:</span><span class="s">"Cebu,Munich,Seoul"</span><span class="p">,</span>
             <span class="s">"print_urls"</span><span class="p">:</span><span class="bp">True</span><span class="p">,</span>
             <span class="s">"suffix_keywords"</span><span class="p">:</span><span class="s">"city"</span><span class="p">,</span>
             <span class="s">"output_directory"</span><span class="p">:</span><span class="n">out_dir</span><span class="p">,</span>
             <span class="s">"type"</span><span class="p">:</span><span class="s">"photo"</span><span class="p">,</span>
            <span class="p">}</span>
<span class="n">paths</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">arguments</span><span class="p">)</span>   <span class="c1">#passing the arguments to the function
</span><span class="k">print</span><span class="p">(</span><span class="n">paths</span><span class="p">)</span>
</code></pre></div></div>

<p>and if you need, here is <a href="https://github.com/hardikvasa/google-images-download/blob/master/google_images_download/google_images_download.py">main</a> code.</p>

<h4 id="create-imagedatabunch">Create ImageDataBunch</h4>

<p>We need to separate validation set because we just <em>grabbed</em> these imagese from Google.<br /> Most of the dataset we use (kaggle/research) splited into <code class="highlighter-rouge">train / validation / test</code><br /> so if they are not devided beforehand we should make databunch, and Jeremy recommended assign 20% to validation.<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Help</span> <span class="n">on</span> <span class="n">function</span> <span class="n">verify_images</span> <span class="ow">in</span> <span class="n">module</span> <span class="n">fastai</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">data</span><span class="p">:</span>

<span class="n">verify_images</span><span class="p">(</span><span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">delete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">max_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">max_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">dest</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s">'.'</span><span class="p">,</span> <span class="n">n_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">interp</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ext</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">img_format</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">resume</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">Check</span> <span class="k">if</span> <span class="n">the</span> <span class="n">images</span> <span class="ow">in</span> <span class="sb">`path`</span> <span class="n">aren</span><span class="s">'t broken, maybe resize them and copy it in `dest`.</span><span class="err">
</span></code></pre></div></div>

<p>Data from <strong>google image url</strong></p>

<p><img src="/assets/images/02-url.png" alt="" /></p>

<p>Data from <strong>package</strong></p>

<p><img src="/assets/images/02-pkg.png" alt="" /></p>

<h3 id="train-model">Train model</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">len(class)</th>
      <th style="text-align: center">len(train)</th>
      <th style="text-align: center">len(valid)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Data_url</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">432</td>
      <td style="text-align: center">108</td>
    </tr>
    <tr>
      <td style="text-align: center">Data_pkg</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">216</td>
      <td style="text-align: center">53</td>
    </tr>
  </tbody>
</table>

<p><br />
Uisng model: <strong>restnet34</strong> <sup id="fnref:2"><a href="#fn:2" class="footnote">1</a></sup>, Measurement: <strong>accuracy</strong> <sup id="fnref:3"><a href="#fn:3" class="footnote">2</a></sup></p>

<h4 id="fit_one_cycle">fit_one_cycle()</h4>

<p><strong>What is fit one cycle?</strong></p>

<p><a href="https://arxiv.org/pdf/1506.01186.pdf">Cyclical Learning Rates for Training Neural Networks</a></p>

<p>One of the way to find <em>good</em> learning rate. Core idea is to start with small learning rate (like 1e-4, 1e-3) and increase the learning rate after each mini-batch till <strong>loss starts exploding</strong>. And pick up learning rate one order lower than exploding point. For example, plotted learning rate is like below picture, picking up around 1e-2 is the best way.</p>

<p><img src="/assets/images/lr-finder.png" alt="" /></p>

<p><strong>Why this methods</strong></p>

<p>Traditionally, the learning rate is decreased as the learning starts converging with time. But this paper suggests to cycle our learning rate, because it makes us avoid <em>local minimum</em>. Basically this cyclic method enables us to explore whole of loss function so that find out global minimum. In other words, higher learning rate behaves like <em>regularisation</em>.</p>

<h4 id="lets-find-tune">Let’s find-tune</h4>

<p>Do train just one last layer by learning rate found by <em>find_lr</em></p>

<p>This section you should find the strongest downward slope that kind of sticking around for quite a while. And choose just one order lower than lowest point. As explained before, I will pick up 1e-2. And of course, this is fine-tuning, we don’t need discriminative learning rate yet.</p>

<p><img src="/assets/images/fine-tune.png" alt="" /></p>

<h4 id="lets-train-the-whole-model">Let’s train the whole model!</h4>

<p><a href="https://colab.research.google.com/github/SpellOnYou/dlff-note/blob/master/toy-project/What_is_your_city_lesson02.ipynb?authuser=1#scrollTo=8iumhHtskuDz">link</a></p>

<p>When you plot the learning rate again, maybe you will get soaring shape of learning rate. 
<strong>Rule of thumb</strong>, When you slice the learning rate, use learning rate you used at unfrozen part. Divide it by 5 or 10 and put it on maximum bound. At minimum bound, get the point just before it soared, and divide it by 10.</p>

<p><img src="/assets/images/whole_lr.png" alt="" /></p>

<h4 id="lets-make-batch-size-bigger">Let’s make batch size bigger!</h4>

<p>Since default batch size is 64, I tried it to 128.
And it gets way more better result(even it’s still underfitting!)</p>

<p><img src="/assets/images/lr_bigger.png" alt="" /></p>

<p>And if I freeze model and train whole model again, the model would be better. Also, you can use this method to the other big dataset model training!</p>

<h3 id="interpretation">Interpretation</h3>

<p><img src="/assets/images/interp.png" alt="" /></p>

<p>See the confusion matrix. Result is quite great.</p>

<p>*Since I’m using colab, I will skip data cleansing. But I highly recommend you to use <code class="highlighter-rouge">ImageCleaner</code> widget, only if you are using jupyter notebook (not jupyter lab)</p>

<h3 id="model-in-production">Model in production</h3>

<p>You can deploy your model in simple way. I referred fast.ai, and used render(it’s free for limited time). You can find detailed document <a href="https://course.fast.ai/deployment_render.html">here</a>.</p>

<p>and you can create a route like this.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">app</span><span class="o">.</span><span class="n">route</span><span class="p">(</span><span class="s">"/classify-url"</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s">"GET"</span><span class="p">])</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">classify_url</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
    <span class="nb">bytes</span> <span class="o">=</span> <span class="k">await</span> <span class="n">get_bytes</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">query_params</span><span class="p">[</span><span class="s">"url"</span><span class="p">])</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">open_image</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="nb">bytes</span><span class="p">))</span>
    <span class="n">_</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">losses</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">JSONResponse</span><span class="p">({</span>
        <span class="s">"predictions"</span><span class="p">:</span> <span class="nb">sorted</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="n">cat_learner</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="nb">map</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">losses</span><span class="p">)),</span>
            <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)</span>
    <span class="p">})</span>
</code></pre></div></div>

<p>You can find my deployed model <a href="https://my-city-classifier.onrender.com/">here</a></p>

<h3 class="no_toc" id="reference">Reference</h3>

<p><a href="https://www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/">How to create a deep learning dataset using Google Images</a></p>

<p><a href="https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6">towardsdatascience - one cycle policy</a></p>
<div class="footnotes">
  <ol>
    <li id="fn:2">
      <p><a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a> <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p><a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">Accuracy_and_precision</a> <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
