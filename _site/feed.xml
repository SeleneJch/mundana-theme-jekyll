<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-11-24T03:17:47+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">SpellOnYou</title><subtitle>Be afraid only of standing still. Remain fresh, body and soul.</subtitle><entry><title type="html">5 reasons took much time to setting GPU for fast.ai than I expected</title><link href="http://localhost:4000/2019/11/GPU-time/" rel="alternate" type="text/html" title="5 reasons took much time to setting GPU for fast.ai than I expected" /><published>2019-11-23T00:00:00+09:00</published><updated>2019-11-23T00:00:00+09:00</updated><id>http://localhost:4000/2019/11/GPU-time</id><content type="html" xml:base="http://localhost:4000/2019/11/GPU-time/">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Before now, me as a undergraduate student, I was parsimony who usually depend on colab, kaggle, friend‚Äôs server(occasional) whenever i need GPU..&lt;br /&gt;
&lt;br /&gt;
And this time it‚Äôs been for a while to install GPU than I expected and I share the several component that stood in my way.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;&lt;span style=&quot;color:red&quot;&gt;Written at Oct 24 2019, if you think this is deprecated, please do not have a leap of faith.&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Just for the record, I‚Äôve used Kaggle, Colab, GCP, Azure, EC2 as GPU cloud.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-did-not-know-there-is-jupyterlab-option-in-google-cloud-platform&quot;&gt;1. Did not know there is &lt;em&gt;JupyterLab&lt;/em&gt; option in &lt;strong&gt;Google Cloud Platform&lt;/strong&gt;.&lt;/h3&gt;

&lt;p&gt;At the first time when GCP came out, there was no &lt;strong&gt;AI  Platform&lt;/strong&gt; service. So from starting vm instance to launching jupyter and installing packages, I did all of the things myself. (and I learned ü§ó)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/8.png&quot; alt=&quot;installing-conda-cli&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$	curl -O https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;[Downloading conda in ssh]&lt;/p&gt;

&lt;p&gt;I created VM instance,selected zone, machine type and disk type. Then, define firewall rules and in ssh terminal, install jupyter and other packages.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;But you can do all of these things just using AI Platform.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/9.png&quot; alt=&quot;installing-conda-cli&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;[AI Platform]&lt;/p&gt;

&lt;p&gt;I think it especially save your time if you are living in Asia-Pacific, which google doesn‚Äôt support not that much GPU resources.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;2-consider-if-the-platform-has-limited-resources-in-a-region-you-live-in&quot;&gt;2. Consider if the platform has limited resources in a region you live in.&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;
I live in &lt;em&gt;South Korea, East Asia&lt;/em&gt;, and it seems like this region has lots of limitation in GPU (except quite expensive AWS)&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;And the Taiwan which was the only one region where I can launch my own VM with GPU (I tried all the other regions in the list) sometimes do normaly, but not always. üò•&lt;br /&gt;
After launching, I did several works and next day I could not start VM. (I didn‚Äôt count it, but tried it a few hours because I didn‚Äôt want cost any more time‚Ä¶)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Endlessly failed to start instance, then I choose to move AWS as an alternative way.&lt;/p&gt;

&lt;h3 id=&quot;3-fastai-gives-deliberate-guide-and-i-didnt-know-it&quot;&gt;3. &lt;span style=&quot;color:blue&quot;&gt;Fast.ai gives deliberate &lt;a href=&quot;https://course.fast.ai/start_gcp.html&quot;&gt;guide&lt;/a&gt; and I didn‚Äôt know it.&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;Fast.ai offer the guide for all available platform. (Colab, salamander, Gradient, Kaggle, Colab, and so on)&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;It is so important, and really needs, because cloud computing options are vary as occasion and purpose arise.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I didn‚Äôt know fast.ai has manual to running GCP, and I think it‚Äôs as good a reason as any for me to be have taken time.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;It helped me so much when I had aws and shortened my time.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I don‚Äôt want to read all of the manual in amazno.. (It is recommended.. but I‚Äôd rather read &lt;a href=&quot;https://git-scm.com/book/en/v2&quot;&gt;GIT PRO&lt;/a&gt; now‚Ä¶)&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh -i ~/.ssh/&amp;lt;your_private_key_pair&amp;gt; -L localhost:8888:localhost:8888 ubuntu@&amp;lt;your instance IP&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4-you-should-wait-to-add-more-volume-just-after-add-volume-by-building-aws-ec2&quot;&gt;4. You should wait to add more volume just after add volume, by building AWS EC2.&lt;/h3&gt;

&lt;p&gt;Since Elastic Block Store(EBS) storage supports optimized storage, users can‚Äôt extend storage volume two times in a row. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, at the first time, I didn‚Äôt know it (again üëª) and when VM lacked volume, I doubled dist capacity (76*2) at a rough but It needs more. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;!‚Äì&lt;/p&gt;

&lt;p&gt;this time I installed GPU in two years, and it became little complicated compared to 2 years ago.
And this time for the first time(maybe not the first time.. but i handled it in my class or with my friend. but it‚Äôs my first time on my own.) I 
very I‚Äôm started to using used google colab, kaggle
and, GCP-JupyterLab, ec2 - friend made, 
aws vm machine but I had a environment variable but i did not know of it.
On these days, I could not get a resources from taiwan‚Ä¶&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;I couldn‚Äôt notice a deliberate&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anyway, as a result I tried myself gcp myself and aws ec2 with fast.ai But I think doing on my self surely takes much time (in this point I wonder why I‚Äôm doing this, and should remind me, especially I was studying disk volume optimization)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;disk-volume-exceed---httpsaskubuntucomquestions919748no-space-left-on-device-even-though-there-is&quot;&gt;disk volume exceed - https://askubuntu.com/questions/919748/no-space-left-on-device-even-though-there-is&lt;/h2&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>dionne</name></author><category term="featured" /><summary type="html">Motivation</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/10.png" /></entry><entry><title type="html">Broadcasting, Einstein sum, Pytorch operator</title><link href="http://localhost:4000/2019/11/v3-notes08-3-fastai/" rel="alternate" type="text/html" title="Broadcasting, Einstein sum, Pytorch operator" /><published>2019-11-21T00:00:00+09:00</published><updated>2019-11-21T00:00:00+09:00</updated><id>http://localhost:4000/2019/11/v3-notes08-3-fastai</id><content type="html" xml:base="http://localhost:4000/2019/11/v3-notes08-3-fastai/">&lt;p&gt;‚Äù Lecture 08 - Deep Learning From Foundations-part2 ‚Äú&lt;/p&gt;

&lt;h3 id=&quot;homework&quot;&gt;Homework&lt;/h3&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;contents&quot;&gt;CONTENTS&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#the-forward-and-backward-passes&quot;&gt;The forward and backward passes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#foundation-version&quot;&gt;Foundation version&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#frobenius-norm&quot;&gt;Basic architecture&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#Loss-function:-MSE&quot;&gt;Loss function: MSE&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#Gradients-backward-pass&quot;&gt;Gradients backward pass&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Refactor-model&quot;&gt;Refactor model&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#Layers-as-classes&quot;&gt;Layers as classes&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#module.forawd()&quot;&gt;Module.forward()&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nn.Linear-and-nn.Module&quot;&gt;nn.Linear and nn.Module&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-forward-and-backward-passes&quot;&gt;The forward and backward passes&lt;/h3&gt;

&lt;h5 id=&quot;nomalization&quot;&gt;Nomalization&lt;/h5&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;train_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_std&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1304&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3073&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Remember!&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset, which is x_train, mean and standard deviation is not 0&amp;amp;1. &lt;strong&gt;But we need them to be&lt;/strong&gt; which means we should substract means and divide data by std.&lt;/li&gt;
  &lt;li&gt;You should not standarlize &lt;em&gt;validation set&lt;/em&gt; because training set and validation set should be aparted.&lt;/li&gt;
  &lt;li&gt;after normalize, mean is close to zero, and standard deviation is close to 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;variable-definition&quot;&gt;Variable definition&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;n,m&lt;/strong&gt;: size of the training set&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;c&lt;/strong&gt;: the number of activations we need in our model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;foundation-version&quot;&gt;Foundation Version&lt;/h3&gt;
&lt;h4 id=&quot;basic-architecture&quot;&gt;Basic architecture&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Our model has one hidden layer, output to have 10 activations, used in cross entropy.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;But in process of building architecture, we will use mean square error, output to have 1 activations and lator change it to cross entropy&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;number of hidden unit; 50&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;see below pic&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/model.jpg&quot; alt=&quot;&quot; height=&quot;80%&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We want to make w1&amp;amp;w2 mean and std be 0&amp;amp;1.
    &lt;ul&gt;
      &lt;li&gt;why initializating and make mean zero and std one is important?&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;simplified-karming-init&quot;&gt;simplified karming init&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;what about hidden(first) layer?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# hidden
&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.3191&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;27.0303&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In output(second) layer,&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# output
&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;58.2665&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;170.9717&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;which is terribly far from normalzed value.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;But if we apply simplified kaiming init&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0516&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9354&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;But, actually, we use activations not only linear function&lt;/li&gt;
  &lt;li&gt;After applying activations relu at linear layer, mean and deviation became 0.5.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/relu.jpg&quot; alt=&quot;&quot; height=&quot;70%&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;glorrot-initialization&quot;&gt;Glorrot initialization&lt;/h5&gt;

&lt;p&gt;Paper2: &lt;a href=&quot;http://proceedings.mlr.press/v9/glorot10a.html&quot;&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/a&gt;
	- Gaussian(, bell shaped, normal distributions) is not trained very well.
	- How to initialize neural nets?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/xavier.png&quot; alt=&quot;&quot; height=&quot;80%&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;with&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;n_i&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;the size of layer n, the number of filters i.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;But there is &lt;strong&gt;No acount&lt;/strong&gt; for import of ReLU&lt;/li&gt;
  &lt;li&gt;If we got 1000 layers, vanishing gradients problem emerges&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;kaiming-initializating&quot;&gt;Kaiming initializating&lt;/h5&gt;

&lt;p&gt;Paper3: &lt;a href=&quot;https://arxiv.org/abs/1502.01852&quot;&gt;Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification&lt;/a&gt;
	- Kaiming He, explained &lt;a href=&quot;https://pouannes.github.io/blog/initialization/&quot;&gt;here&lt;/a&gt;
	- rectifier: rectified linear unit
	- rectifier network: neural network with rectifier linear units&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;initialization-is-important&quot;&gt;Initialization is important&lt;/h5&gt;

&lt;!-- 1:27:19 --&gt;

&lt;h4 id=&quot;loss-function-mse&quot;&gt;Loss function: MSE&lt;/h4&gt;

&lt;h4 id=&quot;gradients-backward-pass&quot;&gt;Gradients backward pass&lt;/h4&gt;

&lt;h3 id=&quot;refactor-model&quot;&gt;Refactor model&lt;/h3&gt;
&lt;h4 id=&quot;layers-as-classes&quot;&gt;Layers as classes&lt;/h4&gt;
&lt;h4 id=&quot;modueforward&quot;&gt;Modue.forward()&lt;/h4&gt;
&lt;h4 id=&quot;nnlinear-and-nnmodule&quot;&gt;nn.Linear and nn.Module&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;reference&quot;&gt;Reference&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.09321&quot;&gt;Fixup Initialization: Residual Learning Without Normalization&lt;/a&gt;¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>dionne</name></author><category term="featured" /><summary type="html">‚Äù Lecture 08 - Deep Learning From Foundations-part2 ‚Äú</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/5-fc.png" /></entry><entry><title type="html">Julia Evans</title><link href="http://localhost:4000/2019/11/julia-evans/" rel="alternate" type="text/html" title="Julia Evans" /><published>2019-11-20T00:00:00+09:00</published><updated>2019-11-20T00:00:00+09:00</updated><id>http://localhost:4000/2019/11/julia-evans</id><content type="html" xml:base="http://localhost:4000/2019/11/julia-evans/">&lt;p&gt;The women who surprised me in many ways.&lt;br /&gt;
First, she approached me to teaching some concepts drawing cartoons.&lt;br /&gt;
It was at Hackers news, which was hightest ranks.&lt;br /&gt;
Personally I have the use of not to reading title, so and cartoon was so cute and clear.&lt;br /&gt;
I naturally gonna understood mechanism and astonished by her explaination ability.&lt;br /&gt;
Her value, which she was taught by many people so want to do same things, moved me.&lt;br /&gt;
Volume of her knowledge, that just reading post title is a deal of work, amazed me.&lt;br /&gt;&lt;/p&gt;

&lt;!-- neural networkÎ∂ÄÌÑ∞, ÏóÑÏ≤≠ ÎßéÏùÄ ÎèÑÎ©îÏù∏.  --&gt;</content><author><name>dionne</name></author><category term="featured" /><summary type="html">The women who surprised me in many ways. First, she approached me to teaching some concepts drawing cartoons. It was at Hackers news, which was hightest ranks. Personally I have the use of not to reading title, so and cartoon was so cute and clear. I naturally gonna understood mechanism and astonished by her explaination ability. Her value, which she was taught by many people so want to do same things, moved me. Volume of her knowledge, that just reading post title is a deal of work, amazed me.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/evans.jpg" /></entry><entry><title type="html">Elif Shafak</title><link href="http://localhost:4000/2019/11/elif-shafak/" rel="alternate" type="text/html" title="Elif Shafak" /><published>2019-11-05T00:00:00+09:00</published><updated>2019-11-05T00:00:00+09:00</updated><id>http://localhost:4000/2019/11/elif-shafak</id><content type="html" xml:base="http://localhost:4000/2019/11/elif-shafak/">&lt;p&gt;For creative-minded people, Istanbul is a treasure.‚Äô Photo ¬© Chris Boland, licensed under &lt;a href=&quot;https://creativecommons.org/licenses/by-nc-nd/2.0/&quot;&gt;CC BY-NC-ND 2.0&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;it suddenly felt like what I was trying to convey was more complicated and detailed than what the circumstances allowed me to say.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;And I did what I usually do in similar situations: I stammered, I shut down, and I stopped talking. I stopped talking because the truth was complicated, even though I knew, deep within, that one should never, ever remain silent for fear of complexity.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/images/31.jpg&quot; alt=&quot;&quot; /&gt; &amp;lt;Figure 1&amp;gt; Elif Shafak&lt;/td&gt;
      &lt;td&gt;Photo credit: www.elifsafak.com.tr&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;I want to talk about emotions and the need to boost our emotional intelligence. I think it‚Äôs a pity that mainstream political theory pays very little attention to emotions.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;Oftentimes, analysts and experts are so busy with data and metrics that they seem to forget those things in life that are difficult to measure and perhaps impossible to cluster under statistical models. But I think this is a mistake, for two main reasons. We are emotional beings.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;I think it‚Äôs going to be one of our biggest intellectual challenges, because our political systems are replete with emotions. In country after country, we have seen illiberal politicians exploiting these emotions. And yet within the academia and among the intelligentsia, we are yet to take emotions seriously. I think we should.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;reference&quot;&gt;Reference&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.britishcouncil.org/voices-magazine/elif-shafak-writing-english-brings-me-closer-turkey&quot;&gt;British Council Worldwide&lt;/a&gt;¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.ted.com/talks/elif_shafak_the_revolutionary_power_of_diverse_thought#t-37432&quot;&gt;Ted Talk&lt;/a&gt;¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>dionne</name></author><summary type="html">For creative-minded people, Istanbul is a treasure.‚Äô Photo ¬© Chris Boland, licensed under CC BY-NC-ND 2.0</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/29.jpg" /></entry><entry><title type="html">Retrospective on Pycon 2019 Korea (CoC Committee)</title><link href="http://localhost:4000/2019/11/coc-retropective/" rel="alternate" type="text/html" title="Retrospective on Pycon 2019 Korea (CoC Committee)" /><published>2019-11-05T00:00:00+09:00</published><updated>2019-11-05T00:00:00+09:00</updated><id>http://localhost:4000/2019/11/coc-retropective</id><content type="html" xml:base="http://localhost:4000/2019/11/coc-retropective/">&lt;p&gt;When I was volunteer, it seems like busy and hectic to managing that crowded conference.&lt;br /&gt;
In my experience, to get things moving, it needs &lt;strong&gt;hierarchy&lt;/strong&gt;.&lt;br /&gt;
But it didn‚Äôt. Organizers emphasized our responsibility, and if I passed each other‚Äôs burden, It could be my burden next time.&lt;br /&gt;
In solidarity of the obligation, we finished conference well.&lt;br /&gt;
And after participating PyCon Korea 2018 as volunteer, I‚Äôve joined PyCon Korea Organizer last year.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;Figure 1&amp;gt; First meeting of PyCon 2019 Korea Organizers &lt;img src=&quot;/assets/images/28.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It‚Äôs been a while since PyCon 2019 finished. It‚Äôs held on Aug 15 - 18, at Coex Grand Balloom&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;Figure 2&amp;gt; Ongoing session, speaking on news comment processing  &lt;img src=&quot;/assets/images/22.jpg&quot; alt=&quot;&quot; /&gt;
&amp;lt;Figure 3&amp;gt; Sponsor Booth iin Coex Hall &lt;img src=&quot;/assets/images/27.jpg&quot; alt=&quot;&quot; /&gt;
&amp;lt;Figure 4&amp;gt; After PyCon 2019, with all of volunteer, organizer, speakers üòç ü•∞ &lt;img src=&quot;/assets/images/17.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Serving as part of the coc TF, I spent large fraction of last year doing CoC job.
here‚Äôs the path what we‚Äôve been grappled with to grasp a solution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;First half: Before the conference&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;toward-diverse-community&quot;&gt;Toward Diverse Community&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Formally we‚Äôve been reusing and modifying PyCon US CoC, but we needed fit in Korean and I was part of that to revise code of conduct.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;except-that-diversity-because-it-is-harassment&quot;&gt;Except ‚ÄòThat‚Äô Diversity, Because it is ‚ÄòHarassment‚Äô&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Specific point was harassment, and the others were not. process of finding the points. How can we settle this point?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Second half: During the conference&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;handling-the-potential-harassment&quot;&gt;Handling the potential Harassment&lt;/h4&gt;

&lt;h4 id=&quot;disjunction-of-policy-and-real-time-situation&quot;&gt;Disjunction of policy and real-time situation&lt;/h4&gt;

&lt;hr /&gt;

&lt;p&gt;This ‚ÄòPyCon 2019 Korea retrospective series‚Äô would be devided into 3 Episodes.&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;‚ÄúRetrospective on Pycon 2019 Korea (CoC Committee)‚Äù&lt;/li&gt;
  &lt;li&gt;‚ÄúRetrospective on Pycon 2019 Korea (Program Chair)‚Äù (20 Nov, To Be Update)&lt;/li&gt;
  &lt;li&gt;‚ÄúMaintaining participation while still making timely decisions‚Äù (29 Nov, To Be Update)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>dionne</name></author><summary type="html">When I was volunteer, it seems like busy and hectic to managing that crowded conference. In my experience, to get things moving, it needs hierarchy. But it didn‚Äôt. Organizers emphasized our responsibility, and if I passed each other‚Äôs burden, It could be my burden next time. In solidarity of the obligation, we finished conference well. And after participating PyCon Korea 2018 as volunteer, I‚Äôve joined PyCon Korea Organizer last year.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/14.jpg" /></entry><entry><title type="html">Douglas Rushkoff</title><link href="http://localhost:4000/2019/11/douglas-rushoff/" rel="alternate" type="text/html" title="Douglas Rushkoff" /><published>2019-11-04T00:00:00+09:00</published><updated>2019-11-04T00:00:00+09:00</updated><id>http://localhost:4000/2019/11/douglas-rushoff</id><content type="html" xml:base="http://localhost:4000/2019/11/douglas-rushoff/">&lt;p&gt;‚ÄúHumans are no longer valued for our creativity.&lt;br /&gt;
Now we are just valued for our data‚Äù&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;It does require specific conditions that people are living in the competitive.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I think, no doubt, It‚Äôs a supercompetitive world.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;After watching this vedio, I remembered meeting an acountant partner at a Big 4 firm, 3 years ago.&lt;/em&gt;&lt;br /&gt;
I met him via mentoring system in university, so the acountant partner encouraged me to be a acountant, because they rate SNU highly.&lt;br /&gt;
Besides, the acountant said, to be frankly said, what neighborhood &amp;amp; school(even high school), where they are living now(commute time!) and single or married are more liable than anything.&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Well You know, those values are relatively objective than cover letter or your apperance‚Ä¶ anything.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When I left his firm, after mentoring, he happened a junior staff.&lt;/p&gt;

&lt;p&gt;‚ÄúShe is married women and graduated from just that schoo. But her performance quite good. Outlier.‚Äù&lt;/p&gt;

&lt;p&gt;And the acountant‚Äôs word, which I‚Äôve heard 3 years ago, coincides in ‚ÄúDigital Future‚Äù&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; that regard creativity creates noise.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.ted.com/talks/douglas_rushkoff_how_to_be_team_human_in_the_digital_future&quot;&gt;Ted talk&lt;/a&gt;¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.ted.com/talks/douglas_rushkoff_how_to_be_team_human_in_the_digital_future#t-174064&quot;&gt;Digital Future&lt;/a&gt;¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>dionne</name></author><summary type="html">‚ÄúHumans are no longer valued for our creativity. Now we are just valued for our data‚Äù1 Ted talk¬†&amp;#8617;</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/12.jpg" /></entry><entry><title type="html">Why am I not listed as a contributor after I contributed?</title><link href="http://localhost:4000/2019/10/Git-Merge/" rel="alternate" type="text/html" title="Why am I not listed as a contributor after I contributed?" /><published>2019-10-30T00:00:00+09:00</published><updated>2019-10-30T00:00:00+09:00</updated><id>http://localhost:4000/2019/10/Git-Merge</id><content type="html" xml:base="http://localhost:4000/2019/10/Git-Merge/">&lt;p&gt;From the end of last year, big changes have witnessed in NLP research.&lt;br /&gt;
Embracing an unprecedented growth, I started to study new exciting results and advances.&lt;br /&gt;
In doing so, I noticed I‚Äôm not listed as contributor of repo which my PR accessed.&lt;/p&gt;

&lt;h3 id=&quot;how-did-i-come-to-a-repository&quot;&gt;How did I come to a repository?&lt;/h3&gt;

&lt;p&gt;When I‚Äôm stuck, I would prefer to code, than to go deep in theory. (It must be so.. too much to understand ü§í)&lt;br /&gt;
It was &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;BERT released by Google AI&lt;/a&gt; I felt keenly the necessity of implementing, because not only couldn‚Äôt understand the way they figured out positional encoding formula, but how it actually works.&lt;br /&gt;What does it mean to ‚Äúscale‚Äù dot product in Attention? (Now I know it‚Äôs far from my section üòÇ)&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h6 id=&quot;figure-1-scaled-dot-product-adopted-from-tensorflow-blog&quot;&gt;Figure 1. Scaled Dot Product. &lt;em&gt;Adopted from&lt;/em&gt; &lt;a href=&quot;www.tensorflow.org&quot;&gt;&lt;em&gt;tensorflow blog&lt;/em&gt;&lt;/a&gt;&lt;/h6&gt;

&lt;h3 id=&quot;what-was-the-code-error&quot;&gt;What was the code error?&lt;/h3&gt;

&lt;p&gt;For implement code in paper, I read the papers &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Transformer&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;BERT&lt;/a&gt;, structured the model, and refered the others‚Äô code.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Meanwhile, I found out a small error in tokenization process, which was changing a token into [MASK], enabled bidirectional representation.&lt;/p&gt;

&lt;h3 id=&quot;ive-made-pr-and-got-merged-but-i-was-not-in-contributors-why&quot;&gt;I‚Äôve made PR, and got merged. But I was not in contributors. Why?&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h6 id=&quot;figure-2-merged-pull-request-adopted-from-graykode-project&quot;&gt;Figure 2. Merged Pull request &lt;em&gt;Adopted from&lt;/em&gt; &lt;a href=&quot;https://github.com/graykode/nlp-tutorial/pull/9&quot;&gt;&lt;em&gt;graykode project&lt;/em&gt;&lt;/a&gt;&lt;/h6&gt;

&lt;p&gt;Actually I happened to know there can be couple of reasons github doesn‚Äôt include my name as contributor.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Well, if contributors tab has more than 100 people, in which case it shows you up only if you are in the top 100 contributors because displaying too many contributors can make webpages down.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Somethimes, however, it doesn‚Äôt that problem. Why not?&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Two possibilities are there.&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First, According to &lt;a href=&quot;https://www.quora.com/Why-doesnt-GitHub-include-my-name-as-a-contributor-even-after-I-contributed&quot;&gt;Joel-Glovier&lt;/a&gt;, if repository maintainer &lt;a href=&quot;https://shinglyu.com/web/2018/03/25/merge-pull-requests-without-merge-commits.html&quot;&gt;merged-as-a-rebase&lt;/a&gt; PR will end up showing as maintainer‚Äôs commit. But maintainer shouldn‚Äôt normally do this.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Second, if you happend to commit using a different git email that what is in your GitHub profile, it will not be attached to your Github user, and ‚Äúdoesn‚Äôt show up‚Äù as you.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;reference&quot;&gt;Reference&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/&quot;&gt;Micha≈Ç Chromiak‚Äôs blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://help.github.com/en/github/setting-up-and-managing-your-github-profile/why-are-my-contributions-not-showing-up-on-my-profile&quot;&gt;Github: why are my contributions are not showing on my profile&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.atlassian.com/git/tutorials/syncing/git-fetch&quot;&gt;atlassian-gitfetch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>dionne</name></author><category term="featured" /><summary type="html">From the end of last year, big changes have witnessed in NLP research. Embracing an unprecedented growth, I started to study new exciting results and advances. In doing so, I noticed I‚Äôm not listed as contributor of repo which my PR accessed.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/1.png" /></entry><entry><title type="html">Lecture 08 - Deep Learning From Foundations-part1</title><link href="http://localhost:4000/2019/10/note08-fastai-1/" rel="alternate" type="text/html" title="Lecture 08 - Deep Learning From Foundations-part1" /><published>2019-10-18T00:00:00+09:00</published><updated>2019-10-18T00:00:00+09:00</updated><id>http://localhost:4000/2019/10/note08-fastai-1</id><content type="html" xml:base="http://localhost:4000/2019/10/note08-fastai-1/">&lt;p&gt;&lt;br /&gt;
&lt;em&gt;I don‚Äôt know if you read this article, but I heartily appreciate Rachael Thomas and Jeremy Howard for providing these priceless lectures for free&lt;/em&gt;
&lt;br /&gt;
&lt;strong&gt;Contents&lt;/strong&gt;
&lt;br /&gt;
1) &lt;a href=&quot;#whats-going-on-in-this-course&quot;&gt;What is going on in this course&lt;/a&gt;
2) &lt;a href=&quot;#library-development-using-jupyter-notebook&quot;&gt;Library development using jupyter notebook&lt;/a&gt;
&lt;br /&gt;
&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/fastai/course-v3/blob/master/nbs/dl2/01_matmul.ipynb&quot;&gt;notebooks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/file/d/18QwDI25Lf0ld0-cEugu7LxjwTc2NRkha/view&quot;&gt;material&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://course.fast.ai/videos/?lesson=8&quot;&gt;video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/spreadsheets/d/1bIPBcf-p9iqNG8BGmIVlJCFa4jEsbOZvcPXGTYe5pjI/edit#gid=0&quot;&gt;broadcasting excel&lt;/a&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-going-on-in-this-course&quot;&gt;What is going on in this course?&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;what-is-from-foundations&quot;&gt;What is &lt;em&gt;‚Äòfrom foundations‚Äô&lt;/em&gt;?&lt;/h4&gt;

&lt;p&gt;1) Recreate fast.ai and Pytorch&lt;/p&gt;

&lt;p&gt;2) using pure python
&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;evade-overfitting&quot;&gt;Evade Overfitting&lt;/h4&gt;

&lt;p&gt;Overfit : validation error getting worse
&lt;del&gt;training loss &amp;lt; validation loss&lt;/del&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;know-the-name-of-the-symbol-you-use&quot;&gt;Know the name of the symbol you use&lt;/h4&gt;

&lt;p&gt;find in this page if you don‚Äôt know the symbol that you are using&lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_mathematical_symbols&quot;&gt;&lt;/a&gt; or just draw it &lt;a href=&quot;http://detexify.kirelabs.org/classify.html&quot;&gt;here&lt;/a&gt; (run by ML!)&lt;/p&gt;

&lt;h4 id=&quot;steps-to-a-basic-modern-cnn-model&quot;&gt;Steps to a basic modern CNN model&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;1) Matrix multiplication -&amp;gt; 2) Relu/Initialization -&amp;gt; 3) Fully-connected Forward
-&amp;gt; 4) Fully-connected Backward -&amp;gt; 5) Train loop -&amp;gt; 6) Convolution-&amp;gt; 7) Optimization -&amp;gt;
8) Batchnormalization -&amp;gt; 9) Resnet&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;todays-implementation-goal-1-matmul---4-fc-backward&quot;&gt;Today‚Äôs implementation goal: 1) matmul -&amp;gt; 4) FC backward&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;library-development-using-jupyter-notebook&quot;&gt;Library development using jupyter notebook&lt;/h3&gt;
&lt;p&gt;https://dbader.org/blog/python-assert-tutorial
&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;jupyter-notebook-certainly-can-make-module&quot;&gt;jupyter notebook certainly can make module&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;There will be &lt;em&gt;#export&lt;/em&gt; tag that Howard (and we) want to extract&lt;/li&gt;
  &lt;li&gt;special &lt;em&gt;notebook2script.py&lt;/em&gt; will detect sign of &lt;em&gt;#expert&lt;/em&gt; and convert following into python module
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;and test it&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;test\_eq(TEST,'test')&lt;br /&gt;test\_eq(TEST,'test1')&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;what is &lt;strong&gt;run_notebook.py&lt;/strong&gt;?
    &lt;ul&gt;
      &lt;li&gt;when you want to test your module in command line interface&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;		!python run\_notebook.py 01_matmul.ipynb&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Is there any difference between 1) and 2)?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1) test -&amp;gt; test01 
2) test01 -&amp;gt; test&lt;/p&gt;

&lt;p&gt;#TODO I don‚Äôt know yet&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;look into &lt;em&gt;run_notebook.py&lt;/em&gt;, package &lt;strong&gt;fire&lt;/strong&gt; Jeremy used. What is that?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;read and run the code in a notebook, and in the process, Jeremy made &lt;a href=&quot;https://opensource.googleblog.com/2017/03/python-fire-command-line.html&quot;&gt;Python Fire&lt;/a&gt; library called!shockingly, fire takes any kind of function and converts into CLI command.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;fire library was released by Google open source, Thursday, March 2, 2017&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;get-data&quot;&gt;Get data&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;pytorch and numpy are pretty much same.&lt;/li&gt;
  &lt;li&gt;variable c explains how many pixels there are in in MNIST, 28 pixels&lt;/li&gt;
  &lt;li&gt;PyTorch‚Äôs &lt;em&gt;view()&lt;/em&gt; method: torch function that manipulating tensor, and squeeze() in torch &amp;amp; mathmatical operation similar function&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.oreilly.com/library/view/natural-language-processing/9781491978221/&quot;&gt;Rao &amp;amp; McMahan&lt;/a&gt; said usually this functions result in feature vector.&lt;/li&gt;
  &lt;li&gt;In part 1, you can use view function several times.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;initial-python-model&quot;&gt;Initial python model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Which is Linear, like $Xw$(weight)$+a$(bias) $= Y$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you don‚Äôt know hou to multiple matrix, refer this site &lt;a href=&quot;http://matrixmultiplication.xyz&quot;&gt;matmul visulization site&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;how-many-time-spends-if-we-we-use-pure-python&quot;&gt;How many time spends if we we use pure python&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;function &lt;span style=&quot;color:blue&quot;&gt;matmul&lt;/span&gt;, typical matrix multiplication function, takes about 1 second for calculating 1 single train data! (maybe assumed stochastic, 5 data points in validation)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;it takes about 11.36 hours to update parameters even single layer and 1 iteration! (if that was my computer, it would be 14 hours..)ü§™&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;THIS is why we need to consider ‚Äòtime‚Äô&amp;amp;‚Äôspace‚Äô&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is kinda slow - what if we could speed it up by 50,000 times? Let‚Äôs try!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;elementwise-ops&quot;&gt;Elementwise ops&lt;/h3&gt;

&lt;h4 id=&quot;how-can-we-make-python-faster&quot;&gt;How can we make python faster?&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;If we want to calculate faster, then do remove pythonic calcuation, by passing its computation down to something that is written something other than python, like pytorch.&lt;/li&gt;
  &lt;li&gt;According to PyTorch &lt;a href=&quot;https://pytorch.org/cppdocs/#aten&quot;&gt;doc&lt;/a&gt; it  uses C++ (via ATen), so we are going to implement that function with python.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;what-is-element-wise-operation&quot;&gt;What is element wise operation?&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;items makes a pair, operate corresponding component
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>dionne</name></author><category term="featured" /><summary type="html">I don‚Äôt know if you read this article, but I heartily appreciate Rachael Thomas and Jeremy Howard for providing these priceless lectures for free Contents 1) What is going on in this course 2) Library development using jupyter notebook Resources notebooks material video broadcasting excel</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/cnn.png" /></entry><entry><title type="html">Lecture 16 - Code-First NLP Note</title><link href="http://localhost:4000/2019/09/code-first-course-note-16/" rel="alternate" type="text/html" title="Lecture 16 - Code-First NLP Note" /><published>2019-09-03T00:00:00+09:00</published><updated>2019-09-03T00:00:00+09:00</updated><id>http://localhost:4000/2019/09/code-first-course-note-16</id><content type="html" xml:base="http://localhost:4000/2019/09/code-first-course-note-16/">&lt;p&gt;&lt;strong&gt;Algorithms can encode &amp;amp; magnify human bias&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;case-study-1-facial-recognition--predictive-policing&quot;&gt;Case Study 1: Facial Recognition &amp;amp; Predictive Policing&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v81/buolamwini18a.html&quot;&gt;Joy Buolamwini &amp;amp; Timnit Gebru&lt;/a&gt;, gendershades.org
    &lt;ul&gt;
      &lt;li&gt;Microsoft, FACE+, IBM - All of these things are sell now.&lt;/li&gt;
      &lt;li&gt;Largest gap between $\therefore\  Lighter Male\ &amp;gt;\  Darker\ Female $&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://qz.com/co/2405308/this-us-mayor-joked-cops-should-mount-50-caliber-guns-where-ai-predicts-crime/&quot;&gt;This US mayor joked cops should ‚Äúmount .50-caliber‚Äù guns where AI predicts crime&lt;/a&gt;&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;With machine learning, with automation, there‚Äôs a 99% success, so that robot is „Ö°will be„Ö°99% accurate in telling us what is going to happen next, which is really interesting.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;- city official in Lancater, CA, approving on using IBM for public security&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bias&quot;&gt;Bias&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Bias is type of error&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Statistical Bias&lt;/strong&gt;: difference between a statistic‚Äôs expected value and the true value&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unjust Bias&lt;/strong&gt;: disproportionate preference for or prejudice against a group&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unconscious bias&lt;/strong&gt;: bias that we don‚Äôt realize we have&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;But, term bias is too generic to be productive.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Different sources of bias have different causes&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Representation Bias&lt;/strong&gt;: Dataset was not representative of the algorithm that might be used on later.&lt;/p&gt;

&lt;p&gt;Above : Data is okay, but algorithm has some problem.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Below : Data has error. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;For example, object detection production that performs very well in common product of US.&lt;br /&gt;
But in contrast, change of target product region, like Zimbabwe, Solomon Island, and so on, reduced the performence remarkably.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;It is not the algorithmic problem, so we should care about data volume of region.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Evaluation Bias&lt;/strong&gt;: Benchmark datasets spur on research, 4.4% of IJB-A images are dark-skinned women. 2/3 of ImageNet images from the West (Sharkar et al, 2017) &lt;img src=&quot;https://spellonyou.github.io/images/shankar.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;case-study-2-recidivism-algorithm-used-prison-sentencing&quot;&gt;Case Study 2: Recidivism Algorithm Used Prison Sentencing&lt;/h3&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;case-study-3-online-ad-delivery&quot;&gt;Case Study 3: Online Ad Delivery&lt;/h3&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;bias-in-nlp&quot;&gt;Bias in NLP&lt;/h3&gt;

&lt;p&gt;( Nothing to do with the course, but I‚Äôm researching this field these days.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;But all about Englsih&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Impact
The person is doctor. The person is nurse -&amp;gt; Í∑∏Îäî ÏùòÏÇ¨Îã§. Í∑∏ÎÖÄÎäî Í∞ÑÌò∏ÏÇ¨Îã§.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;concept-of-biased-data-often-too-generic-to-be-useful&quot;&gt;Concept of ‚Äúbiased data‚Äù often too generic to be useful&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Different sources of bias have different sources&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Data, models and systems are not unchanging numbers on a screen.
They‚Äôre the result of a complex process that starts with years of historical context and involves a series of choices and norms, from data measurement to model evaluation to human interpretation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;- &lt;em&gt;Harini Suresh&lt;/em&gt;, &lt;a href=&quot;https://medium.com/@harinisuresh/the-problem-with-biased-data-5700005e514c&quot;&gt;‚ÄúThe problem with Biased Data‚Äù&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;five-sources-of-bias-in-ml&quot;&gt;Five Sources of Bias in ML&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Representation Bias&lt;/li&gt;
  &lt;li&gt;Evaluation Bias&lt;/li&gt;
  &lt;li&gt;Measurement Bias&lt;/li&gt;
  &lt;li&gt;Aggregation Bias(46:02)&lt;/li&gt;
  &lt;li&gt;Historical Bias(46:26)
    &lt;ul&gt;
      &lt;li&gt;A few studies(47:13)&lt;/li&gt;
      &lt;li&gt;Racial Bias, Even when we have good intentions(new york times)(47:10)&lt;/li&gt;
      &lt;li&gt;gender(48:59)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;humans-are-biased-so-why-does-algorithmic-bias-matter&quot;&gt;Humans are biased, so why does algorithmic bias matter?&lt;/h3&gt;

&lt;h4 id=&quot;algorithms--humans-are-used-differently-humans-are-usually-decision-maker&quot;&gt;Algorithms &amp;amp; humans are used differently (&lt;em&gt;humans are usually decision maker&lt;/em&gt;)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Algorithms are accurate and objective&lt;/li&gt;
  &lt;li&gt;No way to apeal if there if error&lt;/li&gt;
  &lt;li&gt;processed large scale&lt;/li&gt;
  &lt;li&gt;cheap&lt;br /&gt; &lt;img src=&quot;https://cphoto.asiae.co.kr/listimglink/1/2018121911092829374_1545185366.jpg&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;machine-learning-can-amplify-bias&quot;&gt;Machine learning can amplify bias&lt;/h4&gt;

&lt;h4 id=&quot;machine-learning-can-create-feedback-loops&quot;&gt;Machine learning can create feedback loops.&lt;/h4&gt;

&lt;h4 id=&quot;technology-is-power-and-with-that-comes-responsibility&quot;&gt;Technology is power. And with that comes responsibility.&lt;/h4&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;solutions&quot;&gt;Solutions&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Analyze a project at work/school:
    &lt;ul&gt;
      &lt;li&gt;Questions about AI&lt;/li&gt;
      &lt;li&gt;5 types of bias &lt;a href=&quot;https://arxiv.org/abs/1901.10002&quot;&gt;(Suresh &amp;amp; Guttag)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Datasheets for datasets, Modelcards for model reporting&lt;/li&gt;
      &lt;li&gt;Accuracy rate on different sub-groups&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Work with domain experts &amp;amp; those impacted&lt;/li&gt;
  &lt;li&gt;Increase diversity in our workspace&lt;/li&gt;
  &lt;li&gt;Advocate for good policy&lt;/li&gt;
  &lt;li&gt;Be on the ongoing lookout for bias&lt;/li&gt;
&lt;/ol&gt;</content><author><name>dionne</name></author><summary type="html">Algorithms can encode &amp;amp; magnify human bias</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/13.png" /></entry></feed>