<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-07-14T16:57:20+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">SpellOnYou</title><subtitle>Be afraid only of standing still. Remain fresh, body and soul.</subtitle><entry><title type="html">Part2 lesson 9, 03 minibatch training | fastai 2019 course -v3</title><link href="http://localhost:4000/2020/07/TOC-lesson9/" rel="alternate" type="text/html" title="Part2 lesson 9, 03 minibatch training | fastai 2019 course -v3" /><published>2020-07-14T00:00:00+09:00</published><updated>2020-07-14T00:00:00+09:00</updated><id>http://localhost:4000/2020/07/TOC-lesson9</id><content type="html" xml:base="http://localhost:4000/2020/07/TOC-lesson9/">&lt;p&gt;Since fastai instructor including many people who studied fastai highly emphasized to copy the Jeremy’s code from scratch, I’ve been using that approach.&lt;br /&gt;
It was fine with Part1, but from Part2, applying same strategy which was &lt;code class=&quot;highlighter-rouge&quot;&gt;trying to copy nbs from blank cell without peeking up original&lt;/code&gt; was not enough.&lt;br /&gt;First, there were lots of sub-topics worth to dig in, since we’ve moved to bottom-up phase. And I couldn’t inspect deep inside of each topic(sometimes even don’t know what I am doing) with just replicating same code.&lt;br /&gt;Second, literally it was too difficult to rely on my own memory. I should open and close up the Jeremy’s too many times.&lt;br /&gt;I thought if I could draw overall picture of each notebook, replicating process would be easier. And it was. (At least with &lt;code class=&quot;highlighter-rouge&quot;&gt;01_matmul&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;02_fully_connected&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;02b_initializing&lt;/code&gt;)&lt;br /&gt;So I share my own supplementary hoping that it could help you to replicate code easily and grasp the lesson Jeremy intended.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I learned how to re-define our NN model and update training replicating code&lt;/p&gt;

&lt;p&gt;Original notebook &lt;a href=&quot;https://github.com/fastai/course-v3/blob/master/nbs/dl2/03_minibatch_training.ipynb&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#initial-setup&quot; id=&quot;markdown-toc-initial-setup&quot;&gt;Initial Setup&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#data&quot; id=&quot;markdown-toc-data&quot;&gt;Data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#loss-function-cross-entropy-loss&quot; id=&quot;markdown-toc-loss-function-cross-entropy-loss&quot;&gt;Loss function: Cross entropy loss&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#basic-training-loop&quot; id=&quot;markdown-toc-basic-training-loop&quot;&gt;Basic Training Loop&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#using-parameters-and-optimal&quot; id=&quot;markdown-toc-using-parameters-and-optimal&quot;&gt;Using parameters and optimal&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#parameters&quot; id=&quot;markdown-toc-parameters&quot;&gt;Parameters&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#registering-modules&quot; id=&quot;markdown-toc-registering-modules&quot;&gt;Registering modules&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nnmodulelist&quot; id=&quot;markdown-toc-nnmodulelist&quot;&gt;nn.ModuleList&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nnsequential&quot; id=&quot;markdown-toc-nnsequential&quot;&gt;nn.Sequential&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#optim&quot; id=&quot;markdown-toc-optim&quot;&gt;Optim&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dataset-and-dataloader&quot; id=&quot;markdown-toc-dataset-and-dataloader&quot;&gt;Dataset and DataLoader&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#dataset&quot; id=&quot;markdown-toc-dataset&quot;&gt;Dataset&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dataloader&quot; id=&quot;markdown-toc-dataloader&quot;&gt;DataLoader&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#random-sampling&quot; id=&quot;markdown-toc-random-sampling&quot;&gt;Random sampling&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pytorch-dataloader&quot; id=&quot;markdown-toc-pytorch-dataloader&quot;&gt;PyTorch DataLoader&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#validation&quot; id=&quot;markdown-toc-validation&quot;&gt;Validation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#unsolved-questions&quot; id=&quot;markdown-toc-unsolved-questions&quot;&gt;Unsolved questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;initial-setup&quot;&gt;Initial Setup&lt;/h2&gt;

&lt;h3 id=&quot;data&quot;&gt;Data&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Fastai for Colab env&lt;/li&gt;
  &lt;li&gt;import data
    &lt;ul&gt;
      &lt;li&gt;Hyper parames
        &lt;ul&gt;
          &lt;li&gt;Number of input size = data size&lt;/li&gt;
          &lt;li&gt;Number of output unit = value&lt;/li&gt;
          &lt;li&gt;Number of hidden unit&lt;/li&gt;
          &lt;li&gt;Number of hidden size = 1&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Initialize the model &lt;strong&gt;&lt;em&gt;(1st)&lt;/em&gt;&lt;/strong&gt; with which gets initialized and do the forward.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;loss-function-cross-entropy-loss&quot;&gt;Loss function: Cross entropy loss&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Softmax
    &lt;ul&gt;
      &lt;li&gt;Write down the formula of softmax&lt;/li&gt;
      &lt;li&gt;Why do we need this softmax function at the last layer?&lt;/li&gt;
      &lt;li&gt;Write down softmax as code.&lt;/li&gt;
      &lt;li&gt;Draw the softmax probability function using matplotlib&lt;/li&gt;
      &lt;li&gt;Why do I need a log of softmax?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cross Entropy
    &lt;ul&gt;
      &lt;li&gt;Write down the formula of cross entropy&lt;/li&gt;
      &lt;li&gt;Draw the function using matplotlib&lt;/li&gt;
      &lt;li&gt;Why is this function adequate to the loss of categorical variables?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Negative log likelihood function
    &lt;ul&gt;
      &lt;li&gt;Why &lt;code class=&quot;highlighter-rouge&quot;&gt;-sum( x * log( p(x) ) )&lt;/code&gt; has the name of negative log likelihood?&lt;/li&gt;
      &lt;li&gt;Why do we find a &lt;code class=&quot;highlighter-rouge&quot;&gt;negative&lt;/code&gt; value instead of likelihood function? (Hint: Related to finding arg min value) &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LogSumExp
    &lt;ul&gt;
      &lt;li&gt;What is LogSumExp&lt;/li&gt;
      &lt;li&gt;Why this is called trick somehow?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;basic-training-loop&quot;&gt;Basic Training Loop&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Write down the procedure the training loop repeats (4 steps)&lt;/li&gt;
  &lt;li&gt;Define the accuracy function.&lt;/li&gt;
  &lt;li&gt;Do the above step for an epoch with model defined at setting &lt;strong&gt;&lt;em&gt;(1st training loop)&lt;/em&gt;&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Get the cross entropy function from PyTorch and use it at 1 epoch process&lt;/li&gt;
      &lt;li&gt;You don’t have to do this for reversed sequence since you’ve already got gradients from PyTorch.&lt;/li&gt;
      &lt;li&gt;Don’t forget to check the existence of attribute before you update and zero it before you get out of loop&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Print the loss and accuracy of one batch &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;using-parameters-and-optimal&quot;&gt;Using parameters and optimal&lt;/h2&gt;
&lt;h3 id=&quot;parameters&quot;&gt;Parameters&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;we will use &lt;em&gt;nn.Module.&lt;strong&gt;setattr&lt;/strong&gt;&lt;/em&gt; and move relu to functional. &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;Re-define Model class using nn.Module. &lt;strong&gt;&lt;em&gt;(2nd Model)&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;What’s difference from that you made first?&lt;/li&gt;
  &lt;li&gt;See the layers inside model using &lt;code class=&quot;highlighter-rouge&quot;&gt;named_children&lt;/code&gt; method&lt;/li&gt;
  &lt;li&gt;see one layer&lt;/li&gt;
  &lt;li&gt;Re-define &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; function with using parameters, not layers. &lt;strong&gt;&lt;em&gt;(2nd Training Loop)&lt;/em&gt;&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;What’s difference between nn.Module’s layer / parameters?&lt;/li&gt;
      &lt;li&gt;Why this diff makes shorter code?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Make &lt;code class=&quot;highlighter-rouge&quot;&gt;DummyModule&lt;/code&gt; class to simulate pytorch’s &lt;code class=&quot;highlighter-rouge&quot;&gt;__setattr__&lt;/code&gt; function &lt;strong&gt;&lt;em&gt;(3rd Model)&lt;/em&gt;&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;3 dunder method
        &lt;ul&gt;
          &lt;li&gt;Init&lt;/li&gt;
          &lt;li&gt;setattr&lt;/li&gt;
          &lt;li&gt;repr&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;parameter function which generates each parameters in the layers&lt;/li&gt;
      &lt;li&gt;What does the setattr do here?
        &lt;ul&gt;
          &lt;li&gt;How can i check if I defined parameters properly?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;What does the repr do here?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Call the instance of dummymodule and see repr and shape of parameters in instance&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;registering-modules&quot;&gt;Registering modules&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Re-define Model class &lt;strong&gt;&lt;em&gt;(4th Model)&lt;/em&gt;&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Use original layers approach&lt;/li&gt;
      &lt;li&gt;Register the modules&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What is &lt;code class=&quot;highlighter-rouge&quot;&gt;registering modules&lt;/code&gt;?&lt;/li&gt;
  &lt;li&gt;See the model instance (i.e. repr)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;nnmodulelist&quot;&gt;nn.ModuleList&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Define class SequentialModel &lt;strong&gt;&lt;em&gt;(5th Model)&lt;/em&gt;&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Use nn.ModuleList&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What does nn.ModuleList do for us? (hint: fit, loss and accuracy function)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;nnsequential&quot;&gt;nn.Sequential&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Make model instance using nn.Sequential  &lt;strong&gt;&lt;em&gt;(6th Model)&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Why does this class make jobs easier?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;optim&quot;&gt;Optim&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Define class Optimier
    &lt;ul&gt;
      &lt;li&gt;Init - params, lr
        &lt;ul&gt;
          &lt;li&gt;Why do we add list of parameters at learning rate?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;step&lt;/li&gt;
      &lt;li&gt;zero_grad&lt;/li&gt;
      &lt;li&gt;Why do we have to no_grad() at step and zero_grad function?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Do the one epoch learning with optimizer instance &lt;strong&gt;&lt;em&gt;(3rd Training Loop)&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;See the loss and accuracy &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;Do the one epoch learning using PyTorch’s optim.SGD functionality &lt;strong&gt;&lt;em&gt;(4th Training Loop)&lt;/em&gt;&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Define get_model function&lt;/li&gt;
      &lt;li&gt;return: (1) model instance from nn.Sequential, (2) Optimizer function from optim.SGD&lt;/li&gt;
      &lt;li&gt;See the loss and accuracy &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;dataset-and-dataloader&quot;&gt;Dataset and DataLoader&lt;/h2&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;h3 id=&quot;dataloader&quot;&gt;DataLoader&lt;/h3&gt;

&lt;h3 id=&quot;random-sampling&quot;&gt;Random sampling&lt;/h3&gt;

&lt;h3 id=&quot;pytorch-dataloader&quot;&gt;PyTorch DataLoader&lt;/h3&gt;

&lt;h2 id=&quot;validation&quot;&gt;Validation&lt;/h2&gt;

&lt;h2 id=&quot;unsolved-questions&quot;&gt;Unsolved questions&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;I’ve heard that finding convex downward / upward is not the same and there are preferences. Find out which is easier and explain why. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;What’s relationship with above one epoch trainiing and this one batch loss / accuracy? &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Dunder init including &lt;strong&gt;setattr&lt;/strong&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;I don’t know if I’m getting better loss since I’m using better approach or doing same thing at same data repeatedly -&amp;gt; shallow: check I would get same result when I run this cell before others deep: inspect details of mechanisms each approach uses &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;What does it mean ‘except we’ll be doing it in a more flexible way!’? &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>dionne</name></author><category term="feature" /><summary type="html">Since fastai instructor including many people who studied fastai highly emphasized to copy the Jeremy’s code from scratch, I’ve been using that approach. It was fine with Part1, but from Part2, applying same strategy which was trying to copy nbs from blank cell without peeking up original was not enough.First, there were lots of sub-topics worth to dig in, since we’ve moved to bottom-up phase. And I couldn’t inspect deep inside of each topic(sometimes even don’t know what I am doing) with just replicating same code.Second, literally it was too difficult to rely on my own memory. I should open and close up the Jeremy’s too many times.I thought if I could draw overall picture of each notebook, replicating process would be easier. And it was. (At least with 01_matmul, 02_fully_connected, 02b_initializing)So I share my own supplementary hoping that it could help you to replicate code easily and grasp the lesson Jeremy intended.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/fastai/03.png" /></entry><entry><title type="html">Part2 lesson 9 Note | fastai 2019 course -v3</title><link href="http://localhost:4000/2020/07/note09-fastai/" rel="alternate" type="text/html" title="Part2 lesson 9 Note | fastai 2019 course -v3" /><published>2020-07-07T00:00:00+09:00</published><updated>2020-07-07T00:00:00+09:00</updated><id>http://localhost:4000/2020/07/note09-fastai</id><content type="html" xml:base="http://localhost:4000/2020/07/note09-fastai/">&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;CONTENTS&lt;/h2&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#02a_why_sqrt5ipynb&quot; id=&quot;markdown-toc-02a_why_sqrt5ipynb&quot;&gt;02a_why_sqrt5.ipynb&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#02b_initializingipynb&quot; id=&quot;markdown-toc-02b_initializingipynb&quot;&gt;02b_initializing.ipynb&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#03_minibatch_trainingipynb&quot; id=&quot;markdown-toc-03_minibatch_trainingipynb&quot;&gt;03_minibatch_training.ipynb&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#changing-mse---cross-entropy&quot; id=&quot;markdown-toc-changing-mse---cross-entropy&quot;&gt;Changing mse -&amp;gt; cross entropy&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#refactoring-nnmodule&quot; id=&quot;markdown-toc-refactoring-nnmodule&quot;&gt;refactoring nn.module&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#refactoring-nnoptim&quot; id=&quot;markdown-toc-refactoring-nnoptim&quot;&gt;Refactoring nn.optim&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dataset-and-dataloaders&quot; id=&quot;markdown-toc-dataset-and-dataloaders&quot;&gt;Dataset and DataLoaders&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#dataloader&quot; id=&quot;markdown-toc-dataloader&quot;&gt;DataLoader&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#random-sampling&quot; id=&quot;markdown-toc-random-sampling&quot;&gt;Random sampling&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pytorch-dataloader&quot; id=&quot;markdown-toc-pytorch-dataloader&quot;&gt;Pytorch DataLoader&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#validation&quot; id=&quot;markdown-toc-validation&quot;&gt;Validation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#04_callbacksipynb&quot; id=&quot;markdown-toc-04_callbacksipynb&quot;&gt;04_callbacks.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;02a_why_sqrt5ipynb&quot;&gt;02a_why_sqrt5.ipynb&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fastai/course-v3/blob/master/nbs/dl2/02a_why_sqrt5.ipynb&quot;&gt;course notebook link&lt;/a&gt; &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;convolution layer needs square image.&lt;br /&gt;
as a software developer, be sure to keep refactoring what you’ve implemented. for example, jeremy made stats function&lt;br /&gt;
&lt;strong&gt;todo&lt;/strong&gt; &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; when you don’t understand why the weight shape is &lt;code class=&quot;highlighter-rouge&quot;&gt;32, 1, 5, 5&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_ConvNd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_parameters&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;??&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;check pytorch, they don’t handle with default relu ( this could be little different since video was taken at July, 2019)&lt;br /&gt;
PyTorch doesn’t handle well with initialization.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;li&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rec_fs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;this way we can get the size of receptive filter, number of element.&lt;/p&gt;

&lt;p&gt;How we calculate the receptive fan_in and fan_out at convolutional layer.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;function gain(0), gain(0.1), … , gain(math.sqrt(5.)) &amp;lt;- pytorch’s doing with&lt;br /&gt;
and also they are using &lt;code class=&quot;highlighter-rouge&quot;&gt;kaiming_unit&lt;/code&gt; not &lt;code class=&quot;highlighter-rouge&quot;&gt;normal&lt;/code&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;And jeremy compared his function and pytorch function.&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;02b_initializingipynb&quot;&gt;02b_initializing.ipynb&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fastai/course-v3/blob/master/nbs/dl2/02b_initializing.ipynb&quot;&gt;course notebook link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You are out of keep tracking the numbers(=parameters) that matter &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Some tricky initialization researches and methods would make your own experiment hard. &lt;a href=&quot;https://youtu.be/AcA8HAYh7IE?t=1590&quot;&gt;video&lt;/a&gt; &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;shape of the weight was up-side down, and it was because of some code which was 7 years code. Lua library things. &lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;03_minibatch_trainingipynb&quot;&gt;03_minibatch_training.ipynb&lt;/h1&gt;

&lt;h2 id=&quot;changing-mse---cross-entropy&quot;&gt;Changing mse -&amp;gt; cross entropy&lt;/h2&gt;

&lt;p&gt;first, we will change mse to cross_entropy since it’s categorical variable.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://numpy.org/doc/stable/reference/arrays.indexing.html#integer-array-indexing&quot;&gt;integer array indexing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;note how novel jeremy’s nll function is &amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;p&gt;logsumexp trick(numerical stability trick) : when you are handling the number which is very big, &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;log softmax in negative log likelihood is called cross entropy. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;refactoring-nnmodule&quot;&gt;refactoring nn.module&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;manually going through weight and bias&lt;/li&gt;
  &lt;li&gt;grab all of the parameters of model at once -&amp;gt; DummyModule() will do that&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;dunder &lt;code class=&quot;highlighter-rouge&quot;&gt;__setattr__&lt;/code&gt; : will call this method, when you assigned anything inside self,&lt;/p&gt;

&lt;p&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; the attribute name doesn’t start with underscore,/  because if it does, it might be &lt;code class=&quot;highlighter-rouge&quot;&gt;\_modules&lt;/code&gt; and then it’s going to be like a recursive loop. and also it’s not some internal private stuff.&lt;/p&gt;

&lt;p&gt;And these refactoring things are same with &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Q1: what code Jeremy exchanged to &lt;code class=&quot;highlighter-rouge&quot;&gt;super().__init__()&lt;/code&gt; in &lt;code class=&quot;highlighter-rouge&quot;&gt;SequentialModel&lt;/code&gt; class? &lt;br /&gt;
Q2: what code Jeremy exchanged to &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.ModuleList(layers)&lt;/code&gt; &lt;br /&gt;
Q2: what code Jeremy exchanged to &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.SequentialModel(layers)&lt;/code&gt; &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;And if you look on source code, you will find that we didn’t dumbed down the code &lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;refactoring-nnoptim&quot;&gt;Refactoring nn.optim&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;PyTorch's optim.SGD &lt;/code&gt; is doing some significant things including &lt;strong&gt;&lt;em&gt;weight_decay, momentum, dampening, nestrov&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Two thing that you can learned from his interactive data science experiments.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;when you architect your model, put things that should be checked
    &lt;ul&gt;
      &lt;li&gt;even SGD is theoretically imperfect&lt;/li&gt;
      &lt;li&gt;Jeremy used accuracy here, to check those things.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;didn’t use seed (very intentionally)
    &lt;ul&gt;
      &lt;li&gt;we can observe variations when we run the model different times&lt;/li&gt;
      &lt;li&gt;reproducible science&lt;/li&gt;
      &lt;li&gt;BUT, not on your &lt;strong&gt;&lt;em&gt;model&lt;/em&gt;&lt;/strong&gt; when you want to develop intuitive understanding of your model!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;dataset-and-dataloaders&quot;&gt;Dataset and DataLoaders&lt;/h2&gt;

&lt;p&gt;To avoid iterate separately through mini batches of x and y values! &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Dunders &lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;dataloader&quot;&gt;DataLoader&lt;/h3&gt;

&lt;p&gt;initialize class with datasize and batchsize &lt;br /&gt;
&lt;br /&gt;
Use dunder &lt;code class=&quot;highlighter-rouge&quot;&gt;iter&lt;/code&gt;
&lt;br /&gt;
coroutine function (when jeremy tried to explain iteration
&lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;To the people who feels these processes are so hard
&lt;br /&gt;
Maybe you want to read the code very intuitively, and refactor it like expert, &lt;br /&gt; but it’s very hard to maintain and understand the code. And also this ability is key to be a researcher, because if you want try something, you should know how to do it.
&lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;random-sampling&quot;&gt;Random sampling&lt;/h3&gt;

&lt;p&gt;one of the remained problem is we see the training data are in order, meaning at every epoch, we are giving our model the exactly same bunch of our data with previously given.
&lt;br /&gt;
Sampler class, which has init and iter.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;this structure can be used often when you’re doing streaming computations (so that you out of memory), way of looping through something which is itself for co-routine, and then yielding something which does other things to that.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If you want to use different stacked version of tensor (e.g. padded tensor) you can use different kind of collate function. 
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pytorch-dataloader&quot;&gt;Pytorch DataLoader&lt;/h2&gt;

&lt;p&gt;which does exactly same thing that we’ve done
&lt;br /&gt;
But unfortunately, they don’t have function that grasp just one sample of data. but basically our api and pytorch are doing same thing.
&lt;br /&gt;
One thing we didn’t implemented which is in pytorch, &lt;code class=&quot;highlighter-rouge&quot;&gt;num_workers&lt;/code&gt; &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;validation&quot;&gt;Validation&lt;/h2&gt;

&lt;p&gt;Don’t forget to call &lt;code class=&quot;highlighter-rouge&quot;&gt;model.train()&lt;/code&gt; / &lt;code class=&quot;highlighter-rouge&quot;&gt;model.eval()&lt;/code&gt; before you start train/validation, since technique like batchnorm, dropout should be applied to training only, not validation/evaluation data. &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;
&lt;br /&gt;
Also, we don’t backward with validation data, but just keep tracking of accuracy and loss in batch.
&lt;br /&gt;
Question: Are these validation results correct if batch size varies? &lt;a href=&quot;https://youtu.be/AcA8HAYh7IE?t=4392&quot;&gt;Lecture video&lt;/a&gt;
&lt;br /&gt;
✔︎ Why this could be problem?
&lt;br /&gt;
we have 2 batches and one minibatch size of 1000, and the other size of 1, then you must add up with weighted average, not just dividing by 2!
&lt;br /&gt;
&lt;img src=&quot;/assets/images/l9-f1.jpeg&quot; alt=&quot;figure1&quot; /&gt;
&lt;br /&gt;
Be careful, lots of (almost most of) library does things like this without considering size of batch (of course fast.ai is doing well :wink)
&lt;br /&gt;
when you see the &lt;code class=&quot;highlighter-rouge&quot;&gt;get_dls&lt;/code&gt; function, you will notice that batch size in validation data is twice of the train’s -&amp;gt; Since we don’t have to save our backward gradients at validation data set, we have much more space!
&lt;br /&gt;
&lt;em&gt;Q1. Why do we have to zero our gradient at PyTorch?&lt;/em&gt;
&lt;br /&gt;
J: just before &lt;code class=&quot;highlighter-rouge&quot;&gt;using parameters and optim&lt;/code&gt;, we made the gradients zero (regardless of pytorch).  You can control easily, when to optimize or gather up gradients. For example, suppose you just can have size 2 of mini-batch (matter of GPU, whatever). Then if you want to double the actual batch size of learning, you just run the optimizer and zero grad for every 2 iterations of batch_size (or you can get three times of batch size when you do it for every 3 iterations).
&lt;br /&gt; This is called gradient accumulation, and it depends on your model.&lt;/p&gt;

&lt;h1 id=&quot;04_callbacksipynb&quot;&gt;04_callbacks.ipynb&lt;/h1&gt;

&lt;p&gt;Sylvain’s talk, An Infinitely Customizable Training Loop &lt;a href=&quot;https://www.youtube.com/watch?v=roc-dOSeehM&quot;&gt;Video&lt;/a&gt;, &lt;a href=&quot;https://drive.google.com/open?id=1eWWpyHeENyNNCVTtblX2Jm02WZWw-Kes&quot;&gt;Slide&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://youtu.be/AcA8HAYh7IE?t=4998&quot;&gt;current time in lecture&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Study `conv-example.xlsx’ excel &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;https://youtu.be/AcA8HAYh7IE?t=2996 &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;find related PyTorch document &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Jeremy said if we don’t separate train/valid, we will get awful results when test. DO IT BY YOURSELF! &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>dionne</name></author><category term="feature" /><summary type="html">CONTENTS</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/cnn-jiwon.png" /></entry><entry><title type="html">CS224N | Winter2019 | Assignment 3 Solution</title><link href="http://localhost:4000/2020/06/cs224n-19w-a3/" rel="alternate" type="text/html" title="CS224N | Winter2019 | Assignment 3 Solution" /><published>2020-06-18T00:00:00+09:00</published><updated>2020-06-18T00:00:00+09:00</updated><id>http://localhost:4000/2020/06/cs224n-19w-a3</id><content type="html" xml:base="http://localhost:4000/2020/06/cs224n-19w-a3/">&lt;h1 class=&quot;no_toc&quot; id=&quot;cs224n-2019w---assignment3-solution&quot;&gt;CS224n, 2019W - Assignment3 Solution&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;HW3:&lt;/strong&gt; Dependency parsing and neural network foundations&lt;br /&gt; you can find material: &lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a3.zip&quot;&gt;code&lt;/a&gt; &lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a3.pdf&quot;&gt;handout&lt;/a&gt;&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;table-of-contents&quot;&gt;Table of contents&lt;/h2&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-machine-learning--neural-networks-8-points&quot; id=&quot;markdown-toc-1-machine-learning--neural-networks-8-points&quot;&gt;1. Machine Learning &amp;amp; Neural Networks (8 points)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#a-adam-optimizer&quot; id=&quot;markdown-toc-a-adam-optimizer&quot;&gt;(a) Adam Optimizer&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#i&quot; id=&quot;markdown-toc-i&quot;&gt;i.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#ii&quot; id=&quot;markdown-toc-ii&quot;&gt;ii.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#b-dropout&quot; id=&quot;markdown-toc-b-dropout&quot;&gt;(b) Dropout&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#i-1&quot; id=&quot;markdown-toc-i-1&quot;&gt;i.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#ii-1&quot; id=&quot;markdown-toc-ii-1&quot;&gt;ii.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-neural-transition-based-dependency-parsing-42-points&quot; id=&quot;markdown-toc-2-neural-transition-based-dependency-parsing-42-points&quot;&gt;2. Neural Transition-Based Dependency Parsing (42 points)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#a&quot; id=&quot;markdown-toc-a&quot;&gt;(a)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#b&quot; id=&quot;markdown-toc-b&quot;&gt;(b)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#c-d-e&quot; id=&quot;markdown-toc-c-d-e&quot;&gt;(c), (d), (e)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#f&quot; id=&quot;markdown-toc-f&quot;&gt;(f)&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#i-2&quot; id=&quot;markdown-toc-i-2&quot;&gt;i.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#ii-2&quot; id=&quot;markdown-toc-ii-2&quot;&gt;ii.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#iii&quot; id=&quot;markdown-toc-iii&quot;&gt;iii.&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#iv&quot; id=&quot;markdown-toc-iv&quot;&gt;iv.&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-machine-learning--neural-networks-8-points&quot;&gt;1. Machine Learning &amp;amp; Neural Networks (8 points)&lt;/h2&gt;

&lt;h3 id=&quot;a-adam-optimizer&quot;&gt;(a) Adam Optimizer&lt;/h3&gt;

&lt;h4 id=&quot;i&quot;&gt;i.&lt;/h4&gt;

&lt;p&gt;Momentum is working as acceleration reflecting the previous gradients tendency.&lt;br /&gt;
Hight variance is well known to raising overfitting problem.&lt;/p&gt;

&lt;h4 id=&quot;ii&quot;&gt;ii.&lt;/h4&gt;

&lt;p&gt;Parameters that previous gradients are small and not volatile get larger updates.&lt;br /&gt;
This helps model to handle with sparse gradients(merits of AdaGrad) and also non-stationary objectives(merits of RMSProp)&lt;/p&gt;

&lt;h3 id=&quot;b-dropout&quot;&gt;(b) Dropout&lt;/h3&gt;

&lt;h4 id=&quot;i-1&quot;&gt;i.&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma = \frac{1}{1-P_{drop}}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/a3-1-b.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;ii-1&quot;&gt;ii.&lt;/h4&gt;

&lt;p&gt;Drop out is one of the regularizations, which restrict an overfitting. Randomly setting units zero is for make units stronger on abrupt absence of other units.&lt;/p&gt;

&lt;p&gt;And evaluation is made from train, to model being improved. But if we use drop out also at evaluation, it does re-trained with same circumstances, so that can’t do well in test.&lt;/p&gt;

&lt;h2 id=&quot;2-neural-transition-based-dependency-parsing-42-points&quot;&gt;2. Neural Transition-Based Dependency Parsing (42 points)&lt;/h2&gt;

&lt;h3 id=&quot;a&quot;&gt;(a)&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Stack&lt;/th&gt;
      &lt;th&gt;Buffer&lt;/th&gt;
      &lt;th&gt;New dependency&lt;/th&gt;
      &lt;th&gt;Transition&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT]&lt;/td&gt;
      &lt;td&gt;[I, parsed, this, sentence, correctly]&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Initial&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT]&lt;/td&gt;
      &lt;td&gt;[I, parsed, this, sentence, correctly]&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SHIFT&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT, I]&lt;/td&gt;
      &lt;td&gt;[parsed, this, sentence, correctly]&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SHIFT&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT, I, parsed]&lt;/td&gt;
      &lt;td&gt;[this, sentence, correctly]&lt;/td&gt;
      &lt;td&gt;parsed$\rightarrow$&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;LEFT-ARC&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT, parsed]&lt;/td&gt;
      &lt;td&gt;[this, sentence, correctly]&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SHIFT&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT, parsed, this]&lt;/td&gt;
      &lt;td&gt;[sentence, correctly]&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SHIFT&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT, parsed, this, sentence]&lt;/td&gt;
      &lt;td&gt;[correctly]&lt;/td&gt;
      &lt;td&gt;sentence$\rightarrow$this&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;LEFT-ARC&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT, parsed, sentence]&lt;/td&gt;
      &lt;td&gt;[correctly]&lt;/td&gt;
      &lt;td&gt;parsed$\rightarrow$sentence&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RIGHT-ARC&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT, parsed]&lt;/td&gt;
      &lt;td&gt;[correctly]&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SHIFT&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT, parsed, correctly]&lt;/td&gt;
      &lt;td&gt;[]&lt;/td&gt;
      &lt;td&gt;parsed$\rightarrow$correctly&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RIGHT-ARC&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT, parsed]&lt;/td&gt;
      &lt;td&gt;[]&lt;/td&gt;
      &lt;td&gt;ROOT$\rightarrow$parsed&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RIGHT-ARC&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[ROOT]&lt;/td&gt;
      &lt;td&gt;[]&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;b&quot;&gt;(b)&lt;/h3&gt;

&lt;dl&gt;
  &lt;dt&gt;2n&lt;br /&gt;&lt;/dt&gt;
  &lt;dd&gt;n steps for each of buffer and stack&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Whole scripts are &lt;a href=&quot;https://github.com/SpellOnYou/10000-days-of-code/tree/master/code/cs224n-hw/a3&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;c-d-e&quot;&gt;(c), (d), (e)&lt;/h3&gt;

&lt;script src=&quot;https://gist.github.com/SpellOnYou/52755d046fc8be9aa2ac2a127ab5fe3c.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Tips: problem 2-d needs shallow copy, not deep copy and TA advised not to use &lt;code class=&quot;highlighter-rouge&quot;&gt;del&lt;/code&gt; function, here is good explaination &lt;a href=&quot;https://stackoverflow.com/a/45572488/7934832&quot;&gt;remove vs del&lt;/a&gt;
&lt;img src=&quot;/assets/images/cs224n/a3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;f&quot;&gt;(f)&lt;/h3&gt;

&lt;h4 id=&quot;i-2&quot;&gt;i.&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Error type&lt;/strong&gt;: Verb Phrase Attachment Error&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Incorrect dependency&lt;/strong&gt;: wedding → fearing&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Correct dependency&lt;/strong&gt;: heading → fearing&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;ii-2&quot;&gt;ii.&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Error type&lt;/strong&gt;: Coordination Attachment Error&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Incorrect dependency&lt;/strong&gt;:  rescue → and&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Correct dependency&lt;/strong&gt;: rescue → rush&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;iii&quot;&gt;iii.&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Error type&lt;/strong&gt;: Prepositional Phrase Attachment Error&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Incorrect dependency&lt;/strong&gt;:  named → Midland&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Correct dependency&lt;/strong&gt;: guy → Midland&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;iv&quot;&gt;iv.&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Error type&lt;/strong&gt;: Modifier Attachment Error&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Incorrect dependency&lt;/strong&gt;: element → most&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Correct dependency&lt;/strong&gt;: crucial → most&lt;/li&gt;
&lt;/ul&gt;</content><author><name>dionne</name></author><summary type="html">CS224n, 2019W - Assignment3 Solution</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/cs224n/gate-l5.png" /></entry><entry><title type="html">CS224N: NLP with Deep Learning | Winter2019 | Lecture5</title><link href="http://localhost:4000/2020/06/cs224n-19w-course5/" rel="alternate" type="text/html" title="CS224N: NLP with Deep Learning | Winter2019 | Lecture5" /><published>2020-06-10T00:00:00+09:00</published><updated>2020-06-10T00:00:00+09:00</updated><id>http://localhost:4000/2020/06/cs224n-19w-course5</id><content type="html" xml:base="http://localhost:4000/2020/06/cs224n-19w-course5/">&lt;h1 id=&quot;lecture-5-dependency-parsing&quot;&gt;Lecture 5: Dependency Parsing&lt;/h1&gt;

&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of contents&lt;/h2&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#lecture-5-dependency-parsing&quot; id=&quot;markdown-toc-lecture-5-dependency-parsing&quot;&gt;Lecture 5: Dependency Parsing&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#table-of-contents&quot; id=&quot;markdown-toc-table-of-contents&quot;&gt;Table of contents&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#1-consistency-and-dependency&quot; id=&quot;markdown-toc-1-consistency-and-dependency&quot;&gt;1) Consistency and Dependency&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-dependency-grammar&quot; id=&quot;markdown-toc-2-dependency-grammar&quot;&gt;2) Dependency Grammar&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-treebanks&quot; id=&quot;markdown-toc-2-treebanks&quot;&gt;2) Treebanks&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#dependency-conditioning-preferences-some-dependency-rules&quot; id=&quot;markdown-toc-dependency-conditioning-preferences-some-dependency-rules&quot;&gt;Dependency Conditioning Preferences (some dependency rules)&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-arc-standard-transition-based-parser&quot; id=&quot;markdown-toc-3-arc-standard-transition-based-parser&quot;&gt;3) Arc-standard transition-based parser&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4-neural-dependency-parsing&quot; id=&quot;markdown-toc-4-neural-dependency-parsing&quot;&gt;4) Neural dependency parsing&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;

&lt;p&gt;“We will focus more on how human language is made and used”&lt;/p&gt;

&lt;h2 id=&quot;1-consistency-and-dependency&quot;&gt;1) Consistency and Dependency&lt;/h2&gt;

&lt;p&gt;1) Phrase Structure [Grammar]&lt;br /&gt;
organizes words into &lt;strong&gt;nested constitutes&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Phrase structure examples
    &lt;ul&gt;
      &lt;li&gt;NP -&amp;gt; DET + (ADJ) + N + PP&lt;/li&gt;
      &lt;li&gt;PP -&amp;gt; Prep + NP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Different languages have different phrase structure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) Dependency Structure
&lt;br /&gt;: which words &lt;em&gt;depend on&lt;/em&gt;(modify / arguments) which other words&lt;/p&gt;

&lt;p&gt;Note: Having study with TWIML community, folks wondered why this is ‘Context free’ &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why this syntactic structure is important?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1) Prepositional phrase attachment &lt;strong&gt;ambiguity&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;San Jose cops kill man with knife&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;with knife&lt;/code&gt; modifies &lt;code class=&quot;highlighter-rouge&quot;&gt;man&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;with knife&lt;/code&gt; modifies &lt;code class=&quot;highlighter-rouge&quot;&gt;San Jose cops kill man&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;The board approved [its acquisition] [by Royal Trustco Ltd] [of Toronto] [for $27 a share] [at its monthly meeting].&lt;/code&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/l5-pp.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Catalan_number&quot;&gt;Catalan numbers&lt;/a&gt; &lt;script type=&quot;math/tex&quot;&gt;C_n = \frac{2n!}{(n+1)!n!}&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An Exponentially growing series, which arises in many tree-like contexts:&lt;/li&gt;
  &lt;li&gt;e.g., the number of possible triangulations of a polygon with n+2 sides
    &lt;ul&gt;
      &lt;li&gt;turns up in triangulation of probabilistic graphical models(cs228)…&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) Coordination scope ambiguity&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Doctor: No heart, cognitive issues&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;No [heart and/or cognitive] issues&lt;/li&gt;
  &lt;li&gt;[No heart] and [cognitive issues]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;3) Adjectival Modifier Ambiguity&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Students get first hand job experience&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Students get [[first hand] [job] experience]&lt;/li&gt;
  &lt;li&gt;Students get [[first [hand job] experience]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;4) Verb Phrase (VP) attachment ambiguity&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Mutilated body washes up on Rio beach to be used for Olympics beach volleyball&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mutilated body washes up on [Rio [beach to be used for Olympics beach volleyball]]&lt;/li&gt;
  &lt;li&gt;Mutilated body [washes up on Rio beach] to be used for Olympics beach volleyball&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;2-dependency-grammar&quot;&gt;2) Dependency Grammar&lt;/h2&gt;

&lt;p&gt;Two ways of representing the dependency structure of sentence&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;in a line and drawing arrows&lt;/li&gt;
  &lt;li&gt;tree structure&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Generally, people type arrows with the name of grammatical relationships, but in this case we will not go that far ahead.&lt;/p&gt;

&lt;p&gt;Dependency tree is acyclic, single-head, connected.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dependency Grammar/Parsing history&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-treebanks&quot;&gt;2) Treebanks&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://universaldependencies.org/&quot;&gt;Universal Dependencies&lt;/a&gt;, cf (#todo fill out)&lt;/p&gt;

&lt;p&gt;People annotated structure dependencies heuristically (And this project handles with many languages other than English)&lt;/p&gt;

&lt;h3 id=&quot;dependency-conditioning-preferences-some-dependency-rules&quot;&gt;Dependency Conditioning Preferences (some dependency rules)&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Only one word is a dependent of ROOT&lt;/li&gt;
  &lt;li&gt;Don’t make it cyclic.&lt;/li&gt;
  &lt;li&gt;in little case, bootstrapping happens(when parse becomes non-projective)
    &lt;ul&gt;
      &lt;li&gt;will comment further at next class&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;3-arc-standard-transition-based-parser&quot;&gt;3) Arc-standard transition-based parser&lt;/h2&gt;

&lt;p&gt;Shift &lt;code class=&quot;highlighter-rouge&quot;&gt;Buffer&lt;/code&gt;’s element to left &lt;code class=&quot;highlighter-rouge&quot;&gt;Stack&lt;/code&gt;, until stack finds &lt;code class=&quot;highlighter-rouge&quot;&gt;Head&lt;/code&gt; component.
&lt;br /&gt;-&amp;gt; When you find head at stack, start to reduce
&lt;br /&gt;-&amp;gt; but notice not to reduce &lt;code class=&quot;highlighter-rouge&quot;&gt;head&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;root&lt;/code&gt; until buffer has no element.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Left Arc means reduction of left argument, and keep head&lt;/li&gt;
  &lt;li&gt;Right Arc means reduction of right argument, and keep head.&lt;/li&gt;
  &lt;li&gt;Finish condition is when buffer is empty, and stack has only one element, which is &lt;code class=&quot;highlighter-rouge&quot;&gt;[root]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But the problem of this method is how we choose the next step is uncertain.&lt;br /&gt;
Usually these problem was handled using Dynamic programming to avoid too much selection cases.&lt;/p&gt;

&lt;h2 id=&quot;4-neural-dependency-parsing&quot;&gt;4) Neural dependency parsing&lt;/h2&gt;

&lt;p&gt;1) MaltParser&lt;/p&gt;

&lt;p&gt;[Nivre and Hall 2005]&lt;/p&gt;

&lt;p&gt;built ML classifier,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Accuracy = \frac{\#\ correct\ deps}{\#\ of\ deps}&lt;/script&gt;

&lt;p&gt;UAS = 4 / 5 = 80% (exclude label)
&lt;br /&gt;LAS =  2 / 5 = 40% (include label)&lt;/p&gt;

&lt;p&gt;2) Conventional Feature Representation was very complex.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;incomplete&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;expensive computation&lt;/strong&gt; (critical problem)&lt;/li&gt;
  &lt;li&gt;sparse&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3) Neural dependency parsers&lt;/p&gt;

&lt;p&gt;[Chen and Manning 2014]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sent. / s&lt;/code&gt; means number of sentences that algorithm can perform.&lt;/li&gt;
  &lt;li&gt;Not only fastest methods, but also accuracy gets almost highest.&lt;/li&gt;
  &lt;li&gt;the dense representation is the key&lt;/li&gt;
  &lt;li&gt;Used &lt;strong&gt;treebanks&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4) Further researches&lt;/p&gt;

&lt;p&gt;done by Google&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using bigger, deeper networks, tuned hyperparameters&lt;/li&gt;
  &lt;li&gt;Beam search&lt;/li&gt;
  &lt;li&gt;Global inference, CRF over the decision sequence&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;D. Jurafsky and J. H. Martin, &lt;em&gt;Speech and Language Processing&lt;/em&gt;, 2/e, Prentice Hall, 2008. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>dionne</name></author><summary type="html">Lecture 5: Dependency Parsing</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/cs224n/gate-l5.png" /></entry><entry><title type="html">CS224N: NLP with Deep Learning | Winter2019 | Lecture2</title><link href="http://localhost:4000/2020/06/cs224n-19w-course2/" rel="alternate" type="text/html" title="CS224N: NLP with Deep Learning | Winter2019 | Lecture2" /><published>2020-06-05T00:00:00+09:00</published><updated>2020-06-05T00:00:00+09:00</updated><id>http://localhost:4000/2020/06/cs224n-19w-course2</id><content type="html" xml:base="http://localhost:4000/2020/06/cs224n-19w-course2/">&lt;p&gt;Images: Vector directions related to word classes (&lt;a href=&quot;https://www.semanticscholar.org/paper/An-Improved-Model-of-Semantic-Similarity-Based-on-Rohde-Plaut/73e6351a8fb61afc810a8bb3feaa44c41e5c5d7b&quot;&gt;Rohde et al. 2005&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Course: &lt;a href=&quot;http://onlinehub.stanford.edu/cs224&quot;&gt;Stanford cs224n 2019 winter, lecture 02&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;word-vectors-and-word-senses&quot;&gt;Word Vectors and Word Senses&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Finish last time’s lecture&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(showed more examples)&lt;/p&gt;

&lt;p&gt;and we can see the problem it can’t represent the &lt;strong&gt;polysemy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;drawed PCA: remember this way we lose lots of information because we just chose first two principle components&lt;/p&gt;

&lt;p&gt;&lt;em&gt;purpose of this class is just end of the course you can read paper(from classical to contemporary), and understand them&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;what is parameter of word2vec&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Only two, input vector and output vector&lt;/p&gt;

&lt;p&gt;each vector is represented as ROW (at almost all of modern ML library)&lt;/p&gt;

&lt;p&gt;We are going with just one &lt;strong&gt;probability&lt;/strong&gt; -&amp;gt; same prediction at each point.&lt;/p&gt;

&lt;p&gt;quite interesting cz model is so simple and works well&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tip: &lt;a href=&quot;https://en.wikipedia.org/wiki/Function_word&quot;&gt;Function word&lt;/a&gt; has fix high probability, (they are closed) so removing it results in better Word Vectors &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Brief explain of optimization&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;actual loss function would be more complex, bumpy, not convex&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Problem of this model and Stochastic Gradient Descent&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;our objective function has to deal with every each element of corpora.&lt;/li&gt;
  &lt;li&gt;Nobody uses this since &lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_complexity_theory&quot;&gt;cost efficiency&lt;/a&gt; is very horrible&lt;/li&gt;
  &lt;li&gt;So we use SGD (not just one data but with group, which is called &lt;code class=&quot;highlighter-rouge&quot;&gt;batch&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Shortage of SGD&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tends to end up sparse distribution, so that usually use (probability) smoothing.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Details of Word2vec&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Why do we learn two vectors ? M: effective, easy to partial derivative. if we use just one parameter, math becomes more difficult, but practically you average it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;skip-grams/CBOWs&lt;/li&gt;
  &lt;li&gt;Naive softmax is slow because it use all the vocabulary.&lt;br /&gt;
Idea: Train &lt;code class=&quot;highlighter-rouge&quot;&gt;Binary Logistic Regression&lt;/code&gt;
    &lt;ol&gt;
      &lt;li&gt;numerator : actually observed, give high prob&lt;/li&gt;
      &lt;li&gt;denominator : noise, which was randomly selected.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HW2: The skip-gram model with negative sampling&lt;br /&gt;
&lt;strong&gt;k&lt;/strong&gt; would be anything the size of sample you want to choose&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(w) = \frac{U(w)^{\frac{3}{4}}}{Z}&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It comes out that this experiment is not that replicable(which needs lots of hyper-parameter, tricks)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;randomly select the batches in corpus, for each iteration, not ordering and sequencing, and this also saves some memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;why not use n-gram, window-based co-occurrence matrix, and this became sparse matrix
-&amp;gt; occupy so much bigger space, not that robust.
-&amp;gt; then what about other dimension reduction methods?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;(HW1)&lt;/strong&gt; SVD, factorize the matrix&amp;lt;/br&amp;gt;results: least square error in estimation
&lt;br /&gt;this way we can also make word-vectors&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;cant-we-approach-to-build-model-using-frequency---glove&quot;&gt;Can’t we approach to build model using frequency? - Glove&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Hacks to X (&lt;a href=&quot;https://www.semanticscholar.org/paper/An-Improved-Model-of-Semantic-Similarity-Based-on-Rohde-Plaut/73e6351a8fb61afc810a8bb3feaa44c41e5c5d7b&quot;&gt;Rohde et al. 2005&lt;/a&gt;)
&lt;br /&gt; student at CMU&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Remove too much frequent words which is function words&lt;/li&gt;
  &lt;li&gt;weigh more where it is closer&lt;/li&gt;
  &lt;li&gt;Use Pearson correlation instead of counts, then set negative values to 0&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;( And this techs were used in word2vec)&lt;/p&gt;

&lt;p&gt;Sort of direction in vector spaces matches the word’s feature, and below pic shows it matches with POS, in this case verb and noun&lt;/p&gt;

&lt;p&gt;And this is meaningful because this proved constructed VS does well in analogy.&lt;/p&gt;

&lt;p&gt;Conventional methods also can give you good vectors.&lt;/p&gt;

&lt;p&gt;And this could be the origin of &lt;code class=&quot;highlighter-rouge&quot;&gt;Glove&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Count based&lt;/strong&gt; vs &lt;strong&gt;direct based.&lt;/strong&gt;
&lt;br /&gt;&lt;em&gt;Direct based model&lt;/em&gt; goes sample by sample so that can’t use that well the statistics
&lt;br /&gt; On the other hand, &lt;em&gt;Count based model&lt;/em&gt;(usually classical model) can use stats more efficiently and also the memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoding meaning in vector differences (= fraction of log) Using co-occurence
&lt;br /&gt; &lt;strong&gt;Insight&lt;/strong&gt;: Ratio of co-occurrence probabilities can encode meaning components. (not enough just co-occurrence!)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Q.&lt;/em&gt;&lt;/strong&gt; How can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space?&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;Dot product should become similar as much as possible with log of co-occurrence&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/hy4kyit2a/f_auto,fl_lossy,q_70/learn/modules/glove-and-word-vectors-for-sentiment-analysis/use-glove-for-natural-language-processing/images/30c8354b6a239d6f0d9235189ce8d676_gloves-objective-function.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(#TODO1: check again, can’t understand)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;how-to-evaluate-word-vectors&quot;&gt;How to evaluate word vectors?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;intrinsic vs extrinsic(use in real system(=real application) e.x. QA, web search…)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;intrinsic ex: calculate cosine, and see if it matches with language intuition&lt;/p&gt;

&lt;p&gt;(Tot. means analogy)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Comparing using hyper parameter (i.e., Vector dimensions, Window sizes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/evaluation-of-word2vec.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;if you only use context which is at one side, that is not as good as using both sided matrix (#TODO2: check the &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;codes&lt;/a&gt;)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On the Dimensionality of Word Embedding&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;mathy ideas using matrix perturbation idea.
&lt;br /&gt; =&amp;gt; if you increase dimensions, the performance gets flatten and they proved using perturbation theory (??)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;much time helps, and wikipedia is better than news data (1.6b wiki data is better than 4.3b of news data in web) when making word vectors&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WordSim353: Human judgement, which was from psycology&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;more-problem-regarding-word-senses-ambiguity-polysemy&quot;&gt;More problem regarding word senses (Ambiguity, Polysemy)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Most words have lots of meaning - ex. pike
    &lt;ul&gt;
      &lt;li&gt;common words&lt;/li&gt;
      &lt;li&gt;existed for a long time&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Actually, this argument was existed when word2vec  came out, with labelling polysemy(multiple senses) and embeds it. (Huang et al. 2012), and cluster the departed words
&lt;br /&gt; =&amp;gt; but with this method, the senses are not that clear.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Linear Algebraic Structure of Word Senses, with Application to Polysemy (Arora, Ma, TACL2018)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WHY nlp people are mad at Word2Vec idea?&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;It came out that good word2vec directly related with subtask(extrinsic tasks) enhancement (like, name entity)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;see the paper of Sanjeev Arora’s group / ??? &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>dionne</name></author><summary type="html">Images: Vector directions related to word classes (Rohde et al. 2005)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/cs224n/gate-l2.png" /></entry><entry><title type="html">CS224N: NLP with Deep Learning | Winter2019 | Lecture1</title><link href="http://localhost:4000/2020/05/cs224n-19w-course1/" rel="alternate" type="text/html" title="CS224N: NLP with Deep Learning | Winter2019 | Lecture1" /><published>2020-05-30T00:00:00+09:00</published><updated>2020-05-30T00:00:00+09:00</updated><id>http://localhost:4000/2020/05/cs224n-19w-course1</id><content type="html" xml:base="http://localhost:4000/2020/05/cs224n-19w-course1/">&lt;h1 id=&quot;introduction-and-word-vectors&quot;&gt;Introduction and Word Vectors&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;What is language?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Think about how complicated Human language is. - xkcd cartoon&lt;/p&gt;

&lt;p&gt;Considering the information theory, Human language is so slow&lt;/p&gt;

&lt;p&gt;Is the human language is just same with orangutan?&lt;/p&gt;

&lt;p&gt;Commonest linguistic way of thinking of meaning is a denotational meaning(see referential linguistics)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;with computer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But this definition is hard to implement with computer&lt;/p&gt;

&lt;p&gt;-&amp;gt; &lt;strong&gt;WordNet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;But wordnet is incomplete because it can’t handle with&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Traditional approach (a localist representation)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;treating each word as discrete symbols&lt;/strong&gt;: As categorical variable&lt;/p&gt;

&lt;p&gt;but with this approach, we can’t cover all of derivative.&lt;/p&gt;

&lt;p&gt;Moreover, &lt;strong&gt;relationship&lt;/strong&gt;  can’t be represented using localist representation.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Distributional Semantics&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;J. R. Firth 1957&lt;/p&gt;

&lt;p&gt;Let’s see all of the examples of usage, and represent their appearance using dense vectors.&lt;/p&gt;

&lt;p&gt;the picture is just projects of 100-d vectors to 2-d (will talk about this later) / Be sure to keep in mind when you see the word-embeddings, there’s original vector space.&lt;/p&gt;

&lt;p&gt;Q. Are there standard features for i-th dimension in vector representation?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Word2vec&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Mikolov paper&lt;/p&gt;

&lt;p&gt;vector representation is the only parameter with simple word2vec model(actually two)
1) center word vectors
2) context word vectors&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Objective function (changed little bit from likelihood)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1) Negative: minimize rather than maximize 
2) divide by size of corpus(T) i.e. scaling: make it independent of corpus size
3) log: turned out works well when you do things like optimization - it changes the objective function, which was originally was product, to sum&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Prediction function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;exp : makes number positive&lt;/p&gt;

&lt;p&gt;this function has two parts&lt;/p&gt;

&lt;p&gt;1) as doing the dot product, it calculates similarity (=relationship) between specific word(c) and context(o)
2) blue part means mapping from variable to probability distribution [^1]&lt;/p&gt;

&lt;p&gt;Q-&amp;gt;A: &lt;strong&gt;distributional&lt;/strong&gt; of meaning assumes we have intelligent vector which knows what word will be, when I give other words. so, we get high probability&lt;/p&gt;

&lt;p&gt;And notice a word has only one vector, regardless of the context. (since we have only one simple probability distribution)&lt;/p&gt;

&lt;p&gt;(explained formula / how to calculate)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;But be sure to you can calculate what’s happening in calculation process&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;we get this results, meaning we are sliding down the slope by
1) get a observed representation 
2) &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;jupyter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;stanford has lightweight version of glove&lt;/p&gt;

&lt;p&gt;disimilar could be useful, because this has multiple dimensions, so that each vector space has some feature, like women and man, and we can add/subtract that feature between word
-&amp;gt; makes us be able to do &lt;strong&gt;analogy&lt;/strong&gt;, using this semantic relationship.&lt;/p&gt;

&lt;p&gt;and this is foundation of modern distributed neural representations of words&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;this is a fourth declensions noun.&lt;/p&gt;

&lt;p&gt;So the plural of corpus is corpora.&lt;/p&gt;

&lt;p&gt;And whereas if you say&lt;/p&gt;

&lt;p&gt;core Pi everyone will know that you didn’t study Latin in high school.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;[^1]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;soft and max part&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;that’s sort of a hard max.&lt;/p&gt;

&lt;p&gt;Um, soft- this is a softmax because the exponenti- you know,&lt;/p&gt;

&lt;p&gt;if you sort of imagine this but- if we just ignore the problem&lt;/p&gt;

&lt;p&gt;negative numbers for a moment and you got rid of the exp, um,&lt;/p&gt;

&lt;p&gt;then you’d sort of coming out with&lt;/p&gt;

&lt;p&gt;a probability distribution but by and large it’s so be fairly&lt;/p&gt;

&lt;p&gt;flat and wouldn’t particularly pick out the max of&lt;/p&gt;

&lt;p&gt;the different XI numbers whereas when you exponentiate them,&lt;/p&gt;

&lt;p&gt;that sort of makes big numbers way bigger and so, this,&lt;/p&gt;

&lt;p&gt;this softmax sort of mainly puts mass where the max’s or the couple of max’s are.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;the context word and we’re subtracting from that what our model thinks um,&lt;/p&gt;

&lt;p&gt;the context should look like.&lt;/p&gt;

&lt;p&gt;What does the model think that the context should look like?&lt;/p&gt;

&lt;p&gt;This part here is formal in expectation.&lt;/p&gt;

&lt;p&gt;So, what you’re doing is you’re finding the weighted average&lt;/p&gt;

&lt;p&gt;of the models of the representations of each word,&lt;/p&gt;

&lt;p&gt;multiplied by the probability of it in the current model.&lt;/p&gt;

&lt;p&gt;So, this is sort of the expected context word according to our current model,&lt;/p&gt;

&lt;p&gt;and so we’re taking the difference between&lt;/p&gt;

&lt;p&gt;the expected context word and the actual context word that showed up,&lt;/p&gt;

&lt;p&gt;and that difference then turns out to exactly give&lt;/p&gt;

&lt;p&gt;us the slope as to which direction we should be&lt;/p&gt;

&lt;p&gt;walking changing the words&lt;/p&gt;

&lt;p&gt;representation in order to improve our model’s ability to predict.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;the observed representation of &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>dionne</name></author><summary type="html">Introduction and Word Vectors</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/cs224n/gate-l1.png" /></entry><entry><title type="html">Part1 lesson7 note | fastai 2020 course-v4</title><link href="http://localhost:4000/2020/04/v4-2020-lesson07/" rel="alternate" type="text/html" title="Part1 lesson7 note | fastai 2020 course-v4" /><published>2020-04-29T00:00:00+09:00</published><updated>2020-04-29T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/v4-2020-lesson07</id><content type="html" xml:base="http://localhost:4000/2020/04/v4-2020-lesson07/">&lt;p&gt;Note of lesson7 : &lt;a href=&quot;https://www.usfca.edu/data-institute/certificates/deep-learning-part-one&quot;&gt;Deep Learning Part 1 of Spring 2020 at USF&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Today we will study&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Collaborative Filtering&lt;/li&gt;
  &lt;li&gt;Tabular modelling&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;CONTENTS&lt;/h2&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#collaborative-filtering&quot; id=&quot;markdown-toc-collaborative-filtering&quot;&gt;Collaborative Filtering&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#collaborative-filtering-from-scratch&quot; id=&quot;markdown-toc-collaborative-filtering-from-scratch&quot;&gt;Collaborative filtering from scratch&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#weight-decay&quot; id=&quot;markdown-toc-weight-decay&quot;&gt;Weight decay&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#creating-our-own-embedding-module&quot; id=&quot;markdown-toc-creating-our-own-embedding-module&quot;&gt;Creating our own Embedding module&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#interpreting-embeddings-and-biases&quot; id=&quot;markdown-toc-interpreting-embeddings-and-biases&quot;&gt;Interpreting embeddings and biases&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#using-fastaicollab&quot; id=&quot;markdown-toc-using-fastaicollab&quot;&gt;Using fastai.collab&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#embedding-distance&quot; id=&quot;markdown-toc-embedding-distance&quot;&gt;Embedding distance&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#boot-strapping-a-collaborative-filtering&quot; id=&quot;markdown-toc-boot-strapping-a-collaborative-filtering&quot;&gt;Boot strapping a collaborative filtering&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#deep-learning-for-collaborative-filtering&quot; id=&quot;markdown-toc-deep-learning-for-collaborative-filtering&quot;&gt;Deep Learning for collaborative filtering&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#sidebar-kwargs-and-delegates&quot; id=&quot;markdown-toc-sidebar-kwargs-and-delegates&quot;&gt;Sidebar: kwargs and delegates&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#end-sidebar&quot; id=&quot;markdown-toc-end-sidebar&quot;&gt;End sidebar&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#questionnaire&quot; id=&quot;markdown-toc-questionnaire&quot;&gt;Questionnaire&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tabular-modelling&quot; id=&quot;markdown-toc-tabular-modelling&quot;&gt;Tabular modelling&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#categorical-embeddings&quot; id=&quot;markdown-toc-categorical-embeddings&quot;&gt;Categorical embeddings&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#beyond-deep-learning&quot; id=&quot;markdown-toc-beyond-deep-learning&quot;&gt;Beyond Deep Learning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-dataset&quot; id=&quot;markdown-toc-the-dataset&quot;&gt;The dataset&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#kaggle-competitions&quot; id=&quot;markdown-toc-kaggle-competitions&quot;&gt;kaggle Competitions&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#look-at-the-data&quot; id=&quot;markdown-toc-look-at-the-data&quot;&gt;Look at the data&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#decision-trees&quot; id=&quot;markdown-toc-decision-trees&quot;&gt;Decision trees&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#handling-dates&quot; id=&quot;markdown-toc-handling-dates&quot;&gt;Handling dates&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#using-tabularpandas-and-tabularproc&quot; id=&quot;markdown-toc-using-tabularpandas-and-tabularproc&quot;&gt;Using TabularPandas and TabularProc&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#creating-the-decision-tree&quot; id=&quot;markdown-toc-creating-the-decision-tree&quot;&gt;Creating the decision tree&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#categorical-variables&quot; id=&quot;markdown-toc-categorical-variables&quot;&gt;Categorical variables&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#random-forests&quot; id=&quot;markdown-toc-random-forests&quot;&gt;Random forests&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#creating-a-random-forest&quot; id=&quot;markdown-toc-creating-a-random-forest&quot;&gt;Creating a random forest&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#out-of-bag-error&quot; id=&quot;markdown-toc-out-of-bag-error&quot;&gt;Out-of-bag error&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#model-interpretation&quot; id=&quot;markdown-toc-model-interpretation&quot;&gt;Model interpretation&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#tree-variance-for-prediction-confidence&quot; id=&quot;markdown-toc-tree-variance-for-prediction-confidence&quot;&gt;Tree variance for prediction confidence&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#feature-importance&quot; id=&quot;markdown-toc-feature-importance&quot;&gt;Feature importance&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#removing-low-importance-variables&quot; id=&quot;markdown-toc-removing-low-importance-variables&quot;&gt;Removing low-importance variables&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#removing-redundant-features&quot; id=&quot;markdown-toc-removing-redundant-features&quot;&gt;Removing redundant features&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#partical-dependence&quot; id=&quot;markdown-toc-partical-dependence&quot;&gt;Partical dependence&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#data-leakage&quot; id=&quot;markdown-toc-data-leakage&quot;&gt;Data leakage&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#tree-interpreter&quot; id=&quot;markdown-toc-tree-interpreter&quot;&gt;Tree interpreter&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#extrapolation-and-neural-networks&quot; id=&quot;markdown-toc-extrapolation-and-neural-networks&quot;&gt;Extrapolation and neural networks&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#finding-out-of-domain-data&quot; id=&quot;markdown-toc-finding-out-of-domain-data&quot;&gt;Finding out of domain data&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#using-a-neural-network&quot; id=&quot;markdown-toc-using-a-neural-network&quot;&gt;Using a neural network&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#sidebar-fastais-tabular-classes&quot; id=&quot;markdown-toc-sidebar-fastais-tabular-classes&quot;&gt;Sidebar: fastai’s Tabular classes&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#end-sidebar-1&quot; id=&quot;markdown-toc-end-sidebar-1&quot;&gt;End sidebar&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#ensembling&quot; id=&quot;markdown-toc-ensembling&quot;&gt;Ensembling&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#boosting&quot; id=&quot;markdown-toc-boosting&quot;&gt;Boosting&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#combining-embeddings-with-other-methods&quot; id=&quot;markdown-toc-combining-embeddings-with-other-methods&quot;&gt;Combining embeddings with other methods&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#conclusion-our-advice-for-tabular-modeling&quot; id=&quot;markdown-toc-conclusion-our-advice-for-tabular-modeling&quot;&gt;Conclusion: our advice for tabular modeling&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#questionnaire-1&quot; id=&quot;markdown-toc-questionnaire-1&quot;&gt;Questionnaire&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;collaborative-filtering&quot;&gt;Collaborative Filtering&lt;/h2&gt;

&lt;h3 id=&quot;collaborative-filtering-from-scratch&quot;&gt;Collaborative filtering from scratch&lt;/h3&gt;

&lt;p&gt;Last time issue: We are overfitting with just small batch&amp;lt;/br&amp;gt;
Capacity of model, normally reducing the parameter of model ends up very shallow model&lt;/p&gt;

&lt;h4 id=&quot;weight-decay&quot;&gt;Weight decay&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a= 50&lt;/code&gt; means just small change will give you big results.&lt;/p&gt;

&lt;p&gt;(#f)&lt;/p&gt;

&lt;p&gt;weight decay means adding some valud to the gradients, so that it makes more shallow and less bumpy loss.&lt;/p&gt;

&lt;p&gt;Be cautious, when you do statistical model, they lessen the parameter to evade overfitting but modern machine learning doesn’t do that, and add regularization&lt;/p&gt;

&lt;h4 id=&quot;creating-our-own-embedding-module&quot;&gt;Creating our own Embedding module&lt;/h4&gt;

&lt;p&gt;Embedding is just indexing into an array&lt;/p&gt;

&lt;p&gt;Normally layer is made from inhertancing of module&lt;/p&gt;

&lt;p&gt;when you call torch and make tensor, they doesn’t have parameter, and you should define it using &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.parameter&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DotProductBias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_factors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_factors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_factors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movie_factors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_factors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movie_bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_range&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_factors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movie_factors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movie_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmoid_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;pytorch has autograd so that we don’t have to calculate gradient manually&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What’s our advantage making our own embedding instead of using pytorch one
J: no advantage just implementing by yourself and easily learn concept.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;interpreting-embeddings-and-biases&quot;&gt;Interpreting embeddings and biases&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;movie_bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movie_bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;idxs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;movie_bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argsort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idxs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;meaning of this, not just &lt;em&gt;(#f)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;we can use pca, and filter factors to 3, so we can select some kind of latent factors. &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h4 id=&quot;using-fastaicollab&quot;&gt;Using fastai.collab&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collab_learner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_factors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_one_cycle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;you can get similar results using fastai&lt;/p&gt;

&lt;h4 id=&quot;embedding-distance&quot;&gt;Embedding distance&lt;/h4&gt;

&lt;p&gt;Distance between two movies.&lt;/p&gt;

&lt;p&gt;Let’s pick some movie and find distance, form one movie to every other movies.&lt;/p&gt;

&lt;p&gt;(#f)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(#f) J: visualization part is just what’s going in and..? sound quality is not good&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;boot-strapping-a-collaborative-filtering&quot;&gt;Boot strapping a collaborative filtering&lt;/h3&gt;

&lt;h3 id=&quot;deep-learning-for-collaborative-filtering&quot;&gt;Deep Learning for collaborative filtering&lt;/h3&gt;

&lt;p&gt;the other way to do collab, we can concatenate the embeddings&lt;/p&gt;

&lt;p&gt;(#q) can’t understand&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_emb_sz&lt;/code&gt; method, fast.ai will give you the layer’s tensor size&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collab_learner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;use_nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_one_cycle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;if you use params &lt;code class=&quot;highlighter-rouge&quot;&gt;use_nn=True&lt;/code&gt;, you can use concat version of collab&lt;/p&gt;

&lt;h4 id=&quot;sidebar-kwargs-and-delegates&quot;&gt;Sidebar: kwargs and delegates&lt;/h4&gt;

&lt;h4 id=&quot;end-sidebar&quot;&gt;End sidebar&lt;/h4&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;h3 id=&quot;questionnaire&quot;&gt;Questionnaire&lt;/h3&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;tabular-modelling&quot;&gt;Tabular modelling&lt;/h2&gt;

&lt;h3 id=&quot;categorical-embeddings&quot;&gt;Categorical embeddings&lt;/h3&gt;

&lt;p&gt;It’s been a not much time that tabular data is used for deep learning model.&lt;/p&gt;

&lt;p&gt;Ex) German regions in embedding, if we draw embedding of the german city, it resembles actual german map&lt;/p&gt;

&lt;p&gt;many kinds of information of the world can be represented using embedding&lt;/p&gt;

&lt;p&gt;This is how google’s recommendation system works.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/google_collab.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;They use collab, which is concatenation of embeddingsgoogoo&lt;/p&gt;

&lt;h3 id=&quot;beyond-deep-learning&quot;&gt;Beyond Deep Learning&lt;/h3&gt;

&lt;p&gt;sometimes the other model, which is not deep learning models(classical model) do better than modern deep learning model&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ensembles of decision trees (i.e. Random Forests and Gradient Boosting Machines), mainly for structured data (such as you might find in a database table at most companies)&lt;/li&gt;
  &lt;li&gt;Multi-layered neural networks learnt with SGD (i.e. shallow and/or deep learning), mainly for unstructured data (such as audio, vision, and natural language)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;try 1 first, and after that try 2.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;How the decision tree ensenble works?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;we usually scikit-learn library.&lt;/p&gt;

&lt;p&gt;Refer &lt;a href=&quot;https://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1491957662/ref=asap_bc?ie=UTF8&quot;&gt;this book&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;when you use y_range maximum as 5.5? J: because it uses sigmoid, which can’t reach to the max, min value &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Do you recommend real-time service using decision-tree? for my own experience it was too slow.
J:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;the-dataset&quot;&gt;The dataset&lt;/h3&gt;

&lt;h4 id=&quot;kaggle-competitions&quot;&gt;kaggle Competitions&lt;/h4&gt;

&lt;h4 id=&quot;look-at-the-data&quot;&gt;Look at the data&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;At this point, a good next step is to handle ordinal columns&lt;/code&gt; and in this case, it is size of the product. we can make it as a categorical variables&lt;/p&gt;

&lt;p&gt;Jeremy read &lt;a href=&quot;https://render.githubusercontent.com/view/ipynb?commit=5b70a64d66d0880977881cc589cebe38812550f8&amp;amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6661737461692f66617374626f6f6b2f356237306136346436366430383830393737383831636335383963656265333838313235353066382f30395f746162756c61722e6970796e62&amp;amp;nwo=fastai%2Ffastbook&amp;amp;path=09_tabular.ipynb&amp;amp;repository_id=243838973&amp;amp;repository_type=Repository#Decision-trees&quot;&gt;this part&lt;/a&gt;, this it when I do review&lt;/p&gt;

&lt;h3 id=&quot;decision-trees&quot;&gt;Decision trees&lt;/h3&gt;

&lt;p&gt;It doesn’t require you special coding skill&lt;/p&gt;

&lt;p&gt;This is some good example of feature engineering&lt;/p&gt;

&lt;h4 id=&quot;handling-dates&quot;&gt;Handling dates&lt;/h4&gt;

&lt;h4 id=&quot;using-tabularpandas-and-tabularproc&quot;&gt;Using TabularPandas and TabularProc&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TabularProc&lt;/code&gt; - Tabular process, which is in-place function.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Categorify&lt;/code&gt; - make column with a numeric value, which is same with we saw at vocab.&lt;/p&gt;

&lt;p&gt;And we should think about valiation, and at this case, making test set as random is not enough. because this is basically related to time.&lt;/p&gt;

&lt;p&gt;See how test set is divided, and we can set the validation set as future time data.&lt;/p&gt;

&lt;h4 id=&quot;creating-the-decision-tree&quot;&gt;Creating the decision tree&lt;/h4&gt;

&lt;p&gt;DecisionTree Regressor, something when we want to predict continuous variable.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Data augmentation for tabular data, do you have idea? J: Not sure, but will think about data semantics
unordered, and ordered category, does fast.ai distinguish between these? J: yes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;you can use fastai function when drawing decision tree.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;draw_tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;leaves_parallel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;look at diagram from top to bottom. it’s the best way.&lt;/p&gt;

&lt;p&gt;basic way to do regression is predict the average.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;leaf-node&lt;/strong&gt;: not further trees when it is &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;if you want to train further, we can choice leaf-note number bigger&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;m_rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;means we made perfect model, but when we check validation model, it’s bigger than 0
	- one reason is related to leaf-node&lt;/p&gt;

&lt;p&gt;So we made our leaf-note to 25, and our result&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;do random tree (#f) ??&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;categorical-variables&quot;&gt;Categorical variables&lt;/h4&gt;

&lt;p&gt;related to categorical variables, we don’t have to do one-hot encode like using neural net! (we can make it, but there is no evidence it’s better)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What can I do with categorical missing value? J: (#f)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;random-forests&quot;&gt;Random forests&lt;/h3&gt;

&lt;p&gt;(#q could not understand much)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/bagging.pdf&quot;&gt;bagging predictors&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;: “Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions… The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests… show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;random subset&lt;/code&gt;, when you average the error, it is zero???&lt;/p&gt;

&lt;p&gt;Also randomly choose the subset of columns&lt;/p&gt;

&lt;h4 id=&quot;creating-a-random-forest&quot;&gt;Creating a random forest&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200_000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_samples_leaf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;min_samples_leaf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_samples_leaf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oob_score&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;in general, when you increase the &lt;code class=&quot;highlighter-rouge&quot;&gt;n_estimators&lt;/code&gt;, the accuracy enhances more&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/error.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we can set up our own methods, to see what’s going on. you just picked first element of the result, and show them.&lt;/p&gt;

&lt;p&gt;but our evaluation get low. why?&lt;/p&gt;

&lt;h4 id=&quot;out-of-bag-error&quot;&gt;Out-of-bag error&lt;/h4&gt;

&lt;p&gt;it’s not about time set, we don’t need validation set to do it.&lt;/p&gt;

&lt;h3 id=&quot;model-interpretation&quot;&gt;Model interpretation&lt;/h3&gt;

&lt;h4 id=&quot;tree-variance-for-prediction-confidence&quot;&gt;Tree variance for prediction confidence&lt;/h4&gt;

&lt;p&gt;How confident are we in our predictions using a particular row of data?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;look at the standard deviation of the tree, how much did the tree were varied, and if we were very varied, it means we didn’t confident that much&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;feature-importance&quot;&gt;Feature importance&lt;/h4&gt;

&lt;p&gt;For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;m.feature_importance&lt;/code&gt; attribute&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;removing-low-importance-variables&quot;&gt;Removing low-importance variables&lt;/h4&gt;

&lt;p&gt;Which columns are the strongest predictors, which can we ignore?&lt;/p&gt;

&lt;p&gt;remove the ignorable tree, and re train, accuracy is about to same, but the number of feature decreased.
after&lt;/p&gt;

&lt;p&gt;before we had this much features&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/before.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It decreased!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/after.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;removing-redundant-features&quot;&gt;Removing redundant features&lt;/h4&gt;

&lt;p&gt;Which columns are effectively redundant with each other, for purposes of prediction?
How do predictions vary, as we vary these columns?&lt;/p&gt;

&lt;p&gt;in this way we can get simpler and simpler model, remaining the accuracy&lt;/p&gt;

&lt;h4 id=&quot;partical-dependence&quot;&gt;Partical dependence&lt;/h4&gt;

&lt;p&gt;best way is to draw histogram.&lt;/p&gt;

&lt;p&gt;we can draw depends on each label, and see parti&lt;/p&gt;

&lt;p&gt;what is said, how &lt;code class=&quot;highlighter-rouge&quot;&gt;YearMade&lt;/code&gt; impact sales, if all other valud is equal?&lt;/p&gt;

&lt;p&gt;What might be impact just mitigate specific variable?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/partial.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;is cluster doing something related to hierarchical J: yes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;How to the feature importance values related to correlation? J: if you used to linear regression (#f) / Larry D. Here is an answer from Fra Pochetti: If 2 features are highly correlated their relative feature importance would be reduced compared to keeping just one of the two. Here why. A random forest selects features randomly at each split (in general). If 2 variables are correlated, they more or less carry the same signal wrt the dependent variable. Hence you can expect a tree to split on either of the 2 evenly. As an end result, your 2 features will have much less importance, just because they are carrying the same information. They hide each other. I generally remove correlated features even if it is not strictly needed, just to be able to uncover these kind of hidden relationships and spot truly important variables.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;data-leakage&quot;&gt;Data leakage&lt;/h4&gt;

&lt;h4 id=&quot;tree-interpreter&quot;&gt;Tree interpreter&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;treeinterpreter&lt;/code&gt; module&lt;/p&gt;

&lt;p&gt;So we take that one row of data, and put it through the first decision tree, looking to see what split is used at each point throughout the tree.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contributions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;extrapolation-and-neural-networks&quot;&gt;Extrapolation and neural networks&lt;/h3&gt;

&lt;p&gt;Jeremy usually index using special value ‘None’, not unsqeeze.&lt;/p&gt;

&lt;p&gt;random forest can’t extrapolate out side of train set, what it has seen, and this is big problem&lt;/p&gt;

&lt;h4 id=&quot;finding-out-of-domain-data&quot;&gt;Finding out of domain data&lt;/h4&gt;

&lt;p&gt;concatenating all of the independent variable,&lt;/p&gt;

&lt;p&gt;and ask is this data from training set or validation set?&lt;/p&gt;

&lt;p&gt;difference between valid data and train set,&lt;/p&gt;

&lt;p&gt;and extrapolation will not happen if it’s neural net.&lt;/p&gt;

&lt;h4 id=&quot;using-a-neural-network&quot;&gt;Using a neural network&lt;/h4&gt;

&lt;h4 id=&quot;sidebar-fastais-tabular-classes&quot;&gt;Sidebar: fastai’s Tabular classes&lt;/h4&gt;

&lt;p&gt;In fastai, a tabular model is simply a model which takes columns of continuous or categorical data, and predicts a category (a classification model) or a continuous value (a regression model). Categorical independent variables are passed through an embedding, and concatenated, as we saw in the neural net we used for collaborative filtering, and then continuous variables are concatenated as well.&lt;/p&gt;

&lt;h4 id=&quot;end-sidebar-1&quot;&gt;End sidebar&lt;/h4&gt;

&lt;h4 id=&quot;ensembling&quot;&gt;Ensembling&lt;/h4&gt;

&lt;p&gt;It’s simple we can just do average.&lt;/p&gt;

&lt;h4 id=&quot;boosting&quot;&gt;Boosting&lt;/h4&gt;

&lt;p&gt;another approach when we do ensemble,&lt;/p&gt;

&lt;p&gt;this is pretty sensitive to hyper-parameters&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;with tabular, dropping the feature is better than regularization? J: 
(#f) J:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;combining-embeddings-with-other-methods&quot;&gt;Combining embeddings with other methods&lt;/h4&gt;

&lt;p&gt;Entity embedding, which is &lt;code class=&quot;highlighter-rouge&quot;&gt;EE&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion-our-advice-for-tabular-modeling&quot;&gt;Conclusion: our advice for tabular modeling&lt;/h3&gt;

&lt;p&gt;slow regarding inference time, because they have to infer at every tree&lt;/p&gt;

&lt;h3 id=&quot;questionnaire-1&quot;&gt;Questionnaire&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;Jiwon, here is an answer from Jona R on the forum: Not certain, but I think that “0” was not actually a choice that users could rate. It seemed to me like it was used as a placeholder for “no rating” in some of Jeremy’s models.
That means that the lowest you would need to predict is “1”, which means that setting range 0 provides sufficient buffer.&lt;/p&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;wasn’t it 3d? how can we draw it using 2d drawing? &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;why it does not use minimum to minus? like [-0.5, 5.5] &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>dionne</name></author><summary type="html">Note of lesson7 : Deep Learning Part 1 of Spring 2020 at USF</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/wd.png" /></entry><entry><title type="html">Part1 lesson7 | fastai 2019 course-v3</title><link href="http://localhost:4000/2020/04/v3-2019-lesson07-note/" rel="alternate" type="text/html" title="Part1 lesson7 | fastai 2019 course-v3" /><published>2020-04-29T00:00:00+09:00</published><updated>2020-04-29T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/v3-2019-lesson07-note</id><content type="html" xml:base="http://localhost:4000/2020/04/v3-2019-lesson07-note/">&lt;p&gt;Note of lesson7 : Deep Learning Part 1 of Spring 2019 at USF, version 3 course&lt;/p&gt;

&lt;p&gt;Today we will study lots of things so don’t be frustrated.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ResNets&lt;/li&gt;
  &lt;li&gt;Unets&lt;/li&gt;
  &lt;li&gt;GANs&lt;/li&gt;
  &lt;li&gt;RNNs&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;CONTENTS&lt;/h2&gt;

&lt;p&gt;@reshama at fastai forum, who deployed app using fastai both of ios and android version.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#lesson7-resnet-mnistipynb&quot; id=&quot;markdown-toc-lesson7-resnet-mnistipynb&quot;&gt;lesson7-resnet-mnist.ipynb&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-cnn-with-batchnorm&quot; id=&quot;markdown-toc-basic-cnn-with-batchnorm&quot;&gt;Basic CNN with batchnorm&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#refactor&quot; id=&quot;markdown-toc-refactor&quot;&gt;Refactor&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#resnet-ish&quot; id=&quot;markdown-toc-resnet-ish&quot;&gt;Resnet-ish&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lesson3-camvidipynb&quot; id=&quot;markdown-toc-lesson3-camvidipynb&quot;&gt;lesson3-camvid.ipynb&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lesson7-suprres-gan&quot; id=&quot;markdown-toc-lesson7-suprres-gan&quot;&gt;lesson7-suprres-gan&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#crappified-data&quot; id=&quot;markdown-toc-crappified-data&quot;&gt;Crappified data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gans&quot; id=&quot;markdown-toc-gans&quot;&gt;GANs&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#save-generated-images&quot; id=&quot;markdown-toc-save-generated-images&quot;&gt;Save generated images&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lesson7-wgan&quot; id=&quot;markdown-toc-lesson7-wgan&quot;&gt;lesson7-wgan&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lesson7-superres&quot; id=&quot;markdown-toc-lesson7-superres&quot;&gt;lesson7-superres&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#rnns&quot; id=&quot;markdown-toc-rnns&quot;&gt;RNNs&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#maintain-state&quot; id=&quot;markdown-toc-maintain-state&quot;&gt;maintain state&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nnrnn&quot; id=&quot;markdown-toc-nnrnn&quot;&gt;nn.RNN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-layer-gru&quot; id=&quot;markdown-toc-2-layer-gru&quot;&gt;2-layer GRU&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(#f) what do learn&lt;/p&gt;

&lt;h2 id=&quot;lesson7-resnet-mnistipynb&quot;&gt;&lt;a href=&quot;https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-resnet-mnist.ipynb&quot;&gt;lesson7-resnet-mnist.ipynb&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;data block API&lt;/p&gt;

&lt;p&gt;pillow’s convert mode &lt;code class=&quot;highlighter-rouge&quot;&gt;L&lt;/code&gt;, which is grey scale&lt;/p&gt;

&lt;p&gt;items attribute is kind of the thing we get from data&lt;/p&gt;

&lt;p&gt;binary color map, refer to matplotlib for more info&lt;/p&gt;

&lt;p&gt;remember that pytorch put channel first&lt;/p&gt;

&lt;p&gt;you have to say how to split (when using fastai lib)&lt;/p&gt;

&lt;p&gt;validation is (of course) has labels&lt;/p&gt;

&lt;p&gt;1) have itemlist 2) split it 3) label it&lt;/p&gt;

&lt;p&gt;y are category object for now mnist&lt;/p&gt;

&lt;p&gt;transforms&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;kind of digit, you don’t want to rotate or flip since it will change the digit’s meaning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So Jeremy added random crop and added padding&lt;/p&gt;

&lt;p&gt;[]: empty is transforms for validation set, no need for transform for validation set.&lt;/p&gt;

&lt;p&gt;we don’t use pre-trained model, so we don’t need image net stats. (#f)&lt;/p&gt;

&lt;p&gt;plot_multi can show you results of calling some function, which grab the dataset, how many transformed version we create?: infinite, each time we call it, we grab the new transformed dataset.&lt;/p&gt;

&lt;p&gt;(batch_size, channel, row, col)&lt;/p&gt;

&lt;h3 id=&quot;basic-cnn-with-batchnorm&quot;&gt;Basic CNN with batchnorm&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ni&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;how many channels you want? -&amp;gt; as much as you want&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(#f)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(1,8) -&amp;gt; one channel coming in,  &lt;code class=&quot;highlighter-rouge&quot;&gt;8&lt;/code&gt;: how many filters you want to come out&lt;/p&gt;

&lt;p&gt;conv(ni, nf) &amp;lt;- ni, nf is input / output channel&lt;/p&gt;

&lt;p&gt;so as a result, 10 by 1 by 1 (which means grid size is 1)&lt;/p&gt;

&lt;p&gt;careful of the comment, which represents grid size&lt;/p&gt;

&lt;p&gt;it decreases by half since &lt;code class=&quot;highlighter-rouge&quot;&gt;stride = 2&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;refactor&quot;&gt;Refactor&lt;/h3&gt;

&lt;p&gt;exactly the same neural net.&lt;/p&gt;

&lt;p&gt;how can we improve this?&lt;/p&gt;

&lt;h3 id=&quot;resnet-ish&quot;&gt;Resnet-ish&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt; - skip connection, identity, res block&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Motivation of the paper: 56 layer is way more worse than the 20-layer(shallow) network.&lt;/li&gt;
  &lt;li&gt;Idea: keep the 56 layers but make results identical with shallow version&lt;/li&gt;
  &lt;li&gt;How: &lt;code class=&quot;highlighter-rouge&quot;&gt;a building block&lt;/code&gt;, every two convolutions, add the results together with input&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;and this became legendary architecture&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1712.09913&quot;&gt;Visualising the Loss Landscape of Neural Nets&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;x, y - weight space, y - loss&lt;/li&gt;
  &lt;li&gt;without identity connection, it’s very bumpy and with res connection, and this is little bit similar with batch norm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;except the last one, jeremy added res block without last layer.&lt;/p&gt;

&lt;p&gt;don’t forget to refactor your code&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;.9954&lt;/code&gt;, and NIPS 2015 the best paper was 0.45% error - sota changes so fast&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MergeLayer&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;SequentialEx : Sequential extended (#f)&lt;/p&gt;

&lt;p&gt;and real code, we don’t add, but &lt;code class=&quot;highlighter-rouge&quot;&gt;concat&lt;/code&gt;.  and this is &lt;strong&gt;dense net&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;memory intensive because how deep you go in, you have original data, but very little parameter (#q) (#f)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;really work well for segmentation&lt;/p&gt;

&lt;p&gt;(#f) re-building the modern architecture, and he keep trying to show …..(#f)&lt;/p&gt;

&lt;h2 id=&quot;lesson3-camvidipynb&quot;&gt;&lt;a href=&quot;https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson3-camvid-tiramisu.ipynb&quot;&gt;lesson3-camvid.ipynb&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;in order to color cluster, it has to know what that it is&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.07285&quot;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt; - great picture show what does 3 by 3 half stride conv looks like.&lt;/p&gt;

&lt;p&gt;U-net, down sampling path. size keep decreasing, channels keep increasing. And at last the grid size came back to same.&lt;/p&gt;

&lt;p&gt;Then, how do we double the grid size?&lt;/p&gt;

&lt;p&gt;stride - half convolution -&amp;gt; transposed convolution, deconvolution,&lt;/p&gt;

&lt;p&gt;and this is maybe 1, 2 years ago.
why?&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;nearly all of the pixel is zero&lt;/li&gt;
  &lt;li&gt;different amount of information to the different convolution path (see the paper, and understand myself)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; (1) Nearest neighbour interpolation&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;just copy twice, no zero, no computation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(2) bilinear interpolation - weighted average (this is pretty standard with picture)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;&quot; alt=&quot;/assets/images/lesson03-1.png&quot; /&gt; - kind of skip connection but not added to the every 2 conv block, but to the same stage of the downsampling to the same stage of the upsampling&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;see the code how fastai implemented this&lt;/p&gt;

&lt;p&gt;encoder refers to the downsampling part, which is substituted with resnet&lt;/p&gt;

&lt;p&gt;unet_block is connection between the up-sampling and down-sampling&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class UnetBlock
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;they grab the hook, to use resnet, and &lt;code class=&quot;highlighter-rouge&quot;&gt;up_out&lt;/code&gt; using up-sampling&lt;/p&gt;

&lt;p&gt;each time you see two convolutions like &lt;code class=&quot;highlighter-rouge&quot;&gt;conv2(conv1())&lt;/code&gt;, you can think of what if I add the res block? probably you would get a better results.&lt;/p&gt;

&lt;p&gt;identity connection is done with 1) add 2) concat&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q. why call concat before conv1, conv2 and not after? (#q - see the code) /  J: there’s no way to interact with channel (???) remember if you do 2 conv, it’s actually 3d, (#f)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q. size of image feature layer changes, how does dense net work since it concatenate. / J: ?? (#f) 53:00:00&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;lesson7-suprres-gan&quot;&gt;&lt;a href=&quot;https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb&quot;&gt;lesson7-suprres-gan&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;will use to another task, &lt;em&gt;image restoration&lt;/em&gt;, which starts from the image but will make &lt;em&gt;better&lt;/em&gt; image, like black-white to color, resolution, .. (#f)&lt;/p&gt;

&lt;h3 id=&quot;crappified-data&quot;&gt;Crappified data&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;paraLLeL(crappify, il.items)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;fastai’s function which does crappification(it takes time quite much)&lt;/p&gt;

&lt;p&gt;we gonna use, pre-trained model, since we added some noise and we have to catch what original picture was., which is watermark removal&lt;/p&gt;

&lt;p&gt;use MESLoss, and it normally expects two vectors, which can compare so we flat version.&lt;/p&gt;

&lt;p&gt;pre-trained part of unet is down-sampling part.&lt;/p&gt;

&lt;h3 id=&quot;gans&quot;&gt;GANs&lt;/h3&gt;

&lt;p&gt;Motivation of GAN: Our loss function doesn’t represent what we want.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;we are missing the texture of pillow, blanket&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-&amp;gt; the emergent of GAN.
it uses another models(&lt;strong&gt;Critic, discriminator&lt;/strong&gt;) for loss function&lt;/p&gt;

&lt;p&gt;t = 3761 » Jeremy explaination of GAN&lt;/p&gt;

&lt;p&gt;cons: if you don’t have pre-trained Critic/Generator, it’s kind of blind vs blind&lt;/p&gt;

&lt;h3 id=&quot;save-generated-images&quot;&gt;Save generated images&lt;/h3&gt;

&lt;p&gt;let’s create Critics&lt;/p&gt;

&lt;p&gt;(#q) reconstruct = True&lt;/p&gt;

&lt;p&gt;we will learn more of what’s going on inside at part2&lt;/p&gt;

&lt;p&gt;if you don’t want to re-initiate the gpu ram, you can gc.collect() which will remove caching gpu.&lt;/p&gt;

&lt;p&gt;we will use Binary Cross Entropy but will not use resnot&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Because of weight could be increased much that we can’t handle that, but will learn at part2&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;we should be careful when using gan, loss which uses adaptive loss&lt;/p&gt;

&lt;p&gt;we not only use the critic to loss function, but also MSE loss because if we use only the Critics as loss function, generator could make irrelevant photo of the original image.&lt;/p&gt;

&lt;p&gt;GANs hates momentum. - but nobody figured it out.&lt;/p&gt;

&lt;p&gt;tough things of training gan - loss number is meaningless. / so you have to see the results from time to time&lt;/p&gt;

&lt;p&gt;(original craffied picture / generated picture / original picture)&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q. What kind of task you will not use U-net? / J: Unet is used when size of output is aligned to the input. so any kind of generative modelling and segmentation is also generative modelling because we should out pixel segments which is responding to original pixel. and it would be absurd if we use unet to the classification, because we just need the down-sampling process.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;lesson7-wgan&quot;&gt;&lt;a href=&quot;https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-wgan.ipynb&quot;&gt;lesson7-wgan&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;(skipped so fast…)&lt;/p&gt;

&lt;p&gt;interesting because we use bedrooms dataset&lt;/p&gt;

&lt;p&gt;the approach that we use is ‘can we create a bedroom?’&lt;/p&gt;

&lt;p&gt;we feed the generator &lt;strong&gt;random noise&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;and generator should return the de-noised bedroom.&lt;/p&gt;

&lt;h2 id=&quot;lesson7-superres&quot;&gt;&lt;a href=&quot;https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb&quot;&gt;lesson7-superres&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.08155&quot;&gt;Perceptual Losses for Real-Time Style Transfer and Super-Resolution&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;doesn’t like the term ‘perceptual’ - jeremy prefers to say it is feature losses&lt;/p&gt;

&lt;p&gt;down-sampling part is called encoder, and vice versa&lt;/p&gt;

&lt;p&gt;same color represents same grid size&lt;/p&gt;

&lt;p&gt;(#f) explained idea but could not get. - &lt;em&gt;t=5022&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;pick the l1 loss or mse ( no matter)&lt;/p&gt;

&lt;p&gt;grad is false since we will use only the loss&lt;/p&gt;

&lt;p&gt;class featureloss is the implementation of the paper&lt;/p&gt;

&lt;p&gt;(abandoned to write -&amp;gt; too hard…. and too fast)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;upsize medium res data, and you can see the pixels enlarged focus.&lt;/p&gt;

&lt;p&gt;results is quite good&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jantic/DeOldify&quot;&gt;DeOldify&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the student of fast.ai, made colored picture who had the fast.ai course.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(#f) and also said lots of idea related to this. we can new way of Crappification.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q. how about using unet or gan to nlp, kind of generating shakespear’s novel. / J: possible, and it’s quite blue ocean, and will look thorough little bit at part2&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Be sure not to understand at once, go through this at least 3 times or more&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;rnns&quot;&gt;&lt;a href=&quot;https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-human-numbers.ipynb&quot;&gt;RNNs&lt;/a&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;square - input / arrow - layer / circle - activation / triangle - output&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;go to the learn.summary and see the specifics&lt;/p&gt;

&lt;p&gt;we will do mat mul and non-linearity one(or more) more, but (??) will use same weight.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;bptt&lt;/code&gt; - back prop through time&lt;/p&gt;

&lt;p&gt;13017 / 70 / bs - all dataset divide first  by batch size, and after that, divide by length limit (but why??????)&lt;/p&gt;

&lt;p&gt;every mini-batch joins up with previous mini-batch (that’s why he showed textify)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;how we convert the diagram to the code&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;rnn is just a refactoring, there’s nothing new.&lt;/p&gt;

&lt;p&gt;-&amp;gt; but only just predicting last word is wasteful, so we will predict every word, which means just triangle comes inside the model.&lt;/p&gt;

&lt;h3 id=&quot;maintain-state&quot;&gt;maintain state&lt;/h3&gt;

&lt;p&gt;Let’s keep the hidden state.&lt;/p&gt;

&lt;p&gt;(rnn is totally normal nn but just refactored)&lt;/p&gt;

&lt;h3 id=&quot;nnrnn&quot;&gt;nn.RNN&lt;/h3&gt;

&lt;p&gt;this time we will render the output the other loop, which is &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.RNN&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;bptt 20 means 20 layers of the diagram.&lt;/p&gt;

&lt;h3 id=&quot;2-layer-gru&quot;&gt;2-layer GRU&lt;/h3&gt;

&lt;p&gt;there are small nn which decide how much apply the nodes.&lt;/p&gt;

&lt;p&gt;GRU/lstm is depended on specifics&lt;/p&gt;</content><author><name>dionne</name></author><summary type="html">Note of lesson7 : Deep Learning Part 1 of Spring 2019 at USF, version 3 course</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/v3-l7densenet.png" /></entry><entry><title type="html">Part1 lesson6 note | fastai 2020 course-v4</title><link href="http://localhost:4000/2020/04/v4-2020-lesson06/" rel="alternate" type="text/html" title="Part1 lesson6 note | fastai 2020 course-v4" /><published>2020-04-23T00:00:00+09:00</published><updated>2020-04-23T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/v4-2020-lesson06</id><content type="html" xml:base="http://localhost:4000/2020/04/v4-2020-lesson06/">&lt;p&gt;Note of lesson6 : &lt;a href=&quot;https://www.usfca.edu/data-institute/certificates/deep-learning-part-one&quot;&gt;Deep Learning Part 1 of Spring 2020 at USF&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Today we will study&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Image Classification&lt;/li&gt;
  &lt;li&gt;Multi-label classification&lt;/li&gt;
  &lt;li&gt;Collaborative filtering&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;CONTENTS&lt;/h2&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#image-classification&quot; id=&quot;markdown-toc-image-classification&quot;&gt;Image Classification&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#model-interpretation-course-link-book-link&quot; id=&quot;markdown-toc-model-interpretation-course-link-book-link&quot;&gt;Model Interpretation: Course link, Book link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#improving-our-model&quot; id=&quot;markdown-toc-improving-our-model&quot;&gt;Improving our model&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#learning-rate-finder&quot; id=&quot;markdown-toc-learning-rate-finder&quot;&gt;Learning rate finder&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#unfreezing-and-transfer-learning&quot; id=&quot;markdown-toc-unfreezing-and-transfer-learning&quot;&gt;Unfreezing and transfer learning&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#discriminative-learning-rates&quot; id=&quot;markdown-toc-discriminative-learning-rates&quot;&gt;Discriminative learning rates&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#selecting-the-number-of-epochs&quot; id=&quot;markdown-toc-selecting-the-number-of-epochs&quot;&gt;Selecting the number of epochs&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#deeper-architectures&quot; id=&quot;markdown-toc-deeper-architectures&quot;&gt;Deeper architectures&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#assignment&quot; id=&quot;markdown-toc-assignment&quot;&gt;Assignment&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#multi-label-classification&quot; id=&quot;markdown-toc-multi-label-classification&quot;&gt;Multi-label classification&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#constructing-a-data-block&quot; id=&quot;markdown-toc-constructing-a-data-block&quot;&gt;Constructing a data block&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#binary-cross-entropy&quot; id=&quot;markdown-toc-binary-cross-entropy&quot;&gt;Binary cross entropy&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#regression&quot; id=&quot;markdown-toc-regression&quot;&gt;Regression&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#assemble-the-data&quot; id=&quot;markdown-toc-assemble-the-data&quot;&gt;Assemble the data&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#training-a-mode&quot; id=&quot;markdown-toc-training-a-mode&quot;&gt;Training a mode&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#assignment-1&quot; id=&quot;markdown-toc-assignment-1&quot;&gt;Assignment&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#collaborative-filtering&quot; id=&quot;markdown-toc-collaborative-filtering&quot;&gt;Collaborative filtering&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#learning-the-latent-factors&quot; id=&quot;markdown-toc-learning-the-latent-factors&quot;&gt;Learning the latent factors&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#creating-the-dataloaders&quot; id=&quot;markdown-toc-creating-the-dataloaders&quot;&gt;Creating the DataLoaders&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#collaborative-filtering-from-scratch&quot; id=&quot;markdown-toc-collaborative-filtering-from-scratch&quot;&gt;Collaborative filtering from scratch&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Previous; Recap of last class&lt;/p&gt;

&lt;h2 id=&quot;image-classification&quot;&gt;Image Classification&lt;/h2&gt;

&lt;h3 id=&quot;model-interpretation-course-link-book-link&quot;&gt;Model Interpretation: &lt;a href=&quot;https://render.githubusercontent.com/view/ipynb?commit=56ab576a6826ecea66ed555b3b46a90ed409bb19&amp;amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6661737461692f636f757273652d76342f353661623537366136383236656365613636656435353562336234366139306564343039626231392f6e62732f30355f7065745f6272656564732e6970796e62&amp;amp;nwo=fastai%2Fcourse-v4&amp;amp;path=nbs%2F05_pet_breeds.ipynb&amp;amp;repository_id=248051827&amp;amp;repository_type=Repository#Model-Interpretation&quot;&gt;Course link&lt;/a&gt;, &lt;a href=&quot;https://render.githubusercontent.com/view/ipynb?commit=56ab576a6826ecea66ed555b3b46a90ed409bb19&amp;amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6661737461692f636f757273652d76342f353661623537366136383236656365613636656435353562336234366139306564343039626231392f6e62732f30355f7065745f6272656564732e6970796e62&amp;amp;nwo=fastai%2Fcourse-v4&amp;amp;path=nbs%2F05_pet_breeds.ipynb&amp;amp;repository_id=248051827&amp;amp;repository_type=Repository#Model-Interpretation&quot;&gt;Book link&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;When you have many categories, use &lt;code class=&quot;highlighter-rouge&quot;&gt;most_confused()&lt;/code&gt; method rather than plotting confusion matrix &lt;sup id=&quot;fnref:q1&quot;&gt;&lt;a href=&quot;#fn:q1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;improving-our-model&quot;&gt;Improving our model&lt;/h3&gt;

&lt;h4 id=&quot;learning-rate-finder&quot;&gt;Learning rate finder&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;We can try to learn &lt;em&gt;fast&lt;/em&gt; which is done by epoch 1~2 and set learning rate big&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;learn.fine_tune()&lt;/code&gt; has &lt;code class=&quot;highlighter-rouge&quot;&gt;base_lr&lt;/code&gt; parameter&lt;/li&gt;
  &lt;li&gt;If learning rate is too high, loss validation gets too high compared to loss of train
    &lt;ul&gt;
      &lt;li&gt;why? because it diverges&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If learning rate is too small, train loss decreases too slowly, and there are much gap between train and valid loss. &lt;sup id=&quot;fnref:q2&quot;&gt;&lt;a href=&quot;#fn:q2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
    &lt;ul&gt;
      &lt;li&gt;So it will take long time, means too many epochs, resulting overfitting&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;meaning of &lt;code class=&quot;highlighter-rouge&quot;&gt;learning rate &lt;/code&gt;~ &lt;code class=&quot;highlighter-rouge&quot;&gt;loss&lt;/code&gt; graph
    &lt;ul&gt;
      &lt;li&gt;find optimal lr drawing loss depends on learning rate&lt;/li&gt;
      &lt;li&gt;careful it’s logarithm scale&lt;/li&gt;
      &lt;li&gt;this method was invented at 15 and before that people just experimented&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;lr_find()&lt;/code&gt; goes through one sing mini-batch?
J: no, it’s just working through data-loader, and diff is we try many lrs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;why steepest? why not minimum?
J: at the minimum we don’t learn anymore. we want our model to enhance learning while training &lt;br /&gt; R: About lr_find(), it’s natural that it seems too simple&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;unfreezing-and-transfer-learning&quot;&gt;Unfreezing and transfer learning&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;we throw away last layer, and re-train depends on our data/task&lt;/li&gt;
  &lt;li&gt;And the below function is easy way when we do just fine-tune&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;learn.find_tune??
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;calling &lt;code class=&quot;highlighter-rouge&quot;&gt;cnn_learner&lt;/code&gt; freeze our model, so don’t have to do it separately&lt;/li&gt;
  &lt;li&gt;But when you do unfreeze, which means you train all the params, you need to adjust the learning rate more.&lt;/li&gt;
  &lt;li&gt;We can do better because we just used the same learning rate for a whole training&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;discriminative-learning-rates&quot;&gt;Discriminative learning rates&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/discriminative_lr.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;use &lt;code class=&quot;highlighter-rouge&quot;&gt;slice&lt;/code&gt; at lr_max parameter means discriminative learning rate &lt;sup id=&quot;fnref:q4&quot;&gt;&lt;a href=&quot;#fn:q4&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;selecting-the-number-of-epochs&quot;&gt;Selecting the number of epochs&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If you choose too big epoch, valid_loss will not change from specific point, and this is related to &lt;code class=&quot;highlighter-rouge&quot;&gt;fit_one_cycle&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;What &lt;code class=&quot;highlighter-rouge&quot;&gt;fit_one_cycle&lt;/code&gt; does?
    &lt;ul&gt;
      &lt;li&gt;different from just &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;start purposely from low learning rate. and from the point of highest point, they decrease lr again&lt;/li&gt;
      &lt;li&gt;if error_rate stop from specific epoch, use that epoch as epoch and do it again&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Loss is just a thing we want to approximate, so always care &lt;code class=&quot;highlighter-rouge&quot;&gt;error&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;deeper-architectures&quot;&gt;Deeper architectures&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;It would be possible that you don’t want pre-trained but usually it’s helpful&lt;/li&gt;
  &lt;li&gt;usually out of memory happens
    &lt;ul&gt;
      &lt;li&gt;about, &lt;code class=&quot;highlighter-rouge&quot;&gt;.to_fp16()&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;half precision floating point&lt;/code&gt;, NVIDIA GPU support special tensor and it’s faster about 2x/3x&lt;/li&gt;
      &lt;li&gt;you can use this using module &lt;code class=&quot;highlighter-rouge&quot;&gt;from fastai2.callback.fp16 import *&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Try a small model before scaling up the model &amp;lt; because bigger model doesn’t guarantee better performance&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;assignment&quot;&gt;Assignment&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Don’t forget the questionnaire&lt;/li&gt;
  &lt;li&gt;Read paper, &lt;a href=&quot;https://arxiv.org/abs/1506.01186&quot;&gt;Cyclical Learning Rates for Training Neural Networks&lt;/a&gt; and see how you can get best results with adjusting learning rate, find out best learning rate by yourself!
    &lt;ul&gt;
      &lt;li&gt;of course you can see fast.ai learning rate source code&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;How do you know you can do better?
J: you don’t know, who knows. just try and experiment&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;about &lt;code class=&quot;highlighter-rouge&quot;&gt;.to_fp16()&lt;/code&gt; it doesn’t affect result?
J: less bumpy, little bit better but not that big deal&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;question about random
J: not does machine learning but  deep learning specifically. For more info, see Rachael’s Linear Algebra course.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;multi-label-classification&quot;&gt;Multi-label classification&lt;/h2&gt;

&lt;h3 id=&quot;constructing-a-data-block&quot;&gt;Constructing a data block&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;pandas, which is for dataframe
    &lt;ul&gt;
      &lt;li&gt;when you slicing, &lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt; is always optional&lt;/li&gt;
      &lt;li&gt;Read &lt;code class=&quot;highlighter-rouge&quot;&gt;Python for Data Analysis&lt;/code&gt; class written by person who made pandas&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;dataset is abstract idea of class
    &lt;ul&gt;
      &lt;li&gt;If you get &lt;em&gt;index&lt;/em&gt;, and &lt;em&gt;length&lt;/em&gt;, it’s dataset.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;string.ascii_lowercase&lt;/code&gt; is qualified dataset since it has length and index&lt;/li&gt;
      &lt;li&gt;If you index it mostly get tuple, independent variable and dependent variable.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Datasets is usually composed of not stack of data and label
    &lt;ul&gt;
      &lt;li&gt;fast.ai, we implemented use file name as independent variable, and through function we get dependent variable&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dataloader divides Datasets to batches, &lt;a href=&quot;https://render.githubusercontent.com/view/ipynb?commit=b7f3f0d750196e155de05b0788725a352faa7732&amp;amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6661737461692f66617374626f6f6b2f623766336630643735303139366531353564653035623037383837323561333532666161373733322f30365f6d756c74696361742e6970796e62&amp;amp;nwo=fastai%2Ffastbook&amp;amp;path=06_multicat.ipynb&amp;amp;repository_id=243838973&amp;amp;repository_type=Repository#Constructing-a-data-block&quot;&gt;read the book&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Datablock assumes basically we have 2 kind of variable
    &lt;ul&gt;
      &lt;li&gt;randomly select different validation set so we have different output whenever we call dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),
                   get_x = get_x, get_y = get_y)
dsets = dblock.datasets(df)
dsets.train[0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;
(PILImage mode=RGB size=500x375,
 TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;PILImage mode=RGB size=500x375&lt;/code&gt;: independent variable, which is a input
    &lt;ul&gt;
      &lt;li&gt;we did transform filename to tensors&lt;/li&gt;
      &lt;li&gt;output is one-hot encoding&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Shouldn’t it be an integer but why output(&lt;code class=&quot;highlighter-rouge&quot;&gt;TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))&lt;/code&gt;) is float?
J: because we &lt;em&gt;will&lt;/em&gt; use cross-entropy style loss, which does floating calculation&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;why we don’t use with floating point 8 bit?
J: could be fast, but low precision (but maybe possible)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;binary-cross-entropy&quot;&gt;Binary cross entropy&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cnn_learner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;activs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;activs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;you can try grapping the one batch of the data to the model, you can get an output.
    &lt;ul&gt;
      &lt;li&gt;Use a&lt;code class=&quot;highlighter-rouge&quot;&gt;dls.train.one_batch()&lt;/code&gt; function to ensure what’s going in and out&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;binary_cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;result is 0 to 1 but we get a variable between 0 and 1 so that we use sigmoid&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;same reason as we said at softmax, normally we use &lt;code class=&quot;highlighter-rouge&quot;&gt;log &lt;/code&gt; (fast and acurate)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;explained &lt;a href=&quot;https://render.githubusercontent.com/view/ipynb?commit=b7f3f0d750196e155de05b0788725a352faa7732&amp;amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6661737461692f66617374626f6f6b2f623766336630643735303139366531353564653035623037383837323561333532666161373733322f30365f6d756c74696361742e6970796e62&amp;amp;nwo=fastai%2Ffastbook&amp;amp;path=06_multicat.ipynb&amp;amp;repository_id=243838973&amp;amp;repository_type=Repository#Binary-cross-entropy&quot;&gt;the book&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;One-hot encoded target, use &lt;code class=&quot;highlighter-rouge&quot;&gt;BCELoss&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;accuracy as a metric only works for single label dataset
    &lt;ul&gt;
      &lt;li&gt;because accuracy does &lt;code class=&quot;highlighter-rouge&quot;&gt;argmax&lt;/code&gt; which chooses largest number&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;so doing multi-categorical problem, use metrics which can compare each activation with some threshold(not selecting one value), and pass if the number over threshold
    &lt;ul&gt;
      &lt;li&gt;but we might want to change threshold depends on the input, so that we use &lt;code class=&quot;highlighter-rouge&quot;&gt;partial function&lt;/code&gt; which sets default variable, and change if we want&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;the accuracy changes if I use different threshold
    &lt;ul&gt;
      &lt;li&gt;How can I get &lt;em&gt;best&lt;/em&gt; threshold?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;targs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;29&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_multi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thresh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;we can plot and select the best number and don’t have to care of overfitting (because it’s not that bumpy as rule of thumb)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;regression&quot;&gt;&lt;a href=&quot;https://render.githubusercontent.com/view/ipynb?commit=b7f3f0d750196e155de05b0788725a352faa7732&amp;amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6661737461692f66617374626f6f6b2f623766336630643735303139366531353564653035623037383837323561333532666161373733322f30365f6d756c74696361742e6970796e62&amp;amp;nwo=fastai%2Ffastbook&amp;amp;path=06_multicat.ipynb&amp;amp;repository_id=243838973&amp;amp;repository_type=Repository#Regression&quot;&gt;Regression&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;two types of task at supervised learning 1) classification(discrete variable) 2) regression(continuous variable)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;assemble-the-data&quot;&gt;Assemble the data&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_image_files&lt;/code&gt; gather image recursively&lt;/li&gt;
  &lt;li&gt;here, it important to setting evaluation to a specific person using&lt;code class=&quot;highlighter-rouge&quot;&gt;splitter&lt;/code&gt;, not randomly selection -&amp;gt; it’s continuous frame, so if you just select random data, it would be overestimated.&lt;/li&gt;
  &lt;li&gt;pytorch has tensor &lt;code class=&quot;highlighter-rouge&quot;&gt;batch&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;channel&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;height&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;width&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;regarding channel, image has 3 channels
    &lt;ul&gt;
      &lt;li&gt;you can see the version of &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;G&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;B&lt;/code&gt; using the below code&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image2tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image/grizzly.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Reds'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Greens'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Blues'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;show_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;can we give multi-channel bigger than 3?
J: yes, it used often like when you use satellite image. But your pre-trained model is usually for a 3-channel and fastai handles that case, but is is just problem of axis&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;training-a-mode&quot;&gt;Training a mode&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show_results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ds_idx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;left of output is target, right is prediction&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Notice&lt;/em&gt;  It’s interesting that basically our pre-trained model was trained for classification task. But it works well when you do regression
    &lt;ul&gt;
      &lt;li&gt;why? pre-trained model from ImageNet is collection of image’s features like, objects, color, texture, shadow and so on&lt;/li&gt;
      &lt;li&gt;so from pre-trained model, we can get the &lt;em&gt;features&lt;/em&gt; of an image, and do the &lt;em&gt;other job&lt;/em&gt;(e.g., regression) with fine-tuning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;assignment-1&quot;&gt;Assignment&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;go to the bear classifier, and find out when you give image which is not included in label and see what happens.  change model from single labelling to multi-labelling, and tell what happened at fastai forum&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;at traditional machine learning we used cross-validation, we don’t use it at deep learning?
J: First, nowadays cross-validations are less common(if not on kaggle, because praction of data is important), because cross-validation was used there were not lots of data. Second, cross-validation is not related selection between machine learning and deep learning&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;CF algorithm used other than recommendation system
J: whatever it’s kind of recommendation system, if concept is finding out the other candidates using the previous behaviour&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;how can i make data set if I have video? how split?(time phrase)
J: like full color movie, rank-5 tensor by (time, h, w, batch, channel). but usually it would be 4 tensor, because of size of tensor or key problem, but theoretically possible.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;collaborative-filtering&quot;&gt;&lt;a href=&quot;https://render.githubusercontent.com/view/ipynb?commit=b7f3f0d750196e155de05b0788725a352faa7732&amp;amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6661737461692f66617374626f6f6b2f623766336630643735303139366531353564653035623037383837323561333532666161373733322f30385f636f6c6c61622e6970796e62&amp;amp;nwo=fastai%2Ffastbook&amp;amp;path=08_collab.ipynb&amp;amp;repository_id=243838973&amp;amp;repository_type=Repository#Collaborative-filtering-deep-dive&quot;&gt;Collaborative filtering&lt;/a&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;empty part means user didn’t watch or rate the movie&lt;/li&gt;
  &lt;li&gt;upper and below part represent same info&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pic1-v4-L6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;learning-the-latent-factors&quot;&gt;Learning the latent factors&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;CF algorithm is related to &lt;em&gt;latent&lt;/em&gt;  variable(;factors) since we don’t have &lt;em&gt;a label explaining/depicting properties&lt;/em&gt; which made you choose specific movie
    &lt;ul&gt;
      &lt;li&gt;In other words, there is no specific label how we could predict the dependent variable(recommendation; movies user didn’t watch) results from independent var(rating regarding user and movie)&lt;/li&gt;
      &lt;li&gt;but we can assume it has labels like genres and make matrix representing it&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;randomly create user’s factor(;latent, hidden) and movie’s factor. and do matrix multiplication with those, which is dot product.&lt;/li&gt;
  &lt;li&gt;and sum it and compare with target, so that you can learn parameters, which were randomly initialized&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;creating-the-dataloaders&quot;&gt;Creating the DataLoaders&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Let’s make DataLoaders using basic python and PyTorch!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;user and movie is represented using index, label,..,called &lt;em&gt;look up in an index&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;but this is problem, because DL treats only kind of matrix multiplication.&lt;/li&gt;
      &lt;li&gt;so that we make one-hot encoding matrix&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;And this is quite amazing because it came out with any kind of discrete value, we can make it work using arrays&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;how is different when you treat actual data between dense and sparse data?
J: we will not treat sparse data, but there’s course Rachael’s linear algebra&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;embedding layer: multiplying by a one hot encoded matrix, using the computational shortcut that it can be implemented by simply indexing directly.
    &lt;ul&gt;
      &lt;li&gt;there is excel explaining detailed computation&lt;/li&gt;
      &lt;li&gt;This is important because usually people think embedding is something difficult, but it’s just computational shortcut to do matrix multiplication&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;collaborative-filtering-from-scratch&quot;&gt;Collaborative filtering from scratch&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;importance of dunder method of python. (like &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt;)
    &lt;ul&gt;
      &lt;li&gt;explained OOP concept&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Inheritance of python, which used DotProduct class
    &lt;ul&gt;
      &lt;li&gt;Module is fast.ai version of pytorch’s nn.Module&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt;function grab embedding, using specific index
    &lt;ul&gt;
      &lt;li&gt;(again) Remember Embedding is just kind of shortcut to make a one-hot encoding matrix&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DotProduct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_factors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_factors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_factors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movie_factors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_factors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_factors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movie_factors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;64: size of mini_batch, 2: index&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x,y = dls.one_batch()
x.shape
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.Size([64, 2])&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We use bias to represent specific character of &lt;em&gt;each user&lt;/em&gt; and &lt;em&gt;movie&lt;/em&gt;, because embedding of user/movie doesn’t show users’ character, who might generally doesn’t like movie&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;what should we do when my model is overfitting very quickly with small epoch&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;i.e., what should we do to evade overfitting and don’t want to lessen epoch?&lt;/li&gt;
      &lt;li&gt;augmentation can be one solution but we can’t do augmentation with this data&lt;/li&gt;
      &lt;li&gt;in this case, we do other regularization, which means panellize the fast learning &lt;sup id=&quot;fnref:q5&quot;&gt;&lt;a href=&quot;#fn:q5&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Collaborative filtering algorithm works better than svd or kind of that?
J: yes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;My Questions&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:q1&quot;&gt;
      &lt;p&gt;no way to see with regression model? &lt;a href=&quot;#fnref:q1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:q2&quot;&gt;
      &lt;p&gt;But how do I know if loss decreases slowly or not? &lt;a href=&quot;#fnref:q2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:q4&quot;&gt;
      &lt;p&gt;I need to study more of scheduled learning &lt;a href=&quot;#fnref:q4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:q5&quot;&gt;
      &lt;p&gt;Isn’t there any side effects when you did too much regularization? &lt;a href=&quot;#fnref:q5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>dionne</name></author><summary type="html">Note of lesson6 : Deep Learning Part 1 of Spring 2020 at USF</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/lesson6-v4.png" /></entry><entry><title type="html">Part1 lesson6 note | fastai 2019 course-v3</title><link href="http://localhost:4000/2020/04/v3-2019-lesson06-note/" rel="alternate" type="text/html" title="Part1 lesson6 note | fastai 2019 course-v3" /><published>2020-04-15T00:00:00+09:00</published><updated>2020-04-15T00:00:00+09:00</updated><id>http://localhost:4000/2020/04/v3-2019-lesson06-note</id><content type="html" xml:base="http://localhost:4000/2020/04/v3-2019-lesson06-note/">&lt;p&gt;&lt;a href=&quot;https://github.com/SpellOnYou/SpellOnYou.github.io/commit/b931d90bf906450cde28e8fac76da64a1896a63f#diff-8b2f5dce3d5eb87ba947f787ad22e66b&quot;&gt;[v1]&lt;/a&gt; Thu, 16 Apr 2020&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/SpellOnYou/SpellOnYou.github.io/commit/525869c6bec13d4f74f9fd32ae5624033aa2096f#diff-ee8397e253454371f290b8f33ce30bac&quot;&gt;[v2]&lt;/a&gt; Thu, 23 Apr 2020&lt;/p&gt;

&lt;h1 id=&quot;lesson-06&quot;&gt;Lesson 06&lt;/h1&gt;

&lt;p&gt;Will find official notes &lt;a href=&quot;https://github.com/fastai/course-v3/blob/master/files/dl-2019/notes/notes-1-6.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;rossmanntabular&quot;&gt;Rossmann(Tabular)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Tabular data: be careful on Categorical variable vs Continuous variable.&lt;/li&gt;
  &lt;li&gt;if datatype is int, fastai think it is classification, not a regression.&lt;/li&gt;
  &lt;li&gt;Root mean square percentage error. as loss function.&lt;/li&gt;
  &lt;li&gt;When you assign the y_range, it’s better to assign little bit more than actual maximum. &amp;gt; because it’s sigmoid.&lt;/li&gt;
  &lt;li&gt;Intermediate layers, which is weight matrix is 1) 1000, and 2) 500 -&amp;gt; which means our parameter would be 500*1000.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;what-is-dropout-and-embedding-dropout&quot;&gt;What is dropout and embedding dropout?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://jmlr.org/papers/v15/srivastava14a.html&quot;&gt;Nitish Srivastava, Dropout: A Simple way to prevent Neural Networks from Overfitting&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;you can dropout with &lt;code class=&quot;highlighter-rouge&quot;&gt;p&lt;/code&gt; value, make it specified to specific layer, or make it applied to all the layers.&lt;/li&gt;
  &lt;li&gt;Pytorch code 1) bernoulli, which decides whether you will hold it? 2) and divide the noise value depends on noise value. so noise became 2 or remain 0.
    &lt;ul&gt;
      &lt;li&gt;According to pytorch code, We do change at training time, but we do nothing at test time. and this means you don’t have to do anything special with inference time.’&lt;/li&gt;
      &lt;li&gt;&lt;b&gt;TODO&lt;/b&gt;: find at forums &lt;code class=&quot;highlighter-rouge&quot;&gt;what is inference time&lt;/code&gt; - Related to NVIDIA, GPU.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Embedding dropout is just a dropout.
    &lt;ul&gt;
      &lt;li&gt;It’s different between continuous variable and embedding layer.  &lt;b&gt;TODO&lt;/b&gt; Still can’t understand. why embedding dropout is effective. or,… in need.&lt;/li&gt;
      &lt;li&gt;Let’s delete at random, some of the results of the embedding.&lt;/li&gt;
      &lt;li&gt;and It worked well especially at Kaggle&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1502.03167.pdf&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; -&amp;gt; came out false! According to &lt;a href=&quot;https://arxiv.org/pdf/1805.11604.pdf&quot;&gt;How Does Batch Normalization Help Optimization?&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The key was  &lt;code class=&quot;highlighter-rouge&quot;&gt;multiplicative&lt;/code&gt; bias &lt;em&gt;gamma&lt;/em&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;additive&lt;/code&gt; bias &lt;em&gt;beta&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Explain
    &lt;ul&gt;
      &lt;li&gt;Let $$ \hat{y}  = f(w_1, w_2, w_3, … , x) $$ ,  loss = MSE , Then &lt;code class=&quot;highlighter-rouge&quot;&gt;y_range&lt;/code&gt; should be between 1 and 5&lt;/li&gt;
      &lt;li&gt;And Activation function ends with &lt;code class=&quot;highlighter-rouge&quot;&gt;-1 -&amp;gt; +1&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;To mitigate this problem, we can add another parameter, like $$w_n$$&lt;/li&gt;
      &lt;li&gt;But there’re so much interactions in the process so just re-scale the output.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;momentum-parameter-at-batchnorm1d&quot;&gt;Momentum parameter at BatchNorm1d&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Different from momentum like in optimization.&lt;/li&gt;
  &lt;li&gt;This momentum is Exponentially weighted moving average of the mean, instead of deviation.
    &lt;ul&gt;
      &lt;li&gt;If this is small number: &lt;code class=&quot;highlighter-rouge&quot;&gt;mean standard deviation&lt;/code&gt; would be less from mini_batch to mini_batch » less regularization effect. (If this is large number, variation would be greater from mini_batch to mini_batch » more regularization effect)&lt;/li&gt;
      &lt;li&gt;TODO: can’t sure, but i understand, this is not about &lt;code class=&quot;highlighter-rouge&quot;&gt;how to update parameter&lt;/code&gt; but about &lt;code class=&quot;highlighter-rouge&quot;&gt;how much reflect previous value when scale and shift&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Q. Preference between batchnorm and the other regularizations(drop out, weight decay)&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;A. Nope, always try and see the results&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;lesson6-pets-moreipynb&quot;&gt;lesson6-pets-more.ipynb&lt;/h2&gt;

&lt;h3 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Last reg&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_transforms&lt;/code&gt; has lots of params (even not yet learned all) -&amp;gt; check documentation
    &lt;ul&gt;
      &lt;li&gt;Remember you can implement all the doc contents bc it’s made from nbdev&lt;/li&gt;
      &lt;li&gt;TODO: try this!!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Essence of data augmentation is you should maintain the label, while somewhat making sense.
    &lt;ul&gt;
      &lt;li&gt;ex) tilt, because it’s optically sensible, you can always change the angle of the data view.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;zeros, border, and reflection but always &lt;code class=&quot;highlighter-rouge&quot;&gt;reflection&lt;/code&gt; works most of the time, so that is the default&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;convolutional-kernelwhat-is-convolution&quot;&gt;Convolutional Kernel(What is convolution?)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Will make heat_map from scratch, which means the parts convolution focuses on&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/setosa_visualization.png&quot; alt=&quot;setosa_visualization&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;http://setosa.io/ev/image-kernels/
    &lt;ul&gt;
      &lt;li&gt;javascript thing&lt;/li&gt;
      &lt;li&gt;How convolution works&lt;/li&gt;
      &lt;li&gt;Kernel. which does element-wise multiplication, and sum them up&lt;/li&gt;
      &lt;li&gt;so it has on pixel less at borders -&amp;gt; so it uses padding, and fastai uses reflection as said.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;why this Kernel(matrix) helps catching horizontal edge side?
    &lt;ul&gt;
      &lt;li&gt;because below kernel weights differently, depends on &lt;code class=&quot;highlighter-rouge&quot;&gt;x axis&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;why familiar, because it’s similar intuition with Zeiler/Fergus &lt;a href=&quot;https://arxiv.org/abs/1311.2901&quot;&gt;Visualizing and Understanding Convolutional Networks&lt;/a&gt; paper&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matrix-kernel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c&quot;&gt;CNN from different viewpoints&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;output of pixel is results from different linear equations.&lt;/li&gt;
      &lt;li&gt;If you connect this with represents of neural network nodes, you can see that the specific inp nodes connected with specific out nodes.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Summarize&lt;/strong&gt;: cnn does 1) matmul some of the elements are always zero 2) same weight for every row, which is called &lt;code class=&quot;highlighter-rouge&quot;&gt;weight time? weight..?, 1:18:50&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;(picture)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;further-lowdown&quot;&gt;Further lowdown&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Because generally image has 3  channels, we need rank 3 kernel.&lt;/li&gt;
  &lt;li&gt;And &lt;strong&gt;do multiply with all channel output is one pixel&lt;/strong&gt;.(&lt;code class=&quot;highlighter-rouge&quot;&gt;draw by your self&lt;/code&gt;)
    &lt;ul&gt;
      &lt;li&gt;but this kernel will catch one feature, like horizontal, so that we make more kernel so that output becomes (h * w * kernel)&lt;/li&gt;
      &lt;li&gt;And that &lt;code class=&quot;highlighter-rouge&quot;&gt;kernel&lt;/code&gt; come to &lt;code class=&quot;highlighter-rouge&quot;&gt;channel&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stride 2 conv&lt;/strong&gt;: with 3 by 3 kernel, stride 2 conv -&amp;gt; (h/2 * w/2 * kernel) &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;
    &lt;ul&gt;
      &lt;li&gt;skip or jump over input pixel&lt;/li&gt;
      &lt;li&gt;to protect from memory out of control&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;TODO: understand yourself the blocks of conv-kernel:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Usually use big kernel size at first layer (will study this at part2) &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;Bottom&amp;amp;right highlighting kernel, since that parts are positive numbers&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;k = tensor([
    [0.  ,-5/3,1],
    [-5/3,-5/3,1],
    [1.  ,1   ,1],
]).expand(1,3,3,3)/6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Why divided by 6, when doing expand? : &lt;a href=&quot;https://forums.fast.ai/t/lesson-6-in-class-discussion/31440/353?u=spellonyou&quot;&gt;forum answer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.tensor.expand&lt;/code&gt;: for memory efficient, because we should do RGB&lt;/li&gt;
  &lt;li&gt;We do not make separate kernel, but make rank 4 kernel
    &lt;ul&gt;
      &lt;li&gt;4d tensor is just stacked kernel&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;t[None].shape&lt;/code&gt; create new unit axis, and why? we make this -&amp;gt; it should move unit of batch, not one size image.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;average-pooling-feature&quot;&gt;Average pooling, feature&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;suppose our pre-trained model results in size of &lt;code class=&quot;highlighter-rouge&quot;&gt;11 by 11 by 512 &lt;/code&gt; and my classification task has 37 classes
    &lt;ul&gt;
      &lt;li&gt;take the first face of channel, which is 11 by 11 and &lt;code class=&quot;highlighter-rouge&quot;&gt;mean&lt;/code&gt; it, so that make rank 2 tensor, 512 by 1&lt;/li&gt;
      &lt;li&gt;and make 2d matrix, which is 512 by 37 and multiply so that we can get 37 by 1 matrix.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Feature, at convolution block
    &lt;ul&gt;
      &lt;li&gt;So, when we transfer-learning without unfreeze, every element of last matrix (512 by 1) should represent(or could catch) each feature.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heatmap-hook&quot;&gt;Heatmap, Hook&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/heatmap.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hook_output(model[0]) -&amp;gt; acts -&amp;gt; avg_acts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;if we average the block with &lt;code class=&quot;highlighter-rouge&quot;&gt;axis=feature&lt;/code&gt;, result of matrix(11 by 11) depicts &lt;code class=&quot;highlighter-rouge&quot;&gt;how activated was that area?&lt;/code&gt; -&amp;gt; it is heatmap, &lt;code class=&quot;highlighter-rouge&quot;&gt;avg_acts&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;and acts comes from hook, which is more advanced pytorch feature.
    &lt;ul&gt;
      &lt;li&gt;hook into pytorch machine itself, and run any arbitrary Pytorch code&lt;/li&gt;
      &lt;li&gt;Why this is cool?: Normally it gives set of outputs of forward pass, but we can interrupt and hook the forward pass.&lt;/li&gt;
      &lt;li&gt;Also can store the output of the convolutional part of the model, which is before avg_pooling&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Thinking back when we do cut off &lt;code class=&quot;highlighter-rouge&quot;&gt;after&lt;/code&gt; the conv part.
    &lt;ul&gt;
      &lt;li&gt;but with fast.ai the original convolutional part of the model would be &lt;em&gt;the first thing in the model&lt;/em&gt;, specifically could be given from &lt;code class=&quot;highlighter-rouge&quot;&gt;learn.model.eval()[0]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;And this is gotten from &lt;code class=&quot;highlighter-rouge&quot;&gt;hooked_output&lt;/code&gt; and having hooked the output, we can pass our x_minibatch to output.&lt;/li&gt;
      &lt;li&gt;Not directly, but with normalized, minibatch, put on to the gpu&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;one_item()&lt;/code&gt; function do it, when we have one data &lt;code class=&quot;highlighter-rouge&quot;&gt;TODO: this is assignment&lt;/code&gt; do it yourself without one_item function&lt;/li&gt;
      &lt;li&gt;and &lt;code class=&quot;highlighter-rouge&quot;&gt;.cuda()&lt;/code&gt; put it on gpu&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;you should print out very often the shape of tensor, and try think why.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;(personal) Further research&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Yes, as notes of official course, the ConvNN have become more and more important for other ML model but computer vision. (see &lt;a href=&quot;https://arxiv.org/pdf/1807.06521.pdf&quot;&gt;Convolutional Block Attention Module&lt;/a&gt; relationship paper) and nlp is much more fall behind of computer vision at modern deep learning research, less augmentation method, resource optimization, performance(even not comparable, they both has standard task), … &lt;strong&gt;SO&lt;/strong&gt; How about enhance nlp model using this (kind of) relationship?&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Footnote&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;what is conv1d, conv2d, conv3d? &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;why use comparatively huge kernel at first layer? &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>dionne</name></author><summary type="html">[v1] Thu, 16 Apr 2020</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/att_00069.png" /></entry></feed>